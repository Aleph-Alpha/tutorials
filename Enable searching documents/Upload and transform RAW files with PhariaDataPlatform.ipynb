{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload and transform RAW files with PhariaDataPlatform\n",
    "<a id=\"data-setup\"></a>\n",
    "\n",
    "This section describes how to establish a complete document ingestion pipeline in PhariaAI. The ingestion pipeline is a crucial foundation for RAG applications, as it transforms source RAW documents into searchable, AI-ready processed documents.\n",
    "\n",
    "## Pipeline components\n",
    "\n",
    "<img src=\"../Visualizations/E2E-Tutorial-data-pipeline.png\" alt=\"Ingestion workflow\" style=\"width:85%\"/>\n",
    "\n",
    "\n",
    "## What you will learn\n",
    "\n",
    "1. How to configure your environment and connection parameters\n",
    "2. How to create an ingestion pipeline with the PhariaData API\n",
    "3. How to upload documents and monitor their processing\n",
    "4. How to use PhariaSearch on the uploaded files\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting, ensure you have performed the previous tutorial <a href=\"./Setup collections and indexex with PhariaSearch.ipynb\">Setup collections and indexes with PhariaSearch</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure\n",
    "\n",
    "Below, you can see all concepts involved in the creation of the pipeline and their relationships.\n",
    "\n",
    "<img src=\"../Visualizations/E2E-Tutorial-data-pipeline-relationships.png\" alt=\"Resources relationships\" style=\"width:70%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies and configure the environment\n",
    "\n",
    "We begin by importing necessary dependencies and setting up the environment. We use standard Python libraries such as `requests` for API communication, `pandas` for data handling, as well as specialised libraries such as `tenacity` for robust error handling with retry mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from os import (\n",
    "    getenv, \n",
    "    path,\n",
    "    listdir\n",
    ")\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import concurrent.futures\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_exponential,\n",
    "    retry_if_exception_type,\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setups\n",
    "load_dotenv(override=True)\n",
    "\n",
    "TOKEN = getenv(\"PHARIA_AI_TOKEN\")\n",
    "NAMESPACE = getenv(\"PHARIA_DATA_NAMESPACE\")\n",
    "COLLECTION = getenv(\"PHARIA_DATA_COLLECTION\")\n",
    "INDEX = getenv(\"INDEX\")\n",
    "\n",
    "PHARIA_API_BASE_URL = getenv(\"PHARIA_API_BASE_URL\")\n",
    "\n",
    "DATA_PLATFORM_URL = f\"{PHARIA_API_BASE_URL}/studio/data\"\n",
    "DOCUMENT_INDEX_API_URL = f\"{PHARIA_API_BASE_URL}/studio/search\"\n",
    "\n",
    "STAGE_NAME = getenv(\"STAGE_NAME\")\n",
    "REPOSITORY_NAME = getenv(\"REPOSITORY_NAME\")\n",
    "TRANSFORMATION_NAME = getenv(\"TRANSFORMATION_NAME\")\n",
    "TRIGGER_NAME = getenv(\"TRIGGER_NAME\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Create a document repository\n",
    "\n",
    "A repository in PhariaData is a storage container that organises processed documents. \n",
    "\n",
    "The `get_or_create_repository` function checks if a repository with the specified name already exists and creates one if it does not. The function returns the repository ID, which is referenced in later steps when configuring the ingestion pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper function \n",
    "\n",
    "def get_or_create_repository(repository: dict) -> str:\n",
    "    \"\"\"Get or create a repository in the Data Platform.\"\"\"\n",
    "    dataplatform_base_url = DATA_PLATFORM_URL\n",
    "    name = repository[\"name\"]\n",
    "    url = f\"{dataplatform_base_url}/repositories?name={name}\"\n",
    "\n",
    "    token = TOKEN\n",
    "    response = requests.get(\n",
    "        url=url, headers={\"Authorization\": f\"Bearer {token}\"}, verify=False\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    page = response.json()\n",
    "\n",
    "    if page[\"total\"] > 0:\n",
    "        return page[\"repositories\"][0][\"repositoryId\"]\n",
    "    else:\n",
    "        url = f\"{dataplatform_base_url}/repositories\"\n",
    "        response = requests.post(\n",
    "            url=url,\n",
    "            json=repository,\n",
    "            headers={\"Authorization\": f\"Bearer {token}\"},\n",
    "            verify=False,\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        repo_created = response.json()\n",
    "        return repo_created[\"repositoryId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository ID: 64155b2d-86d1-495b-979d-b0e64f317693\n"
     ]
    }
   ],
   "source": [
    "## Create the repository\n",
    "\n",
    "repository_payload = {\n",
    "    \"name\": REPOSITORY_NAME,\n",
    "    \"mediaType\": \"jsonlines\",\n",
    "    \"modality\": \"text\",\n",
    "    \"schema\": None,\n",
    "}\n",
    "\n",
    "repository_id = get_or_create_repository(repository_payload)\n",
    "print(f\"Repository ID: {repository_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Configure a document upload stage\n",
    "\n",
    "A stage provides storage for source documents before they are processed.\n",
    "\n",
    "The stage configuration includes a trigger that defines what happens when source documents are uploaded. This trigger specifies the transformation to apply and where to store the results.\n",
    "\n",
    "The `get_or_create_stage` function returns a stage ID that is used when uploading documents in later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper function \n",
    "\n",
    "def get_or_create_stage(stage: dict) -> str:\n",
    "    \"\"\"Get or create a stage in the Data Platform.\"\"\"\n",
    "    dataplatform_base_url = DATA_PLATFORM_URL\n",
    "    name = stage[\"name\"]\n",
    "    url = f\"{dataplatform_base_url}/stages?name={name}\"\n",
    "\n",
    "    token = TOKEN\n",
    "    response = requests.get(\n",
    "        url=url, headers={\"Authorization\": f\"Bearer {token}\"}, verify=False\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    page = response.json()\n",
    "\n",
    "    if page[\"total\"] > 0:\n",
    "        return page[\"stages\"][0][\"stageId\"]\n",
    "    else:\n",
    "        url = f\"{dataplatform_base_url}/stages\"\n",
    "        response = requests.post(\n",
    "            url=url,\n",
    "            json=stage,\n",
    "            headers={\"Authorization\": f\"Bearer {token}\"},\n",
    "            verify=False,\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        stage_created = response.json()\n",
    "        return stage_created[\"stageId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage ID: 350dcaf6-fff0-4ad0-a69b-bf16eecb0d0b\n"
     ]
    }
   ],
   "source": [
    "## Setup stage\n",
    "\n",
    "stage_payload = {\n",
    "    \"name\": STAGE_NAME,\n",
    "    \"triggers\": [\n",
    "        {\n",
    "            \"transformationName\": TRANSFORMATION_NAME,\n",
    "            \"destinationType\": \"DataPlatform:Repository\",\n",
    "            \"connectorType\": \"DocumentIndex:Collection\",\n",
    "            \"name\": TRIGGER_NAME,\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "stage_id = get_or_create_stage(stage_payload)\n",
    "print(f\"Stage ID: {stage_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Set up automated document processing\n",
    "\n",
    "The trigger configuration defines what happens when source documents are uploaded to the stage. The `ingestion_context` object combines three key elements:\n",
    "\n",
    "1. The trigger name that identifies which trigger to activate\n",
    "2. The destination repository where processed documents are stored\n",
    "3. The collection and namespace where processed documents are indexed\n",
    "\n",
    "This context is included with source document uploads to instruct the system on how to process each document. When a source document is uploaded, the specified trigger automatically applies the transformation and indexes the processed document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion context: {'triggerName': 'testTrigger - DocumentStorageTutorial', 'destinationContext': {'repositoryId': '64155b2d-86d1-495b-979d-b0e64f317693'}, 'connectorContext': {'collection': 'pharia-tutorial-rag', 'namespace': 'Studio'}}\n"
     ]
    }
   ],
   "source": [
    "ingestion_context = {\n",
    "    \"triggerName\": TRIGGER_NAME,\n",
    "    \"destinationContext\": {\"repositoryId\": repository_id},\n",
    "    \"connectorContext\": {\n",
    "        \"collection\": COLLECTION,\n",
    "        \"namespace\": NAMESPACE,\n",
    "    },\n",
    "}\n",
    "print(f\"Ingestion context: {ingestion_context}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Upload and process documents\n",
    "\n",
    "With our infrastructure set-up complete (repository, stage, index, and trigger), we can now upload source documents to the PhariaAI platform. This section demonstrates how to upload source documents and initiate the document ingestion process.\n",
    "\n",
    "The document ingestion workflow transforms source documents into searchable processed documents through several steps: uploading to the stage, applying transformations, storing in the repository, and indexing for search.\n",
    "\n",
    "The `ingest_all_documents` helper function returns a DataFrame with details on each upload attempt, making it easy to track successes and failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper functions\n",
    "\n",
    "@retry(\n",
    "    stop=stop_after_attempt(3),\n",
    "    wait=wait_exponential(multiplier=1, min=2, max=10),\n",
    "    retry=retry_if_exception_type(requests.RequestException),\n",
    ")\n",
    "def ingest_document(\n",
    "    document_path: str, ingestion_context: dict, name: str, stage_id: str\n",
    ") -> dict:\n",
    "    \"\"\"Attempts to ingest a document and returns the ingestion result.\"\"\"\n",
    "    with open(document_path, mode=\"rb\") as file_reader:\n",
    "        dataplatform_base_url = DATA_PLATFORM_URL\n",
    "        url = f\"{dataplatform_base_url}/stages/{stage_id}/files\"\n",
    "        token = TOKEN\n",
    "        response = requests.post(\n",
    "            url=url,\n",
    "            headers={\"Authorization\": f\"Bearer {token}\"},\n",
    "            verify=False,\n",
    "            files={\n",
    "                \"name\": name,\n",
    "                \"sourceData\": file_reader,\n",
    "                \"ingestionContext\": json.dumps(ingestion_context),\n",
    "            },\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "\n",
    "        file_uploaded = response.json()\n",
    "        return {\n",
    "            \"file_id\": file_uploaded[\"fileId\"],\n",
    "            \"status\": \"Success\",\n",
    "            \"error_type\": None,\n",
    "            \"error_message\": None,\n",
    "        }\n",
    "    \n",
    "\n",
    "\n",
    "def ingest_all_documents(\n",
    "    directory_path: str, ingestion_context: dict, stage_id: str, max_workers: int = 3\n",
    "):\n",
    "    \"\"\"Ingest all files in a directory concurrently and store results in a DataFrame.\"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_file = {\n",
    "            executor.submit(\n",
    "                ingest_document,\n",
    "                path.join(directory_path, file),\n",
    "                ingestion_context,\n",
    "                file,\n",
    "                stage_id,\n",
    "            ): file\n",
    "            for file in listdir(directory_path)\n",
    "        }\n",
    "\n",
    "        for future in concurrent.futures.as_completed(future_to_file):\n",
    "            file_name = future_to_file[future]\n",
    "            file_path = path.join(directory_path, file_name)\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"file_path\": file_path,\n",
    "                        \"file_id\": result[\"file_id\"],\n",
    "                        \"status\": result[\"status\"],\n",
    "                        \"error_type\": result[\"error_type\"],\n",
    "                        \"error_message\": result[\"error_message\"],\n",
    "                    }\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while ingesting {file_path}: {e}\")\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"file_path\": file_path,\n",
    "                        \"file_id\": None,\n",
    "                        \"status\": \"Ingestion Failed\",\n",
    "                        \"error\": str(e),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>file_id</th>\n",
       "      <th>status</th>\n",
       "      <th>error_type</th>\n",
       "      <th>error_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>files_to_upload/Azure Cognitive Search_ Outper...</td>\n",
       "      <td>687496e4-5d0b-4abd-a35a-30790dd2d7ba</td>\n",
       "      <td>Success</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>files_to_upload/What is RAG_ - Retrieval-Augme...</td>\n",
       "      <td>7f4f5e3a-02a6-437d-bcfe-22d87d337b9a</td>\n",
       "      <td>Success</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>files_to_upload/RAG.pdf</td>\n",
       "      <td>9112b8af-caf7-4274-a46a-e7c57e4f3c06</td>\n",
       "      <td>Success</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>files_to_upload/Attention is all you need.pdf</td>\n",
       "      <td>7e472330-eebe-4420-8c22-67bbb587e9a5</td>\n",
       "      <td>Success</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_path  \\\n",
       "0  files_to_upload/Azure Cognitive Search_ Outper...   \n",
       "1  files_to_upload/What is RAG_ - Retrieval-Augme...   \n",
       "2                            files_to_upload/RAG.pdf   \n",
       "3      files_to_upload/Attention is all you need.pdf   \n",
       "\n",
       "                                file_id   status error_type error_message  \n",
       "0  687496e4-5d0b-4abd-a35a-30790dd2d7ba  Success       None          None  \n",
       "1  7f4f5e3a-02a6-437d-bcfe-22d87d337b9a  Success       None          None  \n",
       "2  9112b8af-caf7-4274-a46a-e7c57e4f3c06  Success       None          None  \n",
       "3  7e472330-eebe-4420-8c22-67bbb587e9a5  Success       None          None  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ingesting the files\n",
    "directory_path = \"files_to_upload\"\n",
    "df_results = ingest_all_documents(directory_path, ingestion_context, stage_id)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Monitor the source document processing status\n",
    "\n",
    "After uploading source documents, you need to verify their processing status. The code in this section does the following:\n",
    "\n",
    "1. Extracts IDs of successfully uploaded source documents\n",
    "2. Retrieves the transformation ID\n",
    "3. Checks the status of each source document's transformation\n",
    "4. Extracts dataset IDs from completed transformations\n",
    "\n",
    "The `check_files_status` function combines all this information into a comprehensive report that shows which files completed processing and which encountered errors. The dataset IDs are particularly important as they are used to access your processed documents in subsequent operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_successful_document_ids(df: pd.DataFrame) -> list:\n",
    "    \"\"\"Retrieve a list of successful file_ids from the DataFrame.\"\"\"\n",
    "    return df[df[\"status\"] == \"Success\"][\"file_id\"].tolist()\n",
    "\n",
    "@retry(\n",
    "    stop=stop_after_attempt(3),\n",
    "    wait=wait_exponential(multiplier=1, min=2, max=10),\n",
    "    retry=retry_if_exception_type(requests.RequestException),\n",
    ")\n",
    "def check_status_of_ingestion(transformation_id: str, file_id: str) -> dict:\n",
    "    \"\"\"Query the status of the ingestion for a given transformation and file_id.\"\"\"\n",
    "    dataplatform_base_url = DATA_PLATFORM_URL\n",
    "    url = f\"{dataplatform_base_url}/transformations/{transformation_id}/runs?file_id={file_id}\"\n",
    "\n",
    "    token = TOKEN\n",
    "    response = requests.get(\n",
    "        url=url, headers={\"Authorization\": f\"Bearer {token}\"}, verify=False\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    page = response.json()\n",
    "\n",
    "    assert page[\"total\"] > 0\n",
    "    return page[\"runs\"][0]\n",
    "\n",
    "def get_transformation_id(name: str) -> str:\n",
    "    \"\"\"Get the transformation ID from the Data Platform.\"\"\"\n",
    "    dataplatform_base_url = DATA_PLATFORM_URL\n",
    "    url = f\"{dataplatform_base_url}/transformations?name={name}\"\n",
    "\n",
    "    token = TOKEN\n",
    "    response = requests.get(\n",
    "        url=url, headers={\"Authorization\": f\"Bearer {token}\"}, verify=False\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    page = response.json()\n",
    "\n",
    "    assert page[\"total\"] > 0\n",
    "    return page[\"transformations\"][0][\"transformationId\"]\n",
    "\n",
    "def check_files_status(transformation_id: str, df: pd.DataFrame, max_workers: int = 3):\n",
    "    \"\"\"Check the status of ingested files and store the results in a DataFrame.\"\"\"\n",
    "\n",
    "    successful_file_ids = get_successful_document_ids(df)\n",
    "    status_results = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_file = {\n",
    "            executor.submit(\n",
    "                check_status_of_ingestion, transformation_id, file_id\n",
    "            ): file_id\n",
    "            for file_id in successful_file_ids\n",
    "        }\n",
    "\n",
    "        for future in concurrent.futures.as_completed(future_to_file):\n",
    "            file_id = future_to_file[future]\n",
    "            try:\n",
    "                run = future.result()\n",
    "                output = json.dumps(run.get(\"output\", {}), indent=4)\n",
    "                status_results.append(\n",
    "                    {\n",
    "                        \"file_id\": file_id,\n",
    "                        \"run_id\": run[\"runId\"],\n",
    "                        \"status\": run[\"status\"],\n",
    "                        \"output\": output,\n",
    "                        \"error\": run[\"errors\"],\n",
    "                    }\n",
    "                )\n",
    "            except Exception as e:\n",
    "                status_results.append(\n",
    "                    {\n",
    "                        \"file_id\": file_id,\n",
    "                        \"status\": run[\"status\"],\n",
    "                        \"output\": None,\n",
    "                        \"error\": str(e),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return df.merge(\n",
    "        pd.DataFrame(status_results),\n",
    "        on=\"file_id\",\n",
    "        how=\"left\",\n",
    "        suffixes=(\"_ingestion\", \"\"),\n",
    "    )\n",
    "\n",
    "def get_successful_dataset_ids(df: pd.DataFrame) -> list:\n",
    "    \"\"\"Retrieve a list of successful dataset_ids from the DataFrame.\"\"\"\n",
    "    dataset_ids_list = []\n",
    "    for i in range(len(df)):\n",
    "        dataset_ids_list.append(json.loads(df[\"output\"][i]).get(\"datasetId\"))\n",
    "    return dataset_ids_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>file_id</th>\n",
       "      <th>status_ingestion</th>\n",
       "      <th>error_type</th>\n",
       "      <th>error_message</th>\n",
       "      <th>run_id</th>\n",
       "      <th>status</th>\n",
       "      <th>output</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>files_to_upload/Azure Cognitive Search_ Outper...</td>\n",
       "      <td>687496e4-5d0b-4abd-a35a-30790dd2d7ba</td>\n",
       "      <td>Success</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>7ea6222d-f831-48d2-bc40-51d3d82653ab</td>\n",
       "      <td>completed</td>\n",
       "      <td>{\\n    \"type\": \"DataPlatform:Repository:Datase...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>files_to_upload/What is RAG_ - Retrieval-Augme...</td>\n",
       "      <td>7f4f5e3a-02a6-437d-bcfe-22d87d337b9a</td>\n",
       "      <td>Success</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>30a1a78f-b283-4f0f-baf0-ec37cb4b3330</td>\n",
       "      <td>completed</td>\n",
       "      <td>{\\n    \"type\": \"DataPlatform:Repository:Datase...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>files_to_upload/RAG.pdf</td>\n",
       "      <td>9112b8af-caf7-4274-a46a-e7c57e4f3c06</td>\n",
       "      <td>Success</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>d0bd1fdd-2e05-4588-9177-28d0c2c46c09</td>\n",
       "      <td>completed</td>\n",
       "      <td>{\\n    \"type\": \"DataPlatform:Repository:Datase...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>files_to_upload/Attention is all you need.pdf</td>\n",
       "      <td>7e472330-eebe-4420-8c22-67bbb587e9a5</td>\n",
       "      <td>Success</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>b2a399a8-c7ae-42af-b887-8451acaa849b</td>\n",
       "      <td>completed</td>\n",
       "      <td>{\\n    \"type\": \"DataPlatform:Repository:Datase...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_path  \\\n",
       "0  files_to_upload/Azure Cognitive Search_ Outper...   \n",
       "1  files_to_upload/What is RAG_ - Retrieval-Augme...   \n",
       "2                            files_to_upload/RAG.pdf   \n",
       "3      files_to_upload/Attention is all you need.pdf   \n",
       "\n",
       "                                file_id status_ingestion error_type  \\\n",
       "0  687496e4-5d0b-4abd-a35a-30790dd2d7ba          Success       None   \n",
       "1  7f4f5e3a-02a6-437d-bcfe-22d87d337b9a          Success       None   \n",
       "2  9112b8af-caf7-4274-a46a-e7c57e4f3c06          Success       None   \n",
       "3  7e472330-eebe-4420-8c22-67bbb587e9a5          Success       None   \n",
       "\n",
       "  error_message                                run_id     status  \\\n",
       "0          None  7ea6222d-f831-48d2-bc40-51d3d82653ab  completed   \n",
       "1          None  30a1a78f-b283-4f0f-baf0-ec37cb4b3330  completed   \n",
       "2          None  d0bd1fdd-2e05-4588-9177-28d0c2c46c09  completed   \n",
       "3          None  b2a399a8-c7ae-42af-b887-8451acaa849b  completed   \n",
       "\n",
       "                                              output error  \n",
       "0  {\\n    \"type\": \"DataPlatform:Repository:Datase...  None  \n",
       "1  {\\n    \"type\": \"DataPlatform:Repository:Datase...  None  \n",
       "2  {\\n    \"type\": \"DataPlatform:Repository:Datase...  None  \n",
       "3  {\\n    \"type\": \"DataPlatform:Repository:Datase...  None  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformation_id = get_transformation_id(TRANSFORMATION_NAME)\n",
    "status_df = check_files_status(transformation_id, df_results)\n",
    "status_df.to_csv(\"ingestion_status.csv\", index=False)\n",
    "successful_dataset_ids = get_successful_dataset_ids(status_df[status_df[\"status\"] == \"completed\"])\n",
    "status_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Interact with processed documents\n",
    "\n",
    "As done in the previous tutorial, it is possible to use PhariaSearch (previously Document Index) to perform any type of search on the collection where we uploaded the document. For example, let's use our semantic search index to check the newly uploaded documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SearchResult(id=DocumentPath(collection_path=CollectionPath(namespace='Studio', collection='pharia-tutorial-rag'), document_name='7c9e1645-b0cd-4198-9ed9-bcaca5f82cfd'), score=0.66146505, document_chunk=DocumentChunk(text='Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.', start=5535, end=5712, metadata=None)),\n",
       " SearchResult(id=DocumentPath(collection_path=CollectionPath(namespace='Studio', collection='pharia-tutorial-rag'), document_name='d3178aad-7e4f-4116-82f9-594a59a650f4'), score=0.6613786, document_chunk=DocumentChunk(text='Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.', start=5535, end=5712, metadata=None)),\n",
       " SearchResult(id=DocumentPath(collection_path=CollectionPath(namespace='Studio', collection='pharia-tutorial-rag'), document_name='7c9e1645-b0cd-4198-9ed9-bcaca5f82cfd'), score=0.60858935, document_chunk=DocumentChunk(text='An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum ## Scaled Dot-Product Attention', start=8679, end=8910, metadata=None)),\n",
       " SearchResult(id=DocumentPath(collection_path=CollectionPath(namespace='Studio', collection='pharia-tutorial-rag'), document_name='d3178aad-7e4f-4116-82f9-594a59a650f4'), score=0.6078117, document_chunk=DocumentChunk(text='An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum ## Scaled Dot-Product Attention', start=8679, end=8910, metadata=None)),\n",
       " SearchResult(id=DocumentPath(collection_path=CollectionPath(namespace='Studio', collection='pharia-tutorial-rag'), document_name='d3178aad-7e4f-4116-82f9-594a59a650f4'), score=0.55645823, document_chunk=DocumentChunk(text='Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22].', start=5714, end=5927, metadata=None))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pharia_data_sdk.connectors import DocumentIndexClient\n",
    "from pharia_data_sdk.connectors.retrievers import DocumentIndexRetriever\n",
    "\n",
    "search_client = DocumentIndexClient(\n",
    "    token=TOKEN,\n",
    "    base_url=DOCUMENT_INDEX_API_URL,\n",
    ")\n",
    "\n",
    "document_index_retriever = DocumentIndexRetriever(\n",
    "    document_index=search_client,\n",
    "    index_name=INDEX,\n",
    "    namespace=NAMESPACE,\n",
    "    collection=COLLECTION,\n",
    "    k=5,\n",
    ")\n",
    "\n",
    "document_index_retriever.get_relevant_documents_with_scores(\n",
    "    query=\"what is attention?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this section, you successfully set up the complete document ingestion pipeline to be able to ingest RAW file and index them with PhariaSearch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
