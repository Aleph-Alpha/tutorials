{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload and transform RAW files with PhariaDataPlatform\n",
    "<a id=\"data-setup\"></a>\n",
    "\n",
    "This section describes how to establish a complete document ingestion pipeline in PhariaAI. The ingestion pipeline is a crucial foundation for RAG applications, as it transforms source RAW documents into searchable, AI-ready processed documents.\n",
    "\n",
    "## Pipeline components\n",
    "\n",
    "<img src=\"../Visualizations/E2E-Tutorial-data-pipeline.png\" alt=\"Ingestion workflow\" style=\"width:85%\"/>\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting, ensure you have performed the previous tutorial <a href=\"./1. Setup collections and indexex with PhariaSearch.ipynb\">Setup collections and indexes with PhariaSearch</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure\n",
    "\n",
    "Below, you can see all concepts involved in the creation of the pipeline and their relationships.\n",
    "\n",
    "<img src=\"../Visualizations/E2E-Tutorial-data-pipeline-relationships.png\" alt=\"Resources relationships\" style=\"width:70%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Validation\n",
    "\n",
    "\u26a0\ufe0f **CRITICAL: Validate your environment before proceeding**\n",
    "\n",
    "This step ensures your `.env` file is properly configured with valid API endpoints, authentication tokens, and unique resource names.\n",
    "\n",
    "**DO NOT SKIP!** If validation fails:\n",
    "- Check your `.env` file for missing values\n",
    "- Contact your infrastructure administrator if errors persist\n",
    "\n",
    "Only proceed after ALL checks pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udd0d Validating environment configuration...\n",
      "\n",
      "1\ufe0f\u20e3  Checking required environment variables:\n",
      "   \u2705 PHARIA_API_BASE_URL: https://api.customer.pharia.com\n",
      "   \u2705 PHARIA_AI_TOKEN: eyJhbGci...\n",
      "   \u2705 PHARIA_DATA_NAMESPACE: Studio\n",
      "   \u2705 PHARIA_DATA_COLLECTION: pharia-tutorial-rag-vsp-1\n",
      "   \u2705 INDEX: rag-tutorial-index-1\n",
      "   \u2705 HYBRID_INDEX: rag-tutorial-hybrid-index-vsp-1\n",
      "   \u2705 FILTER_INDEX: rag-tutorial-filter-index-vsp-1\n",
      "   \u2705 EMBEDDING_MODEL_NAME: luminous-base\n",
      "\n",
      "2\ufe0f\u20e3  Validating URL format:\n",
      "   \u2705 PHARIA_API_BASE_URL: Valid format\n",
      "\n",
      "3\ufe0f\u20e3  Testing PhariaAI API access:\n",
      "   \u2705 API connection successful\n",
      "\n",
      "==================================================\n",
      "\u2705 All validation checks passed! Your environment is properly configured.\n",
      "\n",
      "You can now proceed with the tutorial.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\nfrom pathlib import Path\nsys.path.insert(0, str(Path.cwd().parent.parent))\nfrom shared.utils.env_validator import validate_environment\n",
    "\n",
    "# Run environment validation\n",
    "validate_environment(env_path=\"../../shared/config/.env\", env_sample_path=\"../../shared/config/.env.sample\")\n",
    "\n",
    "# If validation fails, DO NOT proceed with the rest of the tutorial\n",
    "# Fix the issues identified above first!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import dependencies and configure the environment\n",
    "\n",
    "We begin by importing necessary dependencies and setting up the environment. We use standard Python libraries such as `requests` for API communication, `pandas` for data handling, as well as specialised libraries such as `tenacity` for robust error handling with retry mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[92m\u2705 SUCCESS: Python dependencies imported successfully\u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from os import (\n",
    "    getenv, \n",
    "    path,\n",
    "    listdir\n",
    ")\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import concurrent.futures\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_exponential,\n",
    "    retry_if_exception_type,\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"\\033[92m\u2705 SUCCESS: Python dependencies imported successfully\\033[0m\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[92m\u2705 SUCCESS: Environment variables loaded\u001b[0m\n",
      "   \u2022 Namespace: Studio\n",
      "   \u2022 Collection: pharia-tutorial-rag-vsp-1\n",
      "   \u2022 Index: rag-tutorial-index-1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Setups\n",
    "load_dotenv(override=True)\n",
    "\n",
    "TOKEN = getenv(\"PHARIA_AI_TOKEN\")\n",
    "NAMESPACE = getenv(\"PHARIA_DATA_NAMESPACE\")\n",
    "COLLECTION = getenv(\"PHARIA_DATA_COLLECTION\")\n",
    "INDEX = getenv(\"INDEX\")\n",
    "\n",
    "PHARIA_API_BASE_URL = getenv(\"PHARIA_API_BASE_URL\")\n",
    "\n",
    "DATA_PLATFORM_URL = f\"{PHARIA_API_BASE_URL}/v1/studio/data\"\n",
    "DOCUMENT_INDEX_API_URL = f\"{PHARIA_API_BASE_URL}/v1/studio/search\"\n",
    "\n",
    "STAGE_NAME = getenv(\"STAGE_NAME\")\n",
    "REPOSITORY_NAME = getenv(\"REPOSITORY_NAME\")\n",
    "TRANSFORMATION_NAME = getenv(\"TRANSFORMATION_NAME\")\n",
    "TRIGGER_NAME = getenv(\"TRIGGER_NAME\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"\\033[92m\u2705 SUCCESS: Environment variables loaded\\033[0m\")\n",
    "print(f\"   \u2022 Namespace: {NAMESPACE}\")\n",
    "print(f\"   \u2022 Collection: {COLLECTION}\")\n",
    "print(f\"   \u2022 Index: {INDEX}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a document repository\n",
    "\n",
    "A repository in PhariaData is a storage container that organises processed documents. \n",
    "\n",
    "The `get_or_create_repository` function checks if a repository with the specified name already exists and creates one if it does not. The function returns the repository ID, which is referenced in later steps when configuring the ingestion pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[94m\u2139\ufe0f  INFO: Helper function 'get_or_create_repository' ready. Invoke it in the next cell. \u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Helper function \n",
    "\n",
    "def get_or_create_repository(repository: dict) -> str:\n",
    "    \"\"\"Get or create a repository in the Data Platform.\"\"\"\n",
    "    dataplatform_base_url = DATA_PLATFORM_URL\n",
    "    name = repository[\"name\"]\n",
    "    url = f\"{dataplatform_base_url}/repositories?name={name}\"\n",
    "\n",
    "    token = TOKEN\n",
    "    response = requests.get(\n",
    "        url=url, headers={\"Authorization\": f\"Bearer {token}\"}, verify=False\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    page = response.json()\n",
    "\n",
    "    if page[\"total\"] > 0:\n",
    "        return page[\"repositories\"][0][\"repositoryId\"]\n",
    "    else:\n",
    "        url = f\"{dataplatform_base_url}/repositories\"\n",
    "        response = requests.post(\n",
    "            url=url,\n",
    "            json=repository,\n",
    "            headers={\"Authorization\": f\"Bearer {token}\"},\n",
    "            verify=False,\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        repo_created = response.json()\n",
    "        return repo_created[\"repositoryId\"]\n",
    "\n",
    "print(\"\\n\")        \n",
    "print(\"\\033[94m\u2139\ufe0f  INFO: Helper function 'get_or_create_repository' ready. Invoke it in the next cell. \\033[0m\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[92m\u2705 SUCCESS: Repository created/retrieved\u001b[0m\n",
      "   \u2022 Repository ID: 103cce95-efeb-4365-9071-9d62d8788355\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Create the repository\n",
    "\n",
    "repository_payload = {\n",
    "    \"name\": REPOSITORY_NAME,\n",
    "    \"mediaType\": \"jsonlines\",\n",
    "    \"modality\": \"text\",\n",
    "    \"schema\": None,\n",
    "}\n",
    "\n",
    "repository_id = get_or_create_repository(repository_payload)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"\\033[92m\u2705 SUCCESS: Repository created/retrieved\\033[0m\")\n",
    "print(f\"   \u2022 Repository ID: {repository_id}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Configure a document upload stage\n",
    "\n",
    "A stage provides storage for source documents before they are processed.\n",
    "\n",
    "The stage configuration includes a trigger that defines what happens when source documents are uploaded. This trigger specifies the transformation to apply and where to store the results.\n",
    "\n",
    "The `get_or_create_stage` function returns a stage ID that is used when uploading documents in later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[94m\u2139\ufe0f  INFO: Helper function 'get_or_create_stage' ready. Invoke it in the next cell.\u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Helper function \n",
    "\n",
    "def get_or_create_stage(stage: dict) -> str:\n",
    "    \"\"\"Get or create a stage in the Data Platform.\"\"\"\n",
    "    dataplatform_base_url = DATA_PLATFORM_URL\n",
    "    name = stage[\"name\"]\n",
    "    url = f\"{dataplatform_base_url}/stages?name={name}\"\n",
    "\n",
    "    token = TOKEN\n",
    "    response = requests.get(\n",
    "        url=url, headers={\"Authorization\": f\"Bearer {token}\"}, verify=False\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    page = response.json()\n",
    "\n",
    "    if page[\"total\"] > 0:\n",
    "        return page[\"stages\"][0][\"stageId\"]\n",
    "    else:\n",
    "        url = f\"{dataplatform_base_url}/stages\"\n",
    "        response = requests.post(\n",
    "            url=url,\n",
    "            json=stage,\n",
    "            headers={\"Authorization\": f\"Bearer {token}\"},\n",
    "            verify=False,\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        stage_created = response.json()\n",
    "        return stage_created[\"stageId\"]\n",
    "\n",
    "print(\"\\n\")        \n",
    "print(\"\\033[94m\u2139\ufe0f  INFO: Helper function 'get_or_create_stage' ready. Invoke it in the next cell.\\033[0m\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[92m\u2705 SUCCESS: Stage created.\u001b[0m\n",
      "   \u2022 Stage ID: 1ed12fe7-74d4-465c-aa49-681b77c1013c\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Setup stage\n",
    "\n",
    "stage_payload = {\n",
    "    \"name\": STAGE_NAME,\n",
    "    \"triggers\": [\n",
    "        {\n",
    "            \"transformationName\": TRANSFORMATION_NAME,\n",
    "            \"destinationType\": \"DataPlatform:Repository\",\n",
    "            \"connectorType\": \"DocumentIndex:Collection\",\n",
    "            \"name\": TRIGGER_NAME,\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "stage_id = get_or_create_stage(stage_payload)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"\\033[92m\u2705 SUCCESS: Stage created.\\033[0m\")\n",
    "print(f\"   \u2022 Stage ID: {stage_id}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Set up automated document processing\n",
    "\n",
    "The trigger configuration defines what happens when source documents are uploaded to the stage. The `ingestion_context` object combines three key elements:\n",
    "\n",
    "1. The trigger name that identifies which trigger to activate\n",
    "2. The destination repository where processed documents are stored\n",
    "3. The collection and namespace where processed documents are indexed\n",
    "\n",
    "This context is included with source document uploads to instruct the system on how to process each document. When a source document is uploaded, the specified trigger automatically applies the transformation and indexes the processed document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[94m\u2139\ufe0f  INFO: Ingestion context configured\u001b[0m\n",
      "   \u2022 Trigger: testTrigger - DocumentStorageTutorial\n",
      "   \u2022 Repository ID: 103cce95-efeb-4365-9071-9d62d8788355\n",
      "   \u2022 Collection: pharia-tutorial-rag-vsp-1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ingestion_context = {\n",
    "    \"triggerName\": TRIGGER_NAME,\n",
    "    \"destinationContext\": {\"repositoryId\": repository_id},\n",
    "    \"connectorContext\": {\n",
    "        \"collection\": COLLECTION,\n",
    "        \"namespace\": NAMESPACE,\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"\\033[94m\u2139\ufe0f  INFO: Ingestion context configured\\033[0m\")\n",
    "print(f\"   \u2022 Trigger: {TRIGGER_NAME}\")\n",
    "print(f\"   \u2022 Repository ID: {repository_id}\")\n",
    "print(f\"   \u2022 Collection: {COLLECTION}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Upload and process documents\n",
    "\n",
    "With our infrastructure set-up complete (repository, stage, index, and trigger), we can now upload source documents to the PhariaAI platform. This section demonstrates how to upload source documents and initiate the document ingestion process.\n",
    "\n",
    "The document ingestion workflow transforms source documents into searchable processed documents through several steps: uploading to the stage, applying transformations, storing in the repository, and indexing for search.\n",
    "\n",
    "The `ingest_all_documents` helper function returns a DataFrame with details on each upload attempt, making it easy to track successes and failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[94m\u2139\ufe0f  INFO: Helper functions 'ingest_document' and 'ingest_all_documents' ready. Invoke ingest_all_documents in the next cell.\u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Helper functions\n",
    "\n",
    "@retry(\n",
    "    stop=stop_after_attempt(3),\n",
    "    wait=wait_exponential(multiplier=1, min=2, max=10),\n",
    "    retry=retry_if_exception_type(requests.RequestException),\n",
    ")\n",
    "def ingest_document(\n",
    "    document_path: str, ingestion_context: dict, name: str, stage_id: str\n",
    ") -> dict:\n",
    "    \"\"\"Attempts to ingest a document and returns the ingestion result.\"\"\"\n",
    "    with open(document_path, mode=\"rb\") as file_reader:\n",
    "        dataplatform_base_url = DATA_PLATFORM_URL\n",
    "        url = f\"{dataplatform_base_url}/stages/{stage_id}/files\"\n",
    "        token = TOKEN\n",
    "        response = requests.post(\n",
    "            url=url,\n",
    "            headers={\"Authorization\": f\"Bearer {token}\"},\n",
    "            verify=False,\n",
    "            files={\n",
    "                \"name\": name,\n",
    "                \"sourceData\": file_reader,\n",
    "                \"ingestionContext\": json.dumps(ingestion_context),\n",
    "            },\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "\n",
    "        file_uploaded = response.json()\n",
    "        return {\n",
    "            \"file_id\": file_uploaded[\"fileId\"],\n",
    "            \"status\": \"Success\",\n",
    "            \"error_type\": None,\n",
    "            \"error_message\": None,\n",
    "        }\n",
    "    \n",
    "\n",
    "\n",
    "def ingest_all_documents(\n",
    "    directory_path: str, ingestion_context: dict, stage_id: str, max_workers: int = 3\n",
    "):\n",
    "    \"\"\"Ingest all files in a directory concurrently and store results in a DataFrame.\"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_file = {\n",
    "            executor.submit(\n",
    "                ingest_document,\n",
    "                path.join(directory_path, file),\n",
    "                ingestion_context,\n",
    "                file,\n",
    "                stage_id,\n",
    "            ): file\n",
    "            for file in listdir(directory_path)\n",
    "        }\n",
    "\n",
    "        for future in concurrent.futures.as_completed(future_to_file):\n",
    "            file_name = future_to_file[future]\n",
    "            file_path = path.join(directory_path, file_name)\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"file_path\": file_path,\n",
    "                        \"file_id\": result[\"file_id\"],\n",
    "                        \"status\": result[\"status\"],\n",
    "                        \"error_type\": result[\"error_type\"],\n",
    "                        \"error_message\": result[\"error_message\"],\n",
    "                    }\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"\\n\\033[91m\u26a0\ufe0f  ERROR: Failed to ingest {file_path}: {e}\\033[0m\\n\")\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"file_path\": file_path,\n",
    "                        \"file_id\": None,\n",
    "                        \"status\": \"Ingestion Failed\",\n",
    "                        \"error\": str(e),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    return df_results\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"\\033[94m\u2139\ufe0f  INFO: Helper functions 'ingest_document' and 'ingest_all_documents' ready. Invoke ingest_all_documents in the next cell.\\033[0m\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[93m\u2699\ufe0f  PROCESSING: Starting file ingestion from directory 'files_to_upload'\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[92m\u2705 SUCCESS: All 4 files uploaded successfully\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>file_id</th>\n",
       "      <th>status</th>\n",
       "      <th>error_type</th>\n",
       "      <th>error_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>files_to_upload/What is RAG_ - Retrieval-Augme...</td>\n",
       "      <td>d6c7e20c-c1f6-4d6e-98d1-6eec7a1f7f6e</td>\n",
       "      <td>Success</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>files_to_upload/Azure Cognitive Search_ Outper...</td>\n",
       "      <td>66e69a6f-d62c-45c0-a2b4-9be40426a5cb</td>\n",
       "      <td>Success</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>files_to_upload/RAG.pdf</td>\n",
       "      <td>83613d23-831c-4a80-8c1f-e5f298b68d13</td>\n",
       "      <td>Success</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>files_to_upload/Attention is all you need.pdf</td>\n",
       "      <td>d679c61f-28b1-4bb9-b8ce-3dda3b3001a7</td>\n",
       "      <td>Success</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_path  \\\n",
       "0  files_to_upload/What is RAG_ - Retrieval-Augme...   \n",
       "1  files_to_upload/Azure Cognitive Search_ Outper...   \n",
       "2                            files_to_upload/RAG.pdf   \n",
       "3      files_to_upload/Attention is all you need.pdf   \n",
       "\n",
       "                                file_id   status error_type error_message  \n",
       "0  d6c7e20c-c1f6-4d6e-98d1-6eec7a1f7f6e  Success       None          None  \n",
       "1  66e69a6f-d62c-45c0-a2b4-9be40426a5cb  Success       None          None  \n",
       "2  83613d23-831c-4a80-8c1f-e5f298b68d13  Success       None          None  \n",
       "3  d679c61f-28b1-4bb9-b8ce-3dda3b3001a7  Success       None          None  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ingesting the files\n",
    "directory_path = \"files_to_upload\"\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"\\033[93m\u2699\ufe0f  PROCESSING: Starting file ingestion from directory 'files_to_upload'\\033[0m\")\n",
    "print(\"\\n\")\n",
    "\n",
    "df_results = ingest_all_documents(directory_path, ingestion_context, stage_id)\n",
    "success_count = len(df_results[df_results['status'] == 'Success'])\n",
    "total_count = len(df_results)\n",
    "\n",
    "print(\"\\n\")\n",
    "if success_count == total_count:\n",
    "    print(f\"\\033[92m\u2705 SUCCESS: All {total_count} files uploaded successfully\\033[0m\")\n",
    "else:\n",
    "    print(f\"\\033[91m\u26a0\ufe0f  WARNING: {success_count}/{total_count} files uploaded successfully\\033[0m\")\n",
    "print(\"\\n\")\n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Monitor the source document processing status\n",
    "\n",
    "After uploading source documents, you need to verify their processing status. The code in this section does the following:\n",
    "\n",
    "1. Extracts IDs of successfully uploaded source documents\n",
    "2. Retrieves the transformation ID\n",
    "3. Checks the status of each source document's transformation\n",
    "4. Extracts dataset IDs from completed transformations\n",
    "\n",
    "The `check_files_status` function combines all this information into a comprehensive report that shows which files completed processing and which encountered errors. The dataset IDs are particularly important as they are used to access your processed documents in subsequent operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[94m\u2139\ufe0f  INFO: Status monitoring functions ready. Invoke these in the next cell.\u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_successful_document_ids(df: pd.DataFrame) -> list:\n",
    "    \"\"\"Retrieve a list of successful file_ids from the DataFrame.\"\"\"\n",
    "    return df[df[\"status\"] == \"Success\"][\"file_id\"].tolist()\n",
    "\n",
    "@retry(\n",
    "    stop=stop_after_attempt(3),\n",
    "    wait=wait_exponential(multiplier=1, min=2, max=10),\n",
    "    retry=retry_if_exception_type(requests.RequestException),\n",
    ")\n",
    "def check_status_of_ingestion(transformation_id: str, file_id: str) -> dict:\n",
    "    \"\"\"Query the status of the ingestion for a given transformation and file_id.\"\"\"\n",
    "    dataplatform_base_url = DATA_PLATFORM_URL\n",
    "    url = f\"{dataplatform_base_url}/transformations/{transformation_id}/runs?file_id={file_id}\"\n",
    "\n",
    "    token = TOKEN\n",
    "    response = requests.get(\n",
    "        url=url, headers={\"Authorization\": f\"Bearer {token}\"}, verify=False\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    page = response.json()\n",
    "\n",
    "    assert page[\"total\"] > 0\n",
    "    return page[\"runs\"][0]\n",
    "\n",
    "def get_transformation_id(name: str) -> str:\n",
    "    \"\"\"Get the transformation ID from the Data Platform.\"\"\"\n",
    "    dataplatform_base_url = DATA_PLATFORM_URL\n",
    "    url = f\"{dataplatform_base_url}/transformations?name={name}\"\n",
    "\n",
    "    token = TOKEN\n",
    "    response = requests.get(\n",
    "        url=url, headers={\"Authorization\": f\"Bearer {token}\"}, verify=False\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    page = response.json()\n",
    "\n",
    "    assert page[\"total\"] > 0\n",
    "    return page[\"transformations\"][0][\"transformationId\"]\n",
    "\n",
    "def check_files_status(transformation_id: str, df: pd.DataFrame, max_workers: int = 3):\n",
    "    \"\"\"Check the status of ingested files and store the results in a DataFrame.\"\"\"\n",
    "\n",
    "    successful_file_ids = get_successful_document_ids(df)\n",
    "    status_results = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_file = {\n",
    "            executor.submit(\n",
    "                check_status_of_ingestion, transformation_id, file_id\n",
    "            ): file_id\n",
    "            for file_id in successful_file_ids\n",
    "        }\n",
    "\n",
    "        for future in concurrent.futures.as_completed(future_to_file):\n",
    "            file_id = future_to_file[future]\n",
    "            try:\n",
    "                run = future.result()\n",
    "                output = json.dumps(run.get(\"output\", {}), indent=4)\n",
    "                status_results.append(\n",
    "                    {\n",
    "                        \"file_id\": file_id,\n",
    "                        \"run_id\": run[\"runId\"],\n",
    "                        \"status\": run[\"status\"],\n",
    "                        \"output\": output,\n",
    "                        \"error\": run[\"errors\"],\n",
    "                    }\n",
    "                )\n",
    "            except Exception as e:\n",
    "                status_results.append(\n",
    "                    {\n",
    "                        \"file_id\": file_id,\n",
    "                        \"status\": run[\"status\"],\n",
    "                        \"output\": None,\n",
    "                        \"error\": str(e),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return df.merge(\n",
    "        pd.DataFrame(status_results),\n",
    "        on=\"file_id\",\n",
    "        how=\"left\",\n",
    "        suffixes=(\"_ingestion\", \"\"),\n",
    "    )\n",
    "\n",
    "def get_successful_dataset_ids(df: pd.DataFrame) -> list:\n",
    "    \"\"\"Retrieve a list of successful dataset_ids from the DataFrame.\"\"\"\n",
    "    dataset_ids_list = []\n",
    "    for i in range(len(df)):\n",
    "        dataset_ids_list.append(json.loads(df[\"output\"][i]).get(\"datasetId\"))\n",
    "    return dataset_ids_list\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"\\033[94m\u2139\ufe0f  INFO: Status monitoring functions ready. Invoke these in the next cell.\\033[0m\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[93m\u2699\ufe0f  PROCESSING: Checking transformation status...\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[92m\u2705 SUCCESS: All 4 files completed processing\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>file_id</th>\n",
       "      <th>status_ingestion</th>\n",
       "      <th>error_type</th>\n",
       "      <th>error_message</th>\n",
       "      <th>run_id</th>\n",
       "      <th>status</th>\n",
       "      <th>output</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>files_to_upload/What is RAG_ - Retrieval-Augme...</td>\n",
       "      <td>d6c7e20c-c1f6-4d6e-98d1-6eec7a1f7f6e</td>\n",
       "      <td>Success</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>5cc615ab-edb9-447d-8c15-274af270e475</td>\n",
       "      <td>completed</td>\n",
       "      <td>{\\n    \"type\": \"DataPlatform:Repository:Datase...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>files_to_upload/Azure Cognitive Search_ Outper...</td>\n",
       "      <td>66e69a6f-d62c-45c0-a2b4-9be40426a5cb</td>\n",
       "      <td>Success</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>cfeac8ed-e676-4677-a887-9920a0aa3848</td>\n",
       "      <td>completed</td>\n",
       "      <td>{\\n    \"type\": \"DataPlatform:Repository:Datase...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>files_to_upload/RAG.pdf</td>\n",
       "      <td>83613d23-831c-4a80-8c1f-e5f298b68d13</td>\n",
       "      <td>Success</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>cdccb8ed-6e21-40c5-be99-bd9348230d9f</td>\n",
       "      <td>completed</td>\n",
       "      <td>{\\n    \"type\": \"DataPlatform:Repository:Datase...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>files_to_upload/Attention is all you need.pdf</td>\n",
       "      <td>d679c61f-28b1-4bb9-b8ce-3dda3b3001a7</td>\n",
       "      <td>Success</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>eb869981-4ac8-45f7-a2af-f71a8162966d</td>\n",
       "      <td>completed</td>\n",
       "      <td>{\\n    \"type\": \"DataPlatform:Repository:Datase...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_path  \\\n",
       "0  files_to_upload/What is RAG_ - Retrieval-Augme...   \n",
       "1  files_to_upload/Azure Cognitive Search_ Outper...   \n",
       "2                            files_to_upload/RAG.pdf   \n",
       "3      files_to_upload/Attention is all you need.pdf   \n",
       "\n",
       "                                file_id status_ingestion error_type  \\\n",
       "0  d6c7e20c-c1f6-4d6e-98d1-6eec7a1f7f6e          Success       None   \n",
       "1  66e69a6f-d62c-45c0-a2b4-9be40426a5cb          Success       None   \n",
       "2  83613d23-831c-4a80-8c1f-e5f298b68d13          Success       None   \n",
       "3  d679c61f-28b1-4bb9-b8ce-3dda3b3001a7          Success       None   \n",
       "\n",
       "  error_message                                run_id     status  \\\n",
       "0          None  5cc615ab-edb9-447d-8c15-274af270e475  completed   \n",
       "1          None  cfeac8ed-e676-4677-a887-9920a0aa3848  completed   \n",
       "2          None  cdccb8ed-6e21-40c5-be99-bd9348230d9f  completed   \n",
       "3          None  eb869981-4ac8-45f7-a2af-f71a8162966d  completed   \n",
       "\n",
       "                                              output error  \n",
       "0  {\\n    \"type\": \"DataPlatform:Repository:Datase...  None  \n",
       "1  {\\n    \"type\": \"DataPlatform:Repository:Datase...  None  \n",
       "2  {\\n    \"type\": \"DataPlatform:Repository:Datase...  None  \n",
       "3  {\\n    \"type\": \"DataPlatform:Repository:Datase...  None  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n\")\n",
    "print(\"\\033[93m\u2699\ufe0f  PROCESSING: Checking transformation status...\\033[0m\")\n",
    "print(\"\\n\")\n",
    "\n",
    "transformation_id = get_transformation_id(TRANSFORMATION_NAME)\n",
    "status_df = check_files_status(transformation_id, df_results)\n",
    "status_df.to_csv(\"ingestion_status.csv\", index=False)\n",
    "successful_dataset_ids = get_successful_dataset_ids(status_df[status_df[\"status\"] == \"completed\"])\n",
    "\n",
    "completed_count = len(status_df[status_df['status'] == 'completed'])\n",
    "failed_count = len(status_df[status_df['status'] != 'completed'])\n",
    "\n",
    "print(\"\\n\")\n",
    "if failed_count == 0:\n",
    "    print(f\"\\033[92m\u2705 SUCCESS: All {completed_count} files completed processing\\033[0m\")\n",
    "else:\n",
    "    print(f\"\\033[91m\u26a0\ufe0f  WARNING: {completed_count} files completed, {failed_count} files failed or pending\\033[0m\")\n",
    "print(\"\\n\")\n",
    "    \n",
    "status_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Interact with processed documents\n",
    "\n",
    "As done in the previous tutorial, it is possible to use PhariaSearch (previously Document Index) to perform any type of search on the collection where we uploaded the document. For example, let's use our semantic search index to check the newly uploaded documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SearchResult(id=DocumentPath(collection_path=CollectionPath(namespace='Studio', collection='pharia-tutorial-rag-vsp-1'), document_name='59a9bab5-4d73-44ce-bb63-5d0ddafb89d3'), score=0.66157854, document_chunk=DocumentChunk(text='Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.', start=5535, end=5712, metadata=None)),\n",
       " SearchResult(id=DocumentPath(collection_path=CollectionPath(namespace='Studio', collection='pharia-tutorial-rag-vsp-1'), document_name='e85c6f06-1727-462d-977d-78b7a9311d6a'), score=0.66157854, document_chunk=DocumentChunk(text='Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.', start=5535, end=5712, metadata=None)),\n",
       " SearchResult(id=DocumentPath(collection_path=CollectionPath(namespace='Studio', collection='pharia-tutorial-rag-vsp-1'), document_name='772194d8-cbc2-4b5b-8279-53a3a78ce6f9'), score=0.66147536, document_chunk=DocumentChunk(text='Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.', start=5535, end=5712, metadata=None)),\n",
       " SearchResult(id=DocumentPath(collection_path=CollectionPath(namespace='Studio', collection='pharia-tutorial-rag-vsp-1'), document_name='2f6227af-a302-4c09-8b7c-f4578c8923f3'), score=0.66147536, document_chunk=DocumentChunk(text='Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.', start=5535, end=5712, metadata=None)),\n",
       " SearchResult(id=DocumentPath(collection_path=CollectionPath(namespace='Studio', collection='pharia-tutorial-rag-vsp-1'), document_name='7de91fde-1f52-4624-b128-601607c7b6e5'), score=0.66146505, document_chunk=DocumentChunk(text='Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.', start=5535, end=5712, metadata=None))]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pharia_data_sdk.connectors import DocumentIndexClient\n",
    "from pharia_data_sdk.connectors.retrievers import DocumentIndexRetriever\n",
    "\n",
    "search_client = DocumentIndexClient(\n",
    "    token=TOKEN,\n",
    "    base_url=DOCUMENT_INDEX_API_URL,\n",
    ")\n",
    "\n",
    "document_index_retriever = DocumentIndexRetriever(\n",
    "    document_index=search_client,\n",
    "    index_name=INDEX,\n",
    "    namespace=NAMESPACE,\n",
    "    collection=COLLECTION,\n",
    "    k=5,\n",
    ")\n",
    "\n",
    "document_index_retriever.get_relevant_documents_with_scores(\n",
    "    query=\"what is attention?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this section, you successfully set up the complete document ingestion pipeline to be able to ingest RAW file and index them with PhariaSearch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}