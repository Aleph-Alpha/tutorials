{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started - Evaluation Setup\n",
    "\n",
    "Welcome to the evaluation phase of the PhariaAI tutorial! This guide will help you set up your environment to evaluate and improve the quality of your RAG applications using PhariaStudio's evaluation framework.\n",
    "\n",
    "## What you'll learn\n",
    "\n",
    "This tutorial will show you how to:\n",
    "- Set up your environment for evaluation workflows\n",
    "- Install the required PhariaStudio and PhariaInference SDKs\n",
    "- Configure authentication and project settings\n",
    "- Prepare for both simple keyword-based and advanced LLM-as-a-judge evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites and Setup\n",
    "\n",
    "### Accessing Your Virtual Environment\n",
    "\n",
    "If you completed the previous tutorials, you should already have a virtual environment set up. Simply activate it:\n",
    "\n",
    "**On macOS/Linux:**\n",
    "```bash\n",
    "source .venv/bin/activate\n",
    "```\n",
    "\n",
    "**On Windows:**\n",
    "```bash\n",
    ".venv\\Scripts\\activate\n",
    "```\n",
    "\n",
    "### Required Permissions\n",
    "\n",
    "Before starting, ensure you have the following access permissions:\n",
    "\n",
    "| Permission | What it provides |\n",
    "|------------|------------------|\n",
    "| **StudioUser** | • Access to PhariaStudio<br>• The \"Studio\" namespace in PhariaDocument Index for testing |\n",
    "| **Valid PhariaAI token** | • Authentication for PhariaStudio and PhariaInference services |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Software Installation\n",
    "\n",
    "### Installing Evaluation Dependencies\n",
    "\n",
    "If you haven't yet installed these from previous tutorials, add these essential packages for evaluation workflows now:\n",
    "\n",
    "```bash\n",
    "uv pip install pharia-studio-sdk\n",
    "uv pip install pharia-inference-sdk\n",
    "uv pip install pharia-skill\n",
    "uv pip install jinja2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication and Configuration\n",
    "\n",
    "### Environment Variables Setup\n",
    "\n",
    "Create a `.env` file in your project directory with the following variables:\n",
    "\n",
    "```bash\n",
    "PHARIA_AI_TOKEN=your_token_here\n",
    "PHARIA_STUDIO_PROJECT_NAME=your_project_name\n",
    "PHARIA_STUDIO_ADDRESS=your_studio_address\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Approaches Available\n",
    "\n",
    "### 1. Simple Evaluation (Keyword Matching)\n",
    "\n",
    "**Best for:** Quick validation and basic quality checks\n",
    "- Keyword-based matching against expected content\n",
    "- Pass/fail scoring based on presence of required terms\n",
    "- Fast execution and easy to understand results\n",
    "\n",
    "### 2. Advanced Evaluation (LLM-as-a-Judge)\n",
    "\n",
    "**Best for:** Comprehensive quality assessment\n",
    "- Multi-dimensional scoring (accuracy, factuality, completeness)\n",
    "- LLM-powered evaluation using sophisticated prompts\n",
    "- Source citation validation\n",
    "- More nuanced and reliable quality metrics\n",
    "\n",
    "### Getting Started with Evaluations\n",
    "\n",
    "1. **Start with Simple Evaluation** if you're new to evaluation frameworks\n",
    "2. **Progress to Advanced Evaluation** for production-ready assessment\n",
    "3. **Use both approaches** for comprehensive validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Notes\n",
    "\n",
    "### Interactive Jupyter Experience\n",
    "\n",
    "All evaluation interactions can be executed directly within the Jupyter notebooks provided. The evaluation framework handles:\n",
    "- Automatic connection to PhariaStudio services\n",
    "- Dataset creation and management\n",
    "- Benchmark execution and result storage\n",
    "- Detailed reporting and analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
