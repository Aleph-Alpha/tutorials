{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Evaluation - Simple implementation\n",
    "<a id=\"evaluation1\"></a>\n",
    "\n",
    "This section explains how to use PhariaStudio SDK to build an evaluation setup that helps improving the quality of the AI logic. It focuses on the setup and it is on purpose simpler than a real scenario.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>Note:</b> This tutorial is run entirely on this Jupyter Notebook.\n",
    "</div>\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting, ensure you have followed the [**\"0. Getting Started\"**](./0.%20Getting%20Started.ipynb) step of this evaluation section. Further the QA skill available in this folder uses a specific collection, make sure that it is available in the testing environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The collection used by the QA skill, if not available, please edit the qa.py file to use a different collection\n",
    "NAMESPACE = \"Studio\"\n",
    "COLLECTION = \"papers\"\n",
    "INDEX = \"asym-64\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now import the necessary libraries and set up your environment. We start by importing components from the Intelligence Layer framework that will help us create and run our evaluations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from os import getenv\n",
    "from pydantic import BaseModel\n",
    "from collections.abc import Iterable\n",
    "from typing import Iterable\n",
    "from statistics import mean\n",
    "from uuid import uuid4\n",
    "\n",
    "from pharia_inference_sdk.core import NoOpTracer, Task, TaskSpan\n",
    "from pharia_skill.testing import DevCsi\n",
    "from pharia_studio_sdk import StudioClient\n",
    "from pharia_studio_sdk.evaluation import (Example,\n",
    "                                        SingleOutputEvaluationLogic,\n",
    "                                        StudioBenchmarkRepository,\n",
    "                                        StudioDatasetRepository,\n",
    "                                        AggregationLogic)\n",
    "\n",
    "# Please use the qa.py file provided in the tutorial folder\n",
    "from qa import Input, Output, custom_rag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure\n",
    "\n",
    "### 1. Connect to PhariaStudio\n",
    "\n",
    "First, we need to establish a connection to PhariaStudio, which will be used to store our evaluation datasets, benchmarks, and traces. The StudioClient provides an interface for creating and managing these resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "PHARIA_AI_TOKEN = getenv(\"PHARIA_AI_TOKEN\")\n",
    "PHARIA_STUDIO_PROJECT_NAME = getenv(\"PHARIA_STUDIO_PROJECT_NAME\")\n",
    "PHARIA_STUDIO_ADDRESS = getenv(\"PHARIA_STUDIO_ADDRESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "studio_client = StudioClient(\n",
    "    project=PHARIA_STUDIO_PROJECT_NAME,\n",
    "    studio_url=PHARIA_STUDIO_ADDRESS,\n",
    "    auth_token=PHARIA_AI_TOKEN,\n",
    "    create_project=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a task wrapper for your RAG Skill\n",
    "\n",
    "To evaluate our RAG Skill, we need to wrap it in a PhariaInference SDK task. This wrapper serves as an adapter between your Skill implementation and the evaluation framework. As the evaluation framework runs locally, we can simply use DevCSI to produce the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pharia_skill.testing import DevCsi\n",
    "\n",
    "class QATask(Task[Input, Output]):\n",
    "\n",
    "    def do_run(self, input: Input, task_span: TaskSpan) -> Output:\n",
    "        # If you want to enable tracing, uncomment the following line\n",
    "        # This triggers double tracing when executing benchmarks\n",
    "        #csi = DevCsi(project=PHARIA_STUDIO_PROJECT_NAME) \n",
    "        csi = DevCsi()\n",
    "        return custom_rag(csi, input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, verify that your task wrapper correctly interfaces with the deployed Skill:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = Input(question=\"What is an encoder?\")\n",
    "\n",
    "task = QATask()\n",
    "task.run(test_input, NoOpTracer())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create an evaluation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Create a test dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create an example test dataset with questions that cover different topics in our document collection. For each question, we specify keywords that should appear in a well-informed answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = [\n",
    "    {\n",
    "        \"question\": \"What is mixture-of-experts?\",\n",
    "        \"keywords\": [\"experts\", \"gating\", \"combine\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is an Large Language Model\",\n",
    "        \"keywords\": [\"corpus\", \"parameters\", \"generation\"]\n",
    "    },\n",
    "    {   \n",
    "        \"question\": \"What is a Sequence?\", \n",
    "        \"keywords\": [\"order\", \"elements\", \"series\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is translation?\",\n",
    "        \"keywords\": [\"language\", \"meaning\", \"convert\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the difference between GRNN and RNN?\",\n",
    "        \"keywords\": [\"gates\", \"general\", \"specific\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is LSTM?\", \n",
    "        \"keywords\": [\"memory\", \"gates\", \"vanishing\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is are RNNs?\", \n",
    "        \"keywords\": [\"feedback\", \"sequential\", \"state\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is self-attention?\",\n",
    "        \"keywords\": [\"positions\", \"sequence\", \"relate\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is Attention?\", \n",
    "        \"keywords\": [\"focus\", \"weighting\", \"context\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is a transformer?\",\n",
    "        \"keywords\": [\"attention\", \"parallel\", \"encoder\"]\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Pydantic model for expected output\n",
    "\n",
    "Next, we define a Pydantic model to establish the structure for test output, to ensure type safety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationExpectedOutput(BaseModel):\n",
    "    keywords: list[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Upload the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "studio_dataset_repo = StudioDatasetRepository(studio_client=studio_client)\n",
    "\n",
    "examples = [\n",
    "    Example(\n",
    "        input=Input(question=example[\"question\"]),\n",
    "        expected_output=EvaluationExpectedOutput(keywords=example[\"keywords\"]),\n",
    "    )\n",
    "    for example in test_set\n",
    "]\n",
    "\n",
    "studio_dataset = studio_dataset_repo.create_dataset(\n",
    "    examples=examples, dataset_name=\"demo-dataset\"\n",
    ")\n",
    "\n",
    "studio_dataset.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access the dataset, follow the tutorial [Store an evaluation dataset in PhariaStudio](https://docs.aleph-alpha.com/products/pharia-ai/pharia-studio/tutorial/store-dataset-in-data-platform/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define evaluation logic\n",
    "\n",
    "PhariaStudio SDK requires the creation of `EvaluationLogic` - to evaulate individual examples - and `AggregationLogic` - to aggregate all the individual evaluations into overall metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 EvaluationLogic \n",
    "\n",
    "First, we set up the evaluation logic that is used for each individual example. Our `QaEvaluationLogic` class implements this assessment strategy by extending the Intelligence Layer's `SingleOutputEvaluationLogic` interface, allowing it to integrate with the broader evaluation framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QaEvaluation(BaseModel):\n",
    "    matched_keywords: list[str]\n",
    "    missing_keywords: list[str]\n",
    "    match_score: float\n",
    "    passed: bool\n",
    "\n",
    "\n",
    "class QaEvaluationLogic(\n",
    "    SingleOutputEvaluationLogic[Input, Output, EvaluationExpectedOutput, QaEvaluation]\n",
    "):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.threshold = 0.5  ## Threshold to define when an evaluation is passed\n",
    "\n",
    "    def do_evaluate_single_output(\n",
    "        self, example: Example[Input, EvaluationExpectedOutput], output: Output\n",
    "    ) -> QaEvaluation:\n",
    "        required_keywords = example.expected_output.keywords\n",
    "        if output.answer is None:\n",
    "            output.answer = \"\"\n",
    "        \n",
    "        output_text = output.answer.lower()\n",
    "\n",
    "        matched_keywords = []\n",
    "        missing_keywords = []\n",
    "\n",
    "        for keyword in required_keywords:\n",
    "            if keyword.lower() in output_text:\n",
    "                matched_keywords.append(keyword)\n",
    "            else:\n",
    "                missing_keywords.append(keyword)\n",
    "\n",
    "        match_score = (\n",
    "            len(matched_keywords) / len(required_keywords) if required_keywords else 1.0\n",
    "        )\n",
    "\n",
    "        passed = match_score >= self.threshold\n",
    "\n",
    "        return QaEvaluation(\n",
    "            matched_keywords=matched_keywords,\n",
    "            missing_keywords=missing_keywords,\n",
    "            match_score=match_score,\n",
    "            passed=passed,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 AggregationLogic\n",
    "\n",
    "To assess overall system performance, we need to aggregate individual evaluation results into meaningful metrics. This is defined in the `QaAggregationLogic` class:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QaAggregatedEvaluation(BaseModel):\n",
    "    pass_rate: float\n",
    "    average_match_score: float\n",
    "\n",
    "\n",
    "class QaAggregationLogic(\n",
    "    AggregationLogic[\n",
    "        QaEvaluation,\n",
    "        QaAggregatedEvaluation,\n",
    "    ]\n",
    "):\n",
    "    def aggregate(self, evaluations: Iterable[QaEvaluation]) -> QaAggregatedEvaluation:\n",
    "        evaluation_list = list(evaluations)\n",
    "        if len(evaluation_list) == 0:\n",
    "            return QaAggregatedEvaluation(\n",
    "                pass_rate=0.0,\n",
    "                average_match_score=0.0,\n",
    "            )\n",
    "\n",
    "        passed_count = sum(1 for eval in evaluation_list if eval.passed)\n",
    "        pass_rate = passed_count / len(evaluation_list)\n",
    "\n",
    "        average_match_score = mean(eval.match_score for eval in evaluation_list)\n",
    "\n",
    "        return QaAggregatedEvaluation(\n",
    "            pass_rate=pass_rate,\n",
    "            average_match_score=average_match_score,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create and run a benchmark\n",
    "\n",
    "With our evaluation components ready, we can now create a benchmark in PhariaStudio and run our evaluation on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_repository = StudioBenchmarkRepository(studio_client=studio_client)\n",
    "evaluation_logic = QaEvaluationLogic()\n",
    "aggregation_logic = QaAggregationLogic()\n",
    "\n",
    "benchmark = benchmark_repository.create_benchmark(\n",
    "    dataset_id=studio_dataset.id,\n",
    "    eval_logic=evaluation_logic,\n",
    "    aggregation_logic=aggregation_logic,\n",
    "    name=\"keyword-matching-benchmark\", # Benchmark name needs to be unique\n",
    "    description=\"This benchmark evaluates the keyword matching between the model's output and the expected output.\",\n",
    ")\n",
    "\n",
    "benchmark.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we trigger the becnhmark to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_repository = StudioBenchmarkRepository(studio_client=studio_client)\n",
    "benchmark = benchmark_repository.get_benchmark(\n",
    "    benchmark_id=benchmark.id,\n",
    "    eval_logic=evaluation_logic,\n",
    "    aggregation_logic=aggregation_logic,\n",
    ")\n",
    "\n",
    "benchmark_execution_id = benchmark.execute(\n",
    "    task=task,\n",
    "    name=str(uuid4()),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the benchmark completes, you can view detailed results in the PhariaStudio interface under Evaluate/Benchmarks (check [Create and submit evaluations](https://docs.aleph-alpha.com/products/pharia-ai/pharia-studio/tutorial/write-a-simple-evaluation/) for more details)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improving your RAG application\n",
    "\n",
    "Based on the evaluation results, you can identify areas for improvement in your RAG application. Common improvements include:\n",
    "\n",
    "1. **Refining the prompt**: Adjust the prompt to encourage more precise reference citation\n",
    "2. **Adjusting retrieval parameters**: Modify the number of retrieved documents or relevance thresholds\n",
    "3. **Enhancing document chunking**: Change how documents are split and indexed\n",
    "4. **Implementing better ranking**: Add reranking steps to prioritise the most relevant documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-filippo",
   "language": "python",
   "name": "test-filippo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
