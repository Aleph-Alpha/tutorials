{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Evaluation - Advanced implementation using LLM-as-a-Judge\n",
    "<a id=\"evaluation2\"></a>\n",
    "\n",
    "This tutorial shows how to implement complex evaluation setup using also LLM-as-a-Judge setup. It implies using some complex helper functions and therefore requires advanced knowledge of python. If you haven't followed the [Simple Evaluation](\"./1.%20Evaluation%20%28Simple%29%20-%20Testing%20Your%20RAG%20Application.ipynb\") tutorial, we advice to first run through that and after use this one for a more complicated setup.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>Note:</b> This tutorial is run entirely on this Jupyter Notebook.\n",
    "</div>\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting, ensure you have followed the [**\"0. Getting Started\"**](./0.%20Getting%20Started.ipynb) step of this evaluation section. Further the QA skill available in this folder uses a specific collection, make sure that it is available in the testing environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The collection used by the QA skill, if not available, please edit the qa.py file to use a different collection\n",
    "NAMESPACE = \"Studio\"\n",
    "COLLECTION = \"papers\"\n",
    "INDEX = \"asym-64\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now import the necessary libraries and set up your environment. We start by importing components from the Intelligence Layer framework that will help us create and run our evaluations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from os import getenv\n",
    "from pharia_skill import ChatParams, Message\n",
    "from pydantic import BaseModel\n",
    "from collections.abc import Iterable\n",
    "from typing import Iterable\n",
    "from statistics import mean\n",
    "from uuid import uuid4\n",
    "\n",
    "from pharia_studio_sdk import StudioClient\n",
    "from pharia_inference_sdk.core import NoOpTracer, Task, TaskSpan\n",
    "from pharia_studio_sdk.evaluation import (\n",
    "    Example,\n",
    "    SingleOutputEvaluationLogic,\n",
    "    StudioBenchmarkRepository,\n",
    "    StudioDatasetRepository,\n",
    "    AggregationLogic,\n",
    ")\n",
    "\n",
    "from qa import Input, Output, custom_rag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure\n",
    "\n",
    "### 1. Connect to PhariaStudio\n",
    "\n",
    "First, we need to establish a connection to PhariaStudio, which will be used to store our evaluation datasets, benchmarks, and traces. The StudioClient provides an interface for creating and managing these resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "PHARIA_AI_TOKEN = getenv(\"PHARIA_AI_TOKEN\")\n",
    "PHARIA_STUDIO_PROJECT_NAME = getenv(\"PHARIA_STUDIO_PROJECT_NAME\")\n",
    "PHARIA_STUDIO_ADDRESS = getenv(\"PHARIA_STUDIO_ADDRESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "studio_client = StudioClient(\n",
    "    project=PHARIA_STUDIO_PROJECT_NAME,\n",
    "    studio_url=PHARIA_STUDIO_ADDRESS,\n",
    "    auth_token=PHARIA_AI_TOKEN,\n",
    "    create_project=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a task wrapper for your RAG Skill\n",
    "\n",
    "To evaluate our RAG Skill, we need to wrap it in a PhariaInference SDK task. This wrapper serves as an adapter between your Skill implementation and the evaluation framework. As the evaluation framework runs locally, we can simply use DevCSI to produce the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pharia_skill.testing import DevCsi\n",
    "\n",
    "\n",
    "class QATask(Task[Input, Output]):\n",
    "\n",
    "    def do_run(self, input: Input, task_span: TaskSpan) -> Output:\n",
    "        # If you want to enable tracing, uncomment the following line\n",
    "        # This triggers double tracing when executing benchmarks\n",
    "        # csi = DevCsi(project=PHARIA_STUDIO_PROJECT_NAME)\n",
    "        csi = DevCsi()\n",
    "        return custom_rag(csi, input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, verify that your task wrapper correctly interfaces with the deployed Skill:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = Input(question=\"What is a transformer?\")\n",
    "\n",
    "task = QATask()\n",
    "task.run(test_input, NoOpTracer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create an evaluation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Create a test dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create an example test dataset matching questions covering different topics to their respective expected answers. For each question, we effectively create a Ground Truth against which we can judge the LLMs responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = [\n",
    "    {\n",
    "        \"question\": \"What is mixture-of-experts?\",\n",
    "        \"generated_answer\": \"\"\"## SUMMARY: \n",
    "Mixture-of-experts (MoE) is a neural network architecture that involves multiple expert networks, each specializing in a different subset of the input data, and a gating network that determines the weights for combining the outputs of these experts. This approach allows for more efficient and flexible modeling of complex data distributions by leveraging the strengths of individual experts.\n",
    "\n",
    "## DETAILS: \n",
    "The mixture-of-experts (MoE) model is designed to improve the performance and efficiency of neural networks, especially in scenarios where the data is complex, heterogeneous, or has varying densities. The core idea behind MoE is to have a collection of \"expert\" networks, where each expert is a neural network that specializes in a specific part of the input data space. Instead of having a single, large network trying to model the entire data distribution, MoE divides the workload among these experts, allowing each to become highly proficient in its assigned domain.\n",
    "\n",
    "A critical component of the MoE architecture is the \"gating network.\" The gating network's role is to determine, for each input, which of the expert networks is most likely to produce the correct output. It does this by computing a set of weights, where each weight corresponds to the relevance of an expert to the current input. These weights are then used to combine the outputs of the expert networks, effectively creating a weighted average of their predictions. The gating network learns to assign higher weights to experts that are more likely to be correct for a given input, and lower weights to those that are less relevant.\n",
    "\n",
    "The MoE model is trained end-to-end, meaning that both the expert networks and the gating network are updated simultaneously during the training process. This allows the model to learn not only the parameters of the expert networks but also how to effectively route inputs to the appropriate experts. The training process typically involves minimizing a loss function that measures the difference between the model's predictions and the true outputs.\n",
    "\n",
    "One of the key benefits of the MoE approach is its ability to handle complex, heterogeneous data more effectively than traditional neural network architectures. By allowing different parts of the model to specialize in different aspects of the data, MoE can capture a wider range of patterns and relationships. Additionally, MoE can be more computationally efficient than larger, monolithic models, as only the relevant experts need to be activated for a given input, reducing the computational workload.\n",
    "\n",
    "However, the MoE model also introduces additional complexity, such as the need to determine the optimal number of experts and the architecture of both the expert and gating networks. Furthermore, training MoE models can be challenging due to the complex interactions between the gating network and the expert networks, requiring careful tuning of hyperparameters and training procedures. Despite these challenges, the mixture-of-experts model has shown promising results in various applications, including natural language processing, computer vision, and recommender systems, making it a valuable tool in the deep learning toolkit.\n",
    "\"\"\",\n",
    "        \"sources\": [\"Attention Is All You Need\"],\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is an Large Language Model?\",\n",
    "        \"generated_answer\": \"\"\"## SUMMARY: \n",
    "A Large Language Model (LLM) is a type of artificial intelligence (AI) model designed to process and understand human language, typically trained on vast amounts of text data to learn patterns, relationships, and structures of language. These models are capable of generating coherent and contextually relevant text, answering questions, translating languages, and even creating content, making them a significant advancement in natural language processing (NLP).\n",
    "\n",
    "## DETAILS: \n",
    "Large Language Models are a subset of deep learning models that are specifically designed to handle the complexities and nuances of human language. They are trained on massive datasets of text, which can include books, articles, research papers, websites, and even social media posts. The primary goal of an LLM is to learn the statistical patterns and relationships within language, allowing it to predict the next word in a sequence, given the context of the previous words. This capability enables LLMs to perform a wide range of tasks, including but not limited to:\n",
    "\n",
    "1. **Text Generation:** LLMs can create text that is often indistinguishable from that written by humans. This capability is used in applications such as content creation, chatbots, and automated writing tools.\n",
    "2. **Language Translation:** By understanding the patterns and structures of different languages, LLMs can translate text from one language to another with a high degree of accuracy.\n",
    "3. **Question Answering:** LLMs can be fine-tuned to answer questions based on the information they have been trained on, making them useful for creating virtual assistants and question-answering systems.\n",
    "4. **Text Summarization:** These models can summarize long pieces of text into shorter, more digestible versions, highlighting the key points and main ideas.\n",
    "5. **Sentiment Analysis:** LLMs can analyze text to determine the sentiment or emotional tone behind it, which is useful in applications such as customer service and social media monitoring.\n",
    "\n",
    "The training of LLMs involves several key steps and technologies:\n",
    "\n",
    "1. **Data Collection:** Gathering a large, diverse dataset of text.\n",
    "2. **Model Architecture:** Designing the model's architecture, which often involves transformer models due to their effectiveness in handling sequential data like text.\n",
    "3. **Training:** Using powerful computing resources to train the model on the collected data, which can take weeks, months, or even years.\n",
    "4. **Fine-Tuning:** After initial training, the model can be fine-tuned for specific tasks by training it on smaller, task-specific datasets.\n",
    "\n",
    "Examples of Large Language Models include transformer-based models like BERT (Bidirectional Encoder Representations from Transformers), RoBERTa (Robustly optimized BERT approach), and more recently, models like LLaMA (Large Language Model Meta AI) and PaLM (Pathways Language Model). These models have achieved state-of-the-art results in various NLP tasks and have been integrated into numerous applications and services.\n",
    "\n",
    "However, LLMs also come with challenges and limitations, such as requiring significant computational resources for training and deployment, potential biases inherited from the training data, and ethical considerations regarding privacy, misinformation, and the potential for generating harmful content. Despite these challenges, the development and application of Large Language Models continue to advance, promising to revolutionize how we interact with information and each other through language.\n",
    "\"\"\",\n",
    "        \"sources\": [\"Attention Is All You Need\"],\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is a Sequence?\",\n",
    "        \"generated_answer\": \"\"\"## SUMMARY: \n",
    "According to the paper \"Attention Is All You Need\" by Vaswani et al., a sequence refers to a list of tokens, such as words or characters, that are processed in a specific order. In the context of the Transformer model introduced in the paper, sequences are the primary input and output data structure, where each sequence represents a sentence, phrase, or document.\n",
    "\n",
    "## DETAILS: \n",
    "In the \"Attention Is All You Need\" paper, the authors propose the Transformer model, which relies heavily on the concept of sequences to process input data. According to the paper, a sequence is defined as a list of tokens, where each token can be a word, character, or subword (a subunit of a word). The sequence is the fundamental data structure used to represent input and output data in the Transformer model.\n",
    "\n",
    "The Transformer model is designed to handle sequences of varying lengths, making it particularly well-suited for natural language processing tasks such as machine translation, text classification, and language modeling. The model's architecture is based on self-attention mechanisms, which allow it to weigh the importance of different tokens in the input sequence relative to each other.\n",
    "\n",
    "In the context of the Transformer model, sequences are processed as follows:\n",
    "\n",
    "1. **Tokenization:** The input text is broken down into individual tokens, such as words or subwords.\n",
    "2. **Embedding:** Each token is embedded into a vector space, where semantically similar tokens are mapped to nearby points.\n",
    "3. **Positional Encoding:** The embedded tokens are augmented with positional encodings, which capture the order and position of each token in the sequence.\n",
    "4. **Self-Attention:** The model applies self-attention mechanisms to the input sequence, allowing it to attend to different tokens and weigh their importance relative to each other.\n",
    "5. **Output:** The final output is generated based on the attended tokens, which can be used for tasks such as translation, classification, or language modeling.\n",
    "\n",
    "The use of sequences as the primary data structure in the Transformer model allows for efficient and parallelizable processing of input data, making it particularly well-suited for large-scale natural language processing tasks. The sequence-based architecture of the Transformer model has been widely adopted and has achieved state-of-the-art results in many NLP tasks.\n",
    "\n",
    "In the paper, the authors also introduce several key concepts related to sequences, including:\n",
    "\n",
    "* **Sequence length:** The number of tokens in the input sequence.\n",
    "* **Token embedding:** The vector representation of each token in the sequence.\n",
    "* **Positional encoding:** The augmentation of token embeddings with positional information to capture the order and position of each token.\n",
    "* **Self-attention:** The mechanism by which the model attends to different tokens in the sequence and weighs their importance relative to each other.\n",
    "\n",
    "Overall, the concept of sequences plays a central role in the \"Attention Is All You Need\" paper, enabling the development of the\n",
    "\"\"\",\n",
    "        \"sources\": [\"Attention Is All You Need\"],\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is translation?\",\n",
    "        \"generated_answer\": \"\"\"## SUMMARY: \n",
    "In the context of the \"Attention Is All You Need\" paper and natural language processing (NLP), translation refers to the task of converting text from one language to another, while preserving the original meaning, context, and content. This involves using machine learning models, such as the Transformer, to learn the patterns and relationships between languages and generate accurate translations.\n",
    "\n",
    "## DETAILS: \n",
    "Machine translation is a fundamental task in NLP, aiming to automatically translate text from a source language to a target language. The goal is to produce a translation that is not only grammatically correct but also conveys the same meaning, tone, and nuance as the original text.\n",
    "\n",
    "In the context of the Transformer model, translation is a sequence-to-sequence task, where the input sequence is the text in the source language, and the output sequence is the translated text in the target language. The model learns to map the input sequence to the output sequence through a process of encoding, attending, and decoding.\n",
    "\n",
    "The translation process involves several key steps:\n",
    "\n",
    "1. **Encoding:** The source language text is encoded into a continuous representation, capturing the semantic meaning and context of the input sequence.\n",
    "2. **Attention:** The model attends to different parts of the input sequence, weighing their importance and relevance to the translation task.\n",
    "3. **Decoding:** The model generates the translated text, one token at a time, based on the encoded input sequence and the attention weights.\n",
    "4. **Output:** The final translated text is generated, which can be a sentence, paragraph, or document.\n",
    "\n",
    "The Transformer model is particularly well-suited for machine translation tasks due to its ability to:\n",
    "\n",
    "* **Handle long-range dependencies:** The self-attention mechanism allows the model to capture complex relationships between tokens in the input sequence, even if they are far apart.\n",
    "* **Parallelize computation:** The Transformer model can be parallelized more easily than traditional recurrent neural network (RNN) architectures, making it more efficient for large-scale translation tasks.\n",
    "* **Learn contextual relationships:** The model can learn to represent words and tokens in context, taking into account the surrounding words and the overall meaning of the sentence.\n",
    "\n",
    "The \"Attention Is All You Need\" paper demonstrates the effectiveness of the Transformer model for machine translation tasks, achieving state-of-the-art results on several benchmark datasets, including the WMT 2014 English-to-German and English-to-French translation tasks.\n",
    "\n",
    "In the context of machine translation, the evaluation metrics used to measure the quality of translations include:\n",
    "\n",
    "* **BLEU (Bilingual Evaluation Understudy) score:** Measures the similarity between the generated translation and the reference translation.\n",
    "* **ROUGE (Recall-Oriented Understudy for Gisting Evaluation) score:** Measures the overlap between the generated translation and the reference translation.\n",
    "* **METEOR (Metric for Evaluation of Translation with Explicit ORdering) score:** Measures the similarity between the generated translation and the reference translation, taking into account the order of the words.\n",
    "\n",
    "Overall, the translation task in the context of the \"Attention Is All You Need\" paper and NLP refers to the automatic conversion of text from one language to another, using machine learning models to learn the patterns and relationships between languages and generate accurate translations.\n",
    "\"\"\",\n",
    "        \"sources\": [\"Attention Is All You Need\"],\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the difference between GRNN and RNN?\",\n",
    "        \"generated_answer\": \"\"\"## SUMMARY: \n",
    "The main difference between Gated Recurrent Neural Networks (GRNNs) and Recurrent Neural Networks (RNNs) lies in their ability to handle the vanishing gradient problem and their capacity to learn long-term dependencies in sequential data. GRNNs, including Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU), use gates to control the flow of information, allowing them to learn more complex patterns and maintain gradients over longer sequences. In contrast, traditional RNNs often struggle with vanishing gradients, making it difficult for them to learn long-term dependencies.\n",
    "\n",
    "## DETAILS: \n",
    "Recurrent Neural Networks (RNNs) are a type of neural network designed to handle sequential data, such as time series data, speech, or text. They have a feedback connection that allows the output from the previous time step to be used as input for the current time step, enabling the network to keep track of information over time.\n",
    "\n",
    "However, traditional RNNs suffer from the vanishing gradient problem, which occurs when the gradients used to update the network's weights become smaller as they are backpropagated through time. This makes it challenging for RNNs to learn long-term dependencies in data, as the gradients may become too small to be useful.\n",
    "\n",
    "Gated Recurrent Neural Networks (GRNNs), on the other hand, are designed to address the vanishing gradient problem. They use gates to control the flow of information into and out of the network's memory cells, allowing them to learn more complex patterns and maintain gradients over longer sequences.\n",
    "\n",
    "The key differences between GRNNs and RNNs are:\n",
    "\n",
    "1. **Gates:** GRNNs use gates to control the flow of information, whereas RNNs do not. These gates help GRNNs to selectively forget or remember information, allowing them to learn more complex patterns.\n",
    "2. **Memory Cells:** GRNNs have memory cells that can store information for long periods, whereas RNNs do not. These memory cells help GRNNs to learn long-term dependencies in data.\n",
    "3. **Vanishing Gradient Problem:** GRNNs are less susceptible to the vanishing gradient problem than RNNs, as the gates help to maintain gradients over longer sequences.\n",
    "4. **Learning Long-Term Dependencies:** GRNNs are better at learning long-term dependencies in data than RNNs, due to their ability to maintain gradients and store information in memory cells.\n",
    "\n",
    "Examples of GRNNs include:\n",
    "\n",
    "* **Long Short-Term Memory (LSTM) Networks:** LSTMs use three gates (input, output, and forget gates) to control the flow of information into and out of the network's memory cells.\n",
    "* **Gated Recurrent Units (GRUs):** GRUs use two gates (reset and update gates) to control the flow of information into and out of the network's memory cells.\n",
    "\n",
    "In contrast, traditional RNNs do not use gates or memory cells, and are more prone to the vanishing gradient problem.\n",
    "\n",
    "In summary, while both RNNs and GRNNs are designed to handle sequential data, GRNNs are better equipped to learn long-term dependencies and handle the vanishing gradient problem, making them a popular choice for many applications, including natural language processing, speech recognition, and time series forecasting.      \"\"\",\n",
    "        \"sources\": [\"Attention Is All You Need\"],\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is LSTM?\",\n",
    "        \"generated_answer\": \"\"\"## SUMMARY: \n",
    "Long Short-Term Memory (LSTM) is a type of Recurrent Neural Network (RNN) architecture designed to handle the vanishing gradient problem and learn long-term dependencies in sequential data. LSTMs use memory cells and gates to control the flow of information, allowing them to selectively remember and forget information over time.\n",
    "\n",
    "## DETAILS: \n",
    "LSTMs were introduced by Sepp Hochreiter and JÃ¼rgen Schmidhuber in 1997 as a solution to the vanishing gradient problem in traditional RNNs. The key components of an LSTM network are:\n",
    "\n",
    "1. **Memory Cells:** LSTMs have memory cells that can store information for long periods of time. These cells are the core of the LSTM architecture and allow the network to learn long-term dependencies.\n",
    "2. **Gates:** LSTMs use three types of gates to control the flow of information into and out of the memory cells:\n",
    "    * **Input Gate:** Controls the amount of new information that is added to the memory cell.\n",
    "    * **Output Gate:** Controls the amount of information that is output from the memory cell.\n",
    "    * **Forget Gate:** Controls the amount of information that is discarded from the memory cell.\n",
    "3. **Cell State:** The cell state is the internal state of the memory cell, which stores the information that is being remembered.\n",
    "\n",
    "The LSTM architecture works as follows:\n",
    "\n",
    "1. **Input:** The input is passed through the input gate, which determines how much of the new information is added to the memory cell.\n",
    "2. **Forget Gate:** The forget gate determines how much of the previous information is discarded from the memory cell.\n",
    "3. **Cell State Update:** The cell state is updated based on the input and the forget gate.\n",
    "4. **Output Gate:** The output gate determines how much of the information in the memory cell is output.\n",
    "5. **Hidden State:** The hidden state is the output of the LSTM cell, which is used as input to the next time step.\n",
    "\n",
    "LSTMs have several advantages over traditional RNNs, including:\n",
    "\n",
    "* **Ability to learn long-term dependencies:** LSTMs can learn dependencies that span hundreds or even thousands of time steps.\n",
    "* **Resistance to vanishing gradients:** LSTMs are less susceptible to the vanishing gradient problem, which makes them more stable and easier to train.\n",
    "* **Ability to handle variable-length sequences:** LSTMs can handle sequences of varying lengths, making them suitable for applications such as speech recognition and natural language processing.\n",
    "\n",
    "LSTMs have been widely used in many applications, including:\n",
    "\n",
    "* **Speech recognition:** LSTMs are used in speech recognition systems to model the temporal dependencies in speech signals.\n",
    "* **Natural language processing:** LSTMs are used in natural language processing tasks such as language modeling, text classification, and machine translation.\n",
    "* **Time series forecasting:** LSTMs are used in time series forecasting to model the temporal dependencies in data.\n",
    "\n",
    "Some of the key variants of LSTMs include:\n",
    "\n",
    "* **Gated Recurrent Units (GRUs):** GRUs are a simpler variant of LSTMs that use only two gates (reset and update gates) instead of three.\n",
    "* **Bidirectional LSTMs:** Bidirectional LSTMs are used to model both the forward and backward dependencies in a sequence.\n",
    "* **Stacked LSTMs:** Stacked LSTMs are used to model complex dependencies in data by stacking multiple LSTM layers on top of each other.\n",
    "\"\"\",\n",
    "        \"sources\": [\"Attention Is All You Need\"],\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is are RNNs?\",\n",
    "        \"generated_answer\": \"\"\"## SUMMARY: \n",
    "Recurrent Neural Networks (RNNs) are a type of neural network designed to handle sequential data, such as time series data, speech, or text. RNNs have a feedback connection that allows the output from the previous time step to be used as input for the current time step, enabling the network to keep track of information over time.\n",
    "\n",
    "## DETAILS: \n",
    "RNNs are a fundamental concept in deep learning, and they are particularly useful for modeling sequential data. The key characteristics of RNNs are:\n",
    "\n",
    "1. **Sequential Input:** RNNs are designed to handle sequential input data, where each sample is dependent on the previous samples.\n",
    "2. **Feedback Connection:** RNNs have a feedback connection that allows the output from the previous time step to be used as input for the current time step.\n",
    "3. **Hidden State:** RNNs have a hidden state that captures the information from the previous time steps, allowing the network to keep track of the context.\n",
    "4. **Recurrent Connections:** RNNs have recurrent connections that allow the network to feedback the output from the previous time step to the current time step.\n",
    "\n",
    "The RNN architecture works as follows:\n",
    "\n",
    "1. **Input:** The input is passed through the network at each time step.\n",
    "2. **Hidden State Update:** The hidden state is updated based on the input and the previous hidden state.\n",
    "3. **Output:** The output is generated based on the hidden state.\n",
    "4. **Feedback:** The output is fed back to the network as input for the next time step.\n",
    "\n",
    "RNNs have several advantages, including:\n",
    "\n",
    "* **Ability to handle sequential data:** RNNs are designed to handle sequential data, making them suitable for applications such as speech recognition, natural language processing, and time series forecasting.\n",
    "* **Ability to capture temporal dependencies:** RNNs can capture temporal dependencies in data, allowing them to model complex patterns and relationships.\n",
    "* **Flexibility:** RNNs can be used for a wide range of applications, including classification, regression, and generation tasks.\n",
    "\n",
    "However, RNNs also have some limitations, including:\n",
    "\n",
    "* **Vanishing Gradient Problem:** RNNs can suffer from the vanishing gradient problem, where the gradients used to update the network's weights become smaller as they are backpropagated through time.\n",
    "* **Exploding Gradient Problem:** RNNs can also suffer from the exploding gradient problem, where the gradients become larger as they are backpropagated through time.\n",
    "* **Computational Complexity:** RNNs can be computationally expensive to train, especially for long sequences.\n",
    "\n",
    "Some of the key variants of RNNs include:\n",
    "\n",
    "* **Simple RNNs:** Simple RNNs are the basic form of RNNs, where the hidden state is updated based on the input and the previous hidden state.\n",
    "* **LSTMs (Long Short-Term Memory):** LSTMs are a type of RNN that uses memory cells and gates to control the flow of information, allowing them to learn long-term dependencies.\n",
    "* **GRUs (Gated Recurrent Units):** GRUs are a type of RNN that uses gates to control the flow of information, allowing them to learn long-term dependencies.\n",
    "* **Bidirectional RNNs:** Bidirectional RNNs are used to model both the forward and backward dependencies in a sequence.\n",
    "\n",
    "RNNs have been widely used in many applications, including:\n",
    "\n",
    "* **Speech Recognition:** RNNs are used in speech recognition systems to model the temporal dependencies in speech signals.\n",
    "* **Natural Language Processing:** RNNs are used in natural language processing tasks such as language modeling, text classification, and machine translation.\n",
    "* **Time Series Forecasting:** RNNs are used in time series forecasting to model the temporal dependencies in data.\n",
    "* **Generative Models:** RNNs are used in generative models such as language models and text generators to generate coherent and context-dependent text.RNNs (Recurrent Neural Networks) are a type of neural network that process sequential data, allowing them to maintain a state that can be updated as new data is processed.\n",
    "\"\"\",\n",
    "        \"sources\": [\"Attention Is All You Need\"],\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is self-attention?\",\n",
    "        \"generated_answer\": \"\"\"## SUMMARY: \n",
    "Self-attention is a mechanism in neural networks that allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. It's a key component of the Transformer architecture, introduced in the paper \"Attention Is All You Need\" by Vaswani et al. Self-attention enables the model to capture long-range dependencies and contextual relationships in the input data, making it particularly useful for natural language processing tasks.\n",
    "\n",
    "## DETAILS: \n",
    "Self-attention is a type of attention mechanism that allows the model to compute the representation of a sequence by attending to all positions in the sequence and weighing their importance. This is different from traditional recurrent neural networks (RNNs), which process the input sequence one step at a time, using the previous hidden state to inform the next step.\n",
    "\n",
    "The self-attention mechanism works as follows:\n",
    "\n",
    "1. **Input Sequence:** The input sequence is first embedded into a vector space, where each token (e.g., word or character) is represented as a vector.\n",
    "2. **Query, Key, and Value:** The embedded input sequence is then split into three vectors: Query (Q), Key (K), and Value (V).\n",
    "3. **Attention Weights:** The attention weights are computed by taking the dot product of the Query and Key vectors and applying a softmax function. This produces a set of weights that represent the importance of each token in the sequence relative to the others.\n",
    "4. **Weighted Sum:** The attention weights are then used to compute a weighted sum of the Value vectors, which produces the final output of the self-attention mechanism.\n",
    "\n",
    "The self-attention mechanism has several benefits, including:\n",
    "\n",
    "* **Parallelization:** Self-attention can be parallelized more easily than RNNs, making it faster to train and more efficient for large-scale datasets.\n",
    "* **Long-range dependencies:** Self-attention can capture long-range dependencies in the input sequence, which is particularly useful for natural language processing tasks where context is important.\n",
    "* **Flexibility:** Self-attention can be used in a variety of architectures, including the Transformer, which has become a standard model for many NLP tasks.\n",
    "\n",
    "There are several types of self-attention mechanisms, including:\n",
    "\n",
    "* **Scaled Dot-Product Attention:** This is the original self-attention mechanism introduced in the Transformer paper, which uses a scaled dot product to compute the attention weights.\n",
    "* **Multi-Head Attention:** This is a variant of self-attention that uses multiple attention heads to capture different types of relationships in the input sequence.\n",
    "* **Hierarchical Attention:** This is a variant of self-attention that uses a hierarchical structure to capture relationships at different levels of granularity.\n",
    "\n",
    "Self-attention has been widely adopted in many NLP tasks, including:\n",
    "\n",
    "* **Machine Translation:** Self-attention is used in machine translation models to capture the relationships between words in the input and output sequences.\n",
    "* **Text Classification:** Self-attention is used in text classification models to capture the relationships between words in the input sequence and the class labels.\n",
    "* **Language Modeling:** Self-attention is used in language models to capture the relationships between words in the input sequence and predict the next word.\n",
    "\n",
    "Overall, self-attention is a powerful mechanism that has revolutionized the field of NLP and has many potential applications in other areas of AI research.\n",
    "\"\"\",\n",
    "        \"sources\": [\"Attention Is All You Need\"],\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is Attention?\",\n",
    "        \"generated_answer\": \"\"\"## SUMMARY: \n",
    "Attention is a mechanism in neural networks that allows the model to focus on specific parts of the input data that are relevant for the task at hand. It's a way to selectively weigh the importance of different input elements, such as words in a sentence or pixels in an image, and concentrate on the most relevant ones.\n",
    "\n",
    "## DETAILS: \n",
    "Attention is a key component of many neural network architectures, including transformers, recurrent neural networks (RNNs), and convolutional neural networks (CNNs). The basic idea behind attention is to compute a set of weights that represent the importance of each input element, and then use these weights to compute a weighted sum of the input elements.\n",
    "\n",
    "The attention mechanism typically consists of three components:\n",
    "\n",
    "1. **Query:** The query is the input to the attention mechanism, which is typically a vector or a matrix.\n",
    "2. **Key:** The key is a set of vectors or matrices that represent the input data.\n",
    "3. **Value:** The value is a set of vectors or matrices that represent the output of the attention mechanism.\n",
    "\n",
    "The attention mechanism computes the weights by taking the dot product of the query and key, and then applying a softmax function to obtain a probability distribution over the input elements. The weights are then used to compute a weighted sum of the value vectors, which represents the output of the attention mechanism.\n",
    "\n",
    "Attention has several benefits, including:\n",
    "\n",
    "* **Improved performance:** Attention can improve the performance of neural networks by allowing them to focus on the most relevant input elements.\n",
    "* **Interpretability:** Attention can provide insights into which input elements are most relevant for the task at hand.\n",
    "* **Flexibility:** Attention can be used in a variety of neural network architectures and can be applied to different types of input data.\n",
    "\n",
    "There are several types of attention mechanisms, including:\n",
    "\n",
    "* **Scaled Dot-Product Attention:** This is a type of attention mechanism that uses a scaled dot product to compute the weights.\n",
    "* **Multi-Head Attention:** This is a type of attention mechanism that uses multiple attention heads to compute the weights.\n",
    "* **Hierarchical Attention:** This is a type of attention mechanism that uses a hierarchical structure to compute the weights.\n",
    "\n",
    "Attention has been widely used in many applications, including:\n",
    "\n",
    "* **Natural Language Processing (NLP):** Attention has been used in NLP tasks such as machine translation, text summarization, and question answering.\n",
    "* **Computer Vision:** Attention has been used in computer vision tasks such as image classification, object detection, and image segmentation.\n",
    "* **Speech Recognition:** Attention has been used in speech recognition tasks such as speech-to-text and voice recognition.\n",
    "\n",
    "### Attention Mechanism\n",
    "\n",
    "The attention mechanism can be represented as follows:\n",
    "```\n",
    "Attention(Q, K, V) = softmax(Q * K^T / sqrt(d)) * V\n",
    "```\n",
    "Where:\n",
    "\n",
    "* `Q` is the query vector\n",
    "* `K` is the key vector\n",
    "* `V` is the value vector\n",
    "* `d` is the dimensionality of the query and key vectors\n",
    "* `softmax` is the softmax function\n",
    "* `*` is the dot product operator\n",
    "* `^T` is the transpose operator\n",
    "\n",
    "Note: This is a simplified representation of the attention mechanism. The actual implementation may vary depending on the specific use case and requirements.\n",
    "\"\"\",\n",
    "        \"sources\": [\"Attention Is All You Need\"],\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is a transformer?\",\n",
    "        \"generated_answer\": \"\"\"## SUMMARY: \n",
    "A Transformer is a type of neural network architecture introduced in the paper \"Attention Is All You Need\" by Vaswani et al. in 2017. It's primarily designed for sequence-to-sequence tasks, such as machine translation, text summarization, and image captioning. The Transformer model relies entirely on self-attention mechanisms to process input sequences, eliminating the need for recurrent neural networks (RNNs) and convolutional neural networks (CNNs).\n",
    "\n",
    "## DETAILS: \n",
    "The Transformer architecture is based on the concept of self-attention, which allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This is different from traditional RNNs, which process the input sequence one step at a time, using the previous hidden state to inform the next step.\n",
    "\n",
    "The Transformer model consists of two main components:\n",
    "\n",
    "1. **Encoder:** The encoder takes in a sequence of tokens (e.g., words or characters) and outputs a sequence of vectors. The encoder is composed of a stack of identical layers, each of which consists of two sub-layers: self-attention and feed-forward neural network (FFNN).\n",
    "2. **Decoder:** The decoder takes in the output of the encoder and generates a sequence of tokens. The decoder is also composed of a stack of identical layers, each of which consists of three sub-layers: self-attention, encoder-decoder attention, and FFNN.\n",
    "\n",
    "The Transformer architecture has several key features:\n",
    "\n",
    "* **Self-Attention:** The Transformer model uses self-attention mechanisms to process the input sequence. This allows the model to attend to different parts of the sequence simultaneously and weigh their importance.\n",
    "* **Multi-Head Attention:** The Transformer model uses multi-head attention, which allows the model to capture different types of relationships between the input tokens.\n",
    "* **Positional Encoding:** The Transformer model uses positional encoding to preserve the order of the input sequence.\n",
    "* **Layer Normalization:** The Transformer model uses layer normalization to normalize the output of each layer.\n",
    "\n",
    "The Transformer model has several advantages over traditional sequence-to-sequence models:\n",
    "\n",
    "* **Parallelization:** The Transformer model can be parallelized more easily than RNNs, making it faster to train and more efficient for large-scale datasets.\n",
    "* **Scalability:** The Transformer model can handle longer input sequences than RNNs, making it more suitable for tasks that require processing long sequences.\n",
    "* **Performance:** The Transformer model has achieved state-of-the-art results in many sequence-to-sequence tasks, including machine translation, text summarization, and image captioning.\n",
    "\n",
    "Some of the key applications of the Transformer model include:\n",
    "\n",
    "* **Machine Translation:** The Transformer model has been widely used for machine translation tasks, including English-to-French, English-to-German, and English-to-Chinese.\n",
    "* **Text Summarization:** The Transformer model has been used for text summarization tasks, including summarizing news articles and documents.\n",
    "* **Image Captioning:** The Transformer model has been used for image captioning tasks, including generating captions for images.\n",
    "* **Chatbots:** The Transformer model has been used for chatbot applications, including generating responses to user input.\n",
    "\n",
    "Overall, the Transformer model is a powerful and flexible architecture that has revolutionized the field of natural language processing and has many potential applications in other areas of AI research.\n",
    "\n",
    "### Transformer Architecture\n",
    "\n",
    "The Transformer architecture can be represented as follows:\n",
    "```\n",
    "Encoder:\n",
    "- Input Embedding\n",
    "- Positional Encoding\n",
    "- Layer 1:\n",
    "    - Self-Attention\n",
    "    - FFNN\n",
    "- Layer 2:\n",
    "    - Self-Attention\n",
    "    - FFNN\n",
    "- ...\n",
    "- Layer N:\n",
    "    - Self-Attention\n",
    "    - FFNN\n",
    "\n",
    "Decoder:\n",
    "- Input Embedding\n",
    "- Positional Encoding\n",
    "- Layer 1:\n",
    "    - Self-Attention\n",
    "    - Encoder-Decoder Attention\n",
    "    - FFNN\n",
    "- Layer 2:\n",
    "    - Self-Attention\n",
    "    - Encoder-Decoder Attention\n",
    "    - FFNN\n",
    "- ...\n",
    "- Layer N:\n",
    "    - Self-Attention\n",
    "    - Encoder-Decoder Attention\n",
    "    - FFNN\n",
    "```\n",
    "Note: This is a simplified representation of the Transformer architecture. The actual implementation may vary depending on the specific use case and requirements.\n",
    "\"\"\",\n",
    "        \"sources\": [\"Attention Is All You Need\"],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Pydantic model for expected output\n",
    "\n",
    "Next, we define a Pydantic model to establish the structure for test output, to ensure type safety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpectedOutput(BaseModel):\n",
    "    answer: str | None\n",
    "    sources: list[str] | None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Upload the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "studio_dataset_repo = StudioDatasetRepository(studio_client=studio_client)\n",
    "\n",
    "examples = [\n",
    "    Example(\n",
    "        input=Input(question=example[\"question\"]),\n",
    "        expected_output=ExpectedOutput(\n",
    "            answer=example[\"generated_answer\"], sources=example[\"sources\"]\n",
    "        ),\n",
    "    )\n",
    "    for example in test_set\n",
    "]\n",
    "\n",
    "studio_dataset = studio_dataset_repo.create_dataset(\n",
    "    examples=examples, dataset_name=\"demo-dataset\"\n",
    ")\n",
    "\n",
    "studio_dataset.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access the dataset, follow the tutorial [Store an evaluation dataset in PhariaStudio](https://docs.aleph-alpha.com/products/pharia-ai/pharia-studio/tutorial/store-dataset-in-data-platform/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define evaluation logic\n",
    "\n",
    "PhariaStudio SDK requires the creation of `EvaluationLogic` - to evaulate individual examples - and `AggregationLogic` - to aggregate all the individual evaluations into overall metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 EvaluationLogic - 'LLM as a judge' implementation\n",
    "\n",
    "First, we set up the evaluation logic that is used for each individual example. Our advanced evaluation implements a *LLM as a judge* evaluation, which uses LLMs to assess the quality of generated responses across multiple dimensions.\n",
    "\n",
    "##### Base class\n",
    "\n",
    "The `Checker` abstract base class provides the foundational infrastructure for all evaluation checkers:\n",
    "\n",
    "- **Model integration**: Connects to the PhariaAI platform using `DevCsi` and utilises `llama-3.3-70b-instruct` as the evaluation model\n",
    "- **Weighted scoring**: Implements a sophisticated scoring mechanism that uses token-level log probabilities to compute more reliable scores\n",
    "- **Robust parsing**: Handles edge cases where the model does not return a valid numeric score\n",
    "\n",
    "##### Individual checker implementations\n",
    "\n",
    "From this we derive three specialised checker classes, each designed to assess different aspects of answer quality:\n",
    "\n",
    "1. **AccuracyChecker**: Focuses on factual correctness by comparing specific claims, data points, and verifiable information between the generated and expected answers. The scoring rubric ranges from 1-2 (major factual errors) to 9-10 (highly accurate with no factual errors).\n",
    "\n",
    "2. **FactualityChecker**: Specifically designed to detect hallucinations and fabricated content. It evaluates whether the generated answer contains information not present in the reference, helping identify when models add unsupported claims or speculative statements presented as facts.\n",
    "\n",
    "3. **CompletenessChecker**: Assesses coverage of content by determining how well the generated answer addresses all main topics and key points from the expected answer. This ensures that responses are comprehensive rather than just accurate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "\n",
    "from pharia_skill import TopLogprobs, ChatResponse\n",
    "from pharia_skill.testing import DevCsi\n",
    "from jinja2 import Template\n",
    "from abc import ABC\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "\n",
    "class Checker(ABC):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.dev_csi = DevCsi(project=PHARIA_STUDIO_PROJECT_NAME)\n",
    "        self.evaluation_model = \"llama-3.3-70b-instruct\"\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def get_metric(\n",
    "        self, question: str, expected_answer: str, generated_answer: str\n",
    "    ) -> ChatResponse:\n",
    "        system_prompt = self.system_prompt\n",
    "        user_prompt = self.user_prompt.format(\n",
    "            question=question,\n",
    "            expected_answer=expected_answer,\n",
    "            generated_answer=generated_answer,\n",
    "        )\n",
    "\n",
    "        messages = [\n",
    "            Message.system(system_prompt),\n",
    "            Message.user(user_prompt),\n",
    "            Message.assistant(\"Score: \"),\n",
    "        ]\n",
    "        params = ChatParams(max_tokens=10, temperature=0.0, logprobs=TopLogprobs(10))\n",
    "        response = self.dev_csi.chat(\n",
    "            model=self.evaluation_model, messages=messages, params=params\n",
    "        )\n",
    "\n",
    "        content = response.message.content.strip()\n",
    "        fallback_score = self.parse_score(content)\n",
    "\n",
    "        probs = getattr(response, \"logprobs\", None)\n",
    "        if (\n",
    "            not probs\n",
    "            or not hasattr(probs, \"__getitem__\")\n",
    "            or len(probs) < 2\n",
    "            or not hasattr(probs[-2], \"top\")\n",
    "        ):\n",
    "            self.logger.warning(\"No logprobs found\")\n",
    "            return fallback_score\n",
    "\n",
    "        logprobs = probs[-2].top\n",
    "        return self.compute_weighted_score(logprobs, fallback_score)\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_score(score_str: str) -> float:\n",
    "        \"\"\"Convert score string to float if valid, else return fallback\"\"\"\n",
    "        return (\n",
    "            float(score_str)\n",
    "            if score_str.isdigit() and 0 <= float(score_str) <= 10\n",
    "            else 1\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_weighted_score(logprobs, fallback_score: float) -> float:\n",
    "        \"\"\"Compute weighted score from token logprobs\"\"\"\n",
    "        digit_probs = {\n",
    "            float(prob.token): math.exp(prob.logprob)\n",
    "            for prob in logprobs\n",
    "            if prob.token.isdigit() and 0 <= float(prob.token) <= 10\n",
    "        }\n",
    "\n",
    "        total = sum(digit_probs.values())\n",
    "        if total == 0:\n",
    "            return fallback_score\n",
    "\n",
    "        normalized = {k: v / total for k, v in digit_probs.items()}\n",
    "        return round(sum(k * v for k, v in normalized.items()), 1)\n",
    "\n",
    "\n",
    "class AccuracyChecker(Checker):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.system_prompt = \"\"\"You are a highly precise evaluation assistant specialized in assessing factual accuracy and correctness.\n",
    "\n",
    "Your task is to evaluate how accurately the generated answer reflects the facts and information from the expected answer.\n",
    "\n",
    "SCORING RUBRIC (1-10):\n",
    "- 9-10: Highly accurate - No factual errors, all claims correctly supported\n",
    "- 7-8: Mostly accurate - Minor factual discrepancies or unclear statements\n",
    "- 5-6: Moderately accurate - Some factual errors but generally correct direction\n",
    "- 3-4: Low accuracy - Multiple factual errors or significant misrepresentations\n",
    "- 1-2: Poor accuracy - Major factual errors, contradicts expected information\n",
    "\n",
    "EVALUATION CRITERIA:\n",
    "1. Are specific facts, figures, and data points correct?\n",
    "2. Are the claims and statements supported by the expected information?\n",
    "3. Are there any contradictions with the expected answer?\n",
    "4. Is the information presented without distortion or misinterpretation?\n",
    "\n",
    "Return only a single integer score between 1 and 10. \n",
    "\"\"\"\n",
    "        self.user_prompt: Template = \"\"\"TASK: Evaluate the factual accuracy of the generated answer against the expected reference answer.\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "EXPECTED ANSWER (Reference):\n",
    "{expected_answer}\n",
    "\n",
    "GENERATED ANSWER (To Evaluate):\n",
    "{generated_answer}\n",
    "\n",
    "EVALUATION STEPS:\n",
    "1. Identify factual claims, data points, and specific information in both answers\n",
    "2. Compare the accuracy of facts, figures, dates, and other verifiable information\n",
    "3. Check for any contradictions or misrepresentations\n",
    "4. Assess whether claims are properly supported by the reference information\n",
    "\n",
    "IMPORTANT: Respond with ONLY a single integer from 1 to 10. Do not include any explanation or additional text.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class FactualityChecker(Checker):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.system_prompt = \"\"\"You are a highly precise evaluation assistant specialized in assessing information precision and relevance.\n",
    "\n",
    "Your task is to evaluate how well the generated answer stays grounded in relevant information without adding hallucinated or irrelevant content.\n",
    "\n",
    "SCORING RUBRIC (1-10):\n",
    "- 9-10: Excellent precision - Only relevant information, no hallucinations or fabrications\n",
    "- 7-8: Good precision - Mostly relevant content, minimal irrelevant information\n",
    "- 5-6: Fair precision - Some irrelevant details or minor unsupported claims\n",
    "- 3-4: Poor precision - Significant irrelevant content or unverifiable claims\n",
    "- 1-2: Very poor precision - Extensive hallucinations or fabricated information\n",
    "\n",
    "EVALUATION CRITERIA:\n",
    "1. Does the answer stick to information that can be verified from the expected content?\n",
    "2. Are there any fabricated details, dates, names, or claims not in the reference?\n",
    "3. Is all information relevant to the topic and question asked?\n",
    "4. Are there any speculative statements presented as facts?\n",
    "\n",
    "Return only a single integer score between 1 and 10. \n",
    "\"\"\"\n",
    "        self.user_prompt: Template = \"\"\"TASK: Evaluate the information precision and relevance of the generated answer, focusing on detecting hallucinations or fabricated content.\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "EXPECTED ANSWER (Reference):\n",
    "{expected_answer}\n",
    "\n",
    "GENERATED ANSWER (To Evaluate):\n",
    "{generated_answer}\n",
    "\n",
    "EVALUATION STEPS:\n",
    "1. Compare the generated answer against the reference to identify any added information\n",
    "2. Check for fabricated details, names, dates, or claims not present in the reference\n",
    "3. Assess whether all information is relevant to the topic\n",
    "4. Look for speculative statements presented as definitive facts\n",
    "\n",
    "IMPORTANT: Respond with ONLY a single integer from 1 to 10. Do not include any explanation or additional text.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class CompletenessChecker(Checker):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.system_prompt = \"\"\"You are a highly precise evaluation assistant specialized in assessing content completeness.\n",
    "\n",
    "Your task is to evaluate how completely the generated answer covers the key information from the expected answer.\n",
    "\n",
    "SCORING RUBRIC (1-10):\n",
    "- 9-10: Comprehensive coverage - All major points and most minor details included\n",
    "- 7-8: Good coverage - All major points included, some minor details may be missing\n",
    "- 5-6: Adequate coverage - Most major points included, several details missing\n",
    "- 3-4: Incomplete coverage - Some major points included, many details missing\n",
    "- 1-2: Poor coverage - Few or no major points included\n",
    "\n",
    "EVALUATION CRITERIA:\n",
    "1. Are all main topics/themes from the expected answer present?\n",
    "2. Are supporting details and examples adequately covered?\n",
    "3. Is the depth of information comparable to the expected answer?\n",
    "4. Are any critical pieces of information missing?\n",
    "\n",
    "Return only a single integer score between 1 and 10.\n",
    "\"\"\"\n",
    "        self.user_prompt = \"\"\"TASK: Evaluate how completely the generated answer covers the content from the expected answer.\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "EXPECTED ANSWER (Reference):\n",
    "{expected_answer}\n",
    "\n",
    "GENERATED ANSWER (To Evaluate):\n",
    "{generated_answer}\n",
    "\n",
    "EVALUATION STEPS:\n",
    "1. Identify the main topics and key points in the expected answer\n",
    "2. Check if each main topic is addressed in the generated answer\n",
    "3. Assess the depth and detail level compared to the expected answer\n",
    "4. Consider any missing critical information\n",
    "\n",
    "IMPORTANT: Respond with ONLY a single integer from 1 to 10. Do not include any explanation or additional text.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### QaEvaluation data model\n",
    "\n",
    "\n",
    "As we are now computing more scores, we also need to extend the `QaEvaluation` class to capture all of the different evaluation dimensions:\n",
    "\n",
    "1. **Completeness score**: Measures how thoroughly the generated answer covers the key information present in the expected reference answer\n",
    "2. **Accuracy score**: Evaluates the factual correctness of claims and information in the generated response\n",
    "3. **Factuality score**: Assesses the absence of hallucinations and fabricated content not present in the reference material\n",
    "4. **Correct sources**: Lists the source documents that were properly cited and match the expected sources\n",
    "5. **Incorrect sources**: Catalogues any source documents that were cited incorrectly or are not in the expected source list\n",
    "6. **Source accuracy**: Calculates the precision of source citations (correct sources / total cited sources)\n",
    "7. **Source recall**: Measures the coverage of expected source documents (found expected sources / total expected sources)\n",
    "\n",
    "Beyond the definition of the metrics, the `QaEvaluationLogic` class also implements several key functions:\n",
    "\n",
    "- **`do_evaluate_single_output()`**: The main evaluation method that orchestrates the assessment of a single question-answer pair by calling all three LLM-based checkers and computing source metrics\n",
    "- **`_check_sources()`**: Performs case-insensitive comparison between expected and generated source citations, returning lists of correctly and incorrectly cited sources\n",
    "- **`_calculate_source_accuracy()`**: Computes the precision of source citations by dividing the number of correct sources by the total number of cited sources\n",
    "- **`_calculate_source_recall()`**: Calculates the recall of expected sources by dividing the number of found expected sources by the total number of expected sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QaEvaluation(BaseModel):\n",
    "    completeness_score: float = 0.0  # Coverage of expected content\n",
    "    accuracy_score: float = 0.0  # Factual correctness\n",
    "    factuality_score: float = 0.0  # Absence of hallucinations\n",
    "    correct_sources: list[str] = []  # Properly cited sources\n",
    "    incorrect_sources: list[str] = []  # Incorrectly cited sources\n",
    "    source_accuracy: float = 0.0  # Precision of source citations\n",
    "    source_recall: float = 0.0  # Recall of expected sources\n",
    "\n",
    "\n",
    "class QaEvaluationLogic(\n",
    "    SingleOutputEvaluationLogic[Input, Output, ExpectedOutput, QaEvaluation]\n",
    "):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.accuracy_checker = AccuracyChecker()\n",
    "        self.factuality_checker = FactualityChecker()\n",
    "        self.completeness_checker = CompletenessChecker()\n",
    "\n",
    "    def do_evaluate_single_output(\n",
    "        self, example: Example[Input, ExpectedOutput], output: Output\n",
    "    ) -> QaEvaluation:\n",
    "\n",
    "        completeness_score = self.completeness_checker.get_metric(\n",
    "            question=example.input.question,\n",
    "            expected_answer=example.expected_output.answer,\n",
    "            generated_answer=output.answer,\n",
    "        )\n",
    "\n",
    "        accuracy_score = self.accuracy_checker.get_metric(\n",
    "            question=example.input.question,\n",
    "            expected_answer=example.expected_output.answer,\n",
    "            generated_answer=output.answer,\n",
    "        )\n",
    "\n",
    "        factuality_score = self.factuality_checker.get_metric(\n",
    "            question=example.input.question,\n",
    "            expected_answer=example.expected_output.answer,\n",
    "            generated_answer=output.answer,\n",
    "        )\n",
    "\n",
    "        correct_sources, incorrect_sources = self._check_sources(\n",
    "            expected_sources=example.expected_output.sources,\n",
    "            generated_sources=output.sources,\n",
    "        )\n",
    "        return QaEvaluation(\n",
    "            completeness_score=completeness_score,\n",
    "            accuracy_score=accuracy_score,\n",
    "            factuality_score=factuality_score,\n",
    "            correct_sources=correct_sources,\n",
    "            incorrect_sources=incorrect_sources,\n",
    "            source_accuracy=self._calculate_source_accuracy(\n",
    "                expected_sources=example.expected_output.sources,\n",
    "                generated_sources=output.sources,\n",
    "            ),\n",
    "            source_recall=self._calculate_source_recall(\n",
    "                expected_sources=example.expected_output.sources,\n",
    "                generated_sources=output.sources,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def _check_sources(\n",
    "        self, expected_sources: list[str], generated_sources: list[str]\n",
    "    ) -> tuple[list[str], list[str]]:\n",
    "        if not generated_sources:\n",
    "            return [], []\n",
    "\n",
    "        if not expected_sources:\n",
    "            return [], generated_sources.copy()\n",
    "\n",
    "        expected_set = {source.lower().strip() for source in expected_sources}\n",
    "        generated_set = {source.lower().strip() for source in generated_sources}\n",
    "\n",
    "        correct_sources_lower = expected_set.intersection(generated_set)\n",
    "\n",
    "        correct_sources = []\n",
    "        incorrect_sources = []\n",
    "\n",
    "        for source in generated_sources:\n",
    "            if source.lower().strip() in correct_sources_lower:\n",
    "                correct_sources.append(source)\n",
    "            else:\n",
    "                incorrect_sources.append(source)\n",
    "\n",
    "        return correct_sources, incorrect_sources\n",
    "\n",
    "    def _calculate_source_accuracy(\n",
    "        self, expected_sources: list[str], generated_sources: list[str]\n",
    "    ) -> float:\n",
    "\n",
    "        if not generated_sources:\n",
    "            return 0.0 if expected_sources else 1.0\n",
    "\n",
    "        correct_sources, _ = self._check_sources(expected_sources, generated_sources)\n",
    "        return len(correct_sources) / len(generated_sources)\n",
    "\n",
    "    def _calculate_source_recall(\n",
    "        self, expected_sources: list[str], generated_sources: list[str]\n",
    "    ) -> float:\n",
    "        if not expected_sources:\n",
    "            return 1.0\n",
    "\n",
    "        if not generated_sources:\n",
    "            return 0.0\n",
    "\n",
    "        expected_set = {source.lower().strip() for source in expected_sources}\n",
    "        generated_set = {source.lower().strip() for source in generated_sources}\n",
    "\n",
    "        found_expected = expected_set.intersection(generated_set)\n",
    "        return len(found_expected) / len(expected_sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure our evaluation logic works correctly, we test it with a sample question and answer about neural network encoders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_example = test_set[0]\n",
    "\n",
    "task = QATask()\n",
    "input = Input(question=test_example.get(\"question\"))\n",
    "output = task.run(input, NoOpTracer())\n",
    "\n",
    "example = Example(\n",
    "    input=input,\n",
    "    expected_output=ExpectedOutput(\n",
    "        answer=test_example.get(\"generated_answer\"), sources=test_example.get(\"sources\")\n",
    "    ),\n",
    ")\n",
    "\n",
    "evaluation_logic = QaEvaluationLogic()\n",
    "evaluation = evaluation_logic.do_evaluate_single_output(example, output)\n",
    "print(f\"Evaluation: {evaluation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 AggregationLogic\n",
    "\n",
    "To assess overall system performance, we need to aggregate individual evaluation results into metrics. The advanced `QaAggregationLogic` class handles the evaluation data from our LLM-based assessment and outputs the rounded averages across all individual test runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QaAggregatedEvaluation(BaseModel):\n",
    "    average_completeness_score: float\n",
    "    average_accuracy_score: float\n",
    "    average_factuality_score: float\n",
    "    average_source_accuracy: float\n",
    "    average_source_recall: float\n",
    "\n",
    "\n",
    "class QaAggregationLogic(\n",
    "    AggregationLogic[\n",
    "        QaEvaluation,\n",
    "        QaAggregatedEvaluation,\n",
    "    ]\n",
    "):\n",
    "    def aggregate(self, evaluations: Iterable[QaEvaluation]) -> QaAggregatedEvaluation:\n",
    "        evaluation_list = list(evaluations)\n",
    "        if len(evaluation_list) == 0:\n",
    "            return QaAggregatedEvaluation(\n",
    "                average_completeness_score=0.0,\n",
    "                average_accuracy_score=0.0,\n",
    "                average_factuality_score=0.0,\n",
    "                average_source_accuracy=0.0,\n",
    "                average_source_recall=0.0,\n",
    "            )\n",
    "\n",
    "        average_completeness_score = round(\n",
    "            mean(eval.completeness_score for eval in evaluation_list), 2\n",
    "        )\n",
    "        average_accuracy_score = round(\n",
    "            mean(eval.accuracy_score for eval in evaluation_list), 2\n",
    "        )\n",
    "        average_factuality_score = round(\n",
    "            mean(eval.factuality_score for eval in evaluation_list), 2\n",
    "        )\n",
    "        average_source_accuracy = round(\n",
    "            mean(eval.source_accuracy for eval in evaluation_list), 2\n",
    "        )\n",
    "        average_source_recall = round(\n",
    "            mean(eval.source_recall for eval in evaluation_list), 2\n",
    "        )\n",
    "\n",
    "        return QaAggregatedEvaluation(\n",
    "            average_completeness_score=average_completeness_score,\n",
    "            average_accuracy_score=average_accuracy_score,\n",
    "            average_factuality_score=average_factuality_score,\n",
    "            average_source_accuracy=average_source_accuracy,\n",
    "            average_source_recall=average_source_recall,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify our aggregation mechanism, we test it by aggregating two examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_1 = test_set[0]\n",
    "\n",
    "input_1 = Input(question=example_1.get(\"question\"))\n",
    "output_1 = task.run(input_1, NoOpTracer())\n",
    "example_1 = Example(\n",
    "    input=input_1,\n",
    "    expected_output=ExpectedOutput(\n",
    "        answer=example_1.get(\"answer\"), sources=example_1.get(\"sources\")\n",
    "    ),\n",
    ")\n",
    "\n",
    "example_2 = test_set[1]\n",
    "\n",
    "input_2 = Input(question=example_2.get(\"question\"))\n",
    "output_2 = task.run(input_2, NoOpTracer())\n",
    "example_2 = Example(\n",
    "    input=input_2,\n",
    "    expected_output=ExpectedOutput(\n",
    "        answer=example_2.get(\"answer\"), sources=example_2.get(\"sources\")\n",
    "    ),\n",
    ")\n",
    "\n",
    "aggregation_logic = QaAggregationLogic()\n",
    "evaluation_logic = QaEvaluationLogic()\n",
    "\n",
    "evaluation_1 = evaluation_logic.do_evaluate_single_output(example_1, output_1)\n",
    "evaluation_2 = evaluation_logic.do_evaluate_single_output(example_2, output_2)\n",
    "aggregation = aggregation_logic.aggregate([evaluation_1, evaluation_2])\n",
    "\n",
    "print(evaluation_1)\n",
    "print(evaluation_2)\n",
    "print(f\"Aggregation: {aggregation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create and run a benchmark\n",
    "\n",
    "With our evaluation components ready, we can now create a benchmark in PhariaStudio and run our evaluation on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_repository = StudioBenchmarkRepository(studio_client=studio_client)\n",
    "\n",
    "benchmark = benchmark_repository.create_benchmark(\n",
    "    dataset_id=studio_dataset.id,\n",
    "    eval_logic=evaluation_logic,\n",
    "    aggregation_logic=aggregation_logic,\n",
    "    name=\"LLM-as-a-judge-benchmark\",\n",
    "    description=\"This benchmark evaluates the LLM as a judge.\",\n",
    ")\n",
    "\n",
    "benchmark.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we trigger the becnhmark to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = benchmark_repository.get_benchmark(\n",
    "    benchmark_id=benchmark.id,\n",
    "    eval_logic=evaluation_logic,\n",
    "    aggregation_logic=aggregation_logic,\n",
    ")\n",
    "\n",
    "benchmark_execution_id = benchmark.execute(\n",
    "    task=task,\n",
    "    name=str(uuid4()),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the benchmark completes, you can view detailed results in the PhariaStudio interface under Evaluate/Benchmarks (check [Create and submit evaluations](https://docs.aleph-alpha.com/products/pharia-ai/pharia-studio/tutorial/write-a-simple-evaluation/) for more details)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improving your RAG application\n",
    "\n",
    "Based on the evaluation results, you can identify areas for improvement in your RAG application. Common improvements include:\n",
    "\n",
    "1. **Refining the prompt**: Adjust the prompt to encourage more precise reference citation\n",
    "2. **Adjusting retrieval parameters**: Modify the number of retrieved documents or relevance thresholds\n",
    "3. **Enhancing document chunking**: Change how documents are split and indexed\n",
    "4. **Implementing better ranking**: Add reranking steps to prioritise the most relevant documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-filippo",
   "language": "python",
   "name": "test-filippo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
