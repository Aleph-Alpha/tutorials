{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data setup - Setting up the document ingestion pipeline\n",
    "<a id=\"data-setup\"></a>\n",
    "\n",
    "This section describes how to establish a complete document ingestion pipeline in PhariaAI. The ingestion pipeline is a crucial foundation for RAG applications, as it transforms source documents into searchable, AI-ready processed documents.\n",
    "\n",
    "## Pipeline components\n",
    "\n",
    "The pipeline consists of several interconnected components:\n",
    "\n",
    "- **Stage**: Provides a entry point storage for source documents\n",
    "- **Trigger**: Defines the processing workflow when documents are uploaded.\n",
    "- **Transformation**: Converts raw files into structured, searchable content\n",
    "- **Repository**: Stores the source documents and processed documents\n",
    "- **Collection**: Groups processed documents in a searchable container with unified access patterns and shared indexes\n",
    "- **Index**: Enables efficient semantic search across your documents\n",
    "\n",
    "The document ingestion workflow we will build transforms source documents into searchable processed documents in the following steps:\n",
    "1. **Load**: uploading to the stage and triggering the transformation\n",
    "2. **Transform**: applying transformations and storing in the repository\n",
    "3. **Search**: storing in the collections and indexing for search\n",
    "\n",
    "\n",
    "<img src=\"../Visualizations/E2E-Tutorial-data-pipeline.png\" alt=\"Ingestion workflow\" style=\"width:85%\"/>\n",
    "\n",
    "\n",
    "## What you will learn\n",
    "\n",
    "1. How to configure your environment and connection parameters\n",
    "2. How to create an ingestion pipeline with the PhariaData API\n",
    "3. How to upload documents and monitor their processing\n",
    "4. How to interact with your processed content through search and retrieval\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting, ensure you have the following:\n",
    "\n",
    "- **API token**: A valid Aleph Alpha API token with appropriate permissions\n",
    "- **API URLs**: Access to running instances of `pharia-data-api` and `document-index-api`\n",
    "- **Permissions**: The *StudioUser* permission, as described in [User Setup](1.%20Introduction%20-%20Getting%20Started.ipynb#user-setup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure\n",
    "\n",
    "This section guides you through setting up your document ingestion pipeline. You will first import necessary libraries, configure your environment, and then build the essential components for document processing. The workflow follows a systematic approach of creating a repository, setting up a document staging area, configuring an index, and establishing triggers for automated document transformation.\n",
    "\n",
    "Below, you can see all concepts involved in the creation of the pipeline and their relationships.\n",
    "\n",
    "<img src=\"../Visualizations/E2E-Tutorial-data-pipeline-relationships.png\" alt=\"Resources relationships\" style=\"width:70%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import dependencies and configure the environment\n",
    "\n",
    "We begin by importing necessary dependencies and setting up the environment. We use standard Python libraries such as `requests` for API communication, `pandas` for data handling, as well as specialised libraries such as `tenacity` for robust error handling with retry mechanisms.\n",
    "\n",
    "The environment configuration establishes connections to two key PhariaAI services:\n",
    "- The PhariaData API for managing document transformations and storage\n",
    "- The PhariaDocument Index API for creating searchable indexes\n",
    "\n",
    "We use several key libraries for our document processing workflow. The code below imports all of these libraries and disables warnings to keep our notebook output clean:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `.env` using the following command and add your PhariaAI Token\n",
    "\n",
    "```bash\n",
    "cp .env.sample .env\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import concurrent.futures\n",
    "from dotenv import load_dotenv\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_exponential,\n",
    "    retry_if_exception_type,\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we configure the essential parameters that provide authentication and identify your workspace:\n",
    "\n",
    "- **TOKEN**: Your Aleph Alpha API authentication token, loaded from your environment file\n",
    "- **NAMESPACE**: The organisational namespace where your collections are stored (\"Studio\")\n",
    "- **COLLECTION**: The name of the document collection for this tutorial (\"pharia-tutorial-rag\")\n",
    "\n",
    "**Note:** The namespace identifier depends on your specific PhariaAI setup and permission level. The collection name can be freely chosen to help you organise and separate different RAG projects. Using descriptive collection names (like \"legal-contracts\" or \"product-documentation\") can help you manage multiple document sets within the same namespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setups\n",
    "load_dotenv(\".env\", override=True)\n",
    "\n",
    "TOKEN = os.getenv(\"PHARIA_AI_TOKEN\") #<your-token>\n",
    "NAMESPACE = os.getenv(\"PHARIA_DATA_NAMESPACE\")\n",
    "COLLECTION = os.getenv(\"PHARIA_DATA_COLLECTION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define the API endpoints that connect to PhariaAI's core document services:\n",
    "\n",
    "- **DATA_PLATFORM_URL**: The endpoint for the PhariaData API service that manages document storage and transformations\n",
    "- **DOCUMENT_INDEX_API_URL**: The endpoint for the PhariaDocument Index service that enables vector search capabilities\n",
    "\n",
    "These endpoints are stored as environment variables, making them accessible to all the helper functions we create throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHARIA_API_BASE_URL = os.getenv(\"PHARIA_API_BASE_URL\")\n",
    "\n",
    "DATA_PLATFORM_URL = f\"{PHARIA_API_BASE_URL}/studio/data\"\n",
    "DOCUMENT_INDEX_API_URL = f\"{PHARIA_API_BASE_URL}/studio/search\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different Deployment Environments\n",
    "\n",
    "Aleph Alpha operates multiple deployment environments for different teams and use cases. It's crucial that you use the correct environment URLs to avoid access issues during this tutorial.\n",
    "\n",
    "**Product Team Environment (Default for this tutorial)**\n",
    "If you're part of the Product team or working on product-related tasks, use these URLs:\n",
    "\n",
    "```python\n",
    "Base URL Pattern: https://pharia-{service}.stage.product.pharia.com/\n",
    "```\n",
    "\n",
    "**Customer Team Environment**\n",
    "If you're part of the Customer team or working on customer-related tasks, use these URLs:\n",
    "```python\n",
    "Base URL Pattern: https://pharia-{service}.customer.pharia.com/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2. Create a document repository and collection\n",
    "\n",
    "A repository in PhariaData is a storage container that organises processed documents. In this tutorial, we create a repository named \"RAG_Tutorial_Repository\".\n",
    "\n",
    "The `get_or_create_repository` function checks if a repository with the specified name already exists and creates one if it does not. The function returns the repository ID, which is referenced in later steps when configuring the ingestion pipeline.\n",
    "\n",
    "Similarly, we create a collection, to act as the search container for our files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPOSITORY_NAME = os.getenv(\"REPOSITORY_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper function \n",
    "\n",
    "def get_or_create_repository(repository: dict) -> str:\n",
    "    \"\"\"Get or create a repository in the Data Platform.\"\"\"\n",
    "    dataplatform_base_url = DATA_PLATFORM_URL\n",
    "    name = repository[\"name\"]\n",
    "    url = f\"{dataplatform_base_url}/repositories?name={name}\"\n",
    "\n",
    "    token = TOKEN\n",
    "    response = requests.get(\n",
    "        url=url, headers={\"Authorization\": f\"Bearer {token}\"}, verify=False\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    page = response.json()\n",
    "\n",
    "    if page[\"total\"] > 0:\n",
    "        return page[\"repositories\"][0][\"repositoryId\"]\n",
    "    else:\n",
    "        url = f\"{dataplatform_base_url}/repositories\"\n",
    "        response = requests.post(\n",
    "            url=url,\n",
    "            json=repository,\n",
    "            headers={\"Authorization\": f\"Bearer {token}\"},\n",
    "            verify=False,\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        repo_created = response.json()\n",
    "        return repo_created[\"repositoryId\"]\n",
    "    \n",
    "def get_or_create_collection(namespace: str, collection: str) -> str:\n",
    "    \"\"\"Get or create a collection in the Document Index.\"\"\"\n",
    "    try:\n",
    "        di_base_url = DOCUMENT_INDEX_API_URL\n",
    "        url = f\"{di_base_url}/collections/{namespace}\"\n",
    "        token = TOKEN\n",
    "        response = requests.get(\n",
    "            url=url, headers={\"Authorization\": f\"Bearer {token}\"}, verify=False\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        collections_list = response.json()\n",
    "        \n",
    "        if len(collections_list) == 0 or collection not in collections_list:\n",
    "            url = f\"{di_base_url}/collections/{namespace}/{collection}\"\n",
    "            response = requests.put(\n",
    "                url=url, headers={\"Authorization\": f\"Bearer {token}\"}, verify=False\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return f\"{collection} created\"\n",
    "        else:\n",
    "            return f\"{collection} exists\"\n",
    "    except Exception as e:\n",
    "        return f\"{e}, Response: {response}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the repository\n",
    "\n",
    "repository_payload = {\n",
    "    \"name\": REPOSITORY_NAME,\n",
    "    \"mediaType\": \"jsonlines\",\n",
    "    \"modality\": \"text\",\n",
    "    \"schema\": None,\n",
    "}\n",
    "\n",
    "repository_id = get_or_create_repository(repository_payload)\n",
    "print(f\"Repository ID: {repository_id}\")\n",
    "\n",
    "collection_id = get_or_create_collection(NAMESPACE, COLLECTION)\n",
    "print(f\"Collection: {collection_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3. Configure a document upload stage\n",
    "\n",
    "A stage provides temporary storage for source documents before they are processed. In this step, we create a stage named \"DocumentStorageTutorialTest\" that uses the \"DocumentToMarkdown\" transformation to convert source documents.\n",
    "\n",
    "The stage configuration includes a trigger that defines what happens when source documents are uploaded. This trigger specifies the transformation to apply and where to store the results.\n",
    "\n",
    "The `get_or_create_stage` function returns a stage ID that is used when uploading documents in later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Environment Variables\n",
    "STAGE_NAME = os.getenv(\"STAGE_NAME\")\n",
    "TRANSFORMATION_NAME = os.getenv(\"TRANSFORMATION_NAME\")\n",
    "TRIGGER_NAME = os.getenv(\"TRIGGER_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper function \n",
    "\n",
    "def get_or_create_stage(stage: dict) -> str:\n",
    "    \"\"\"Get or create a stage in the Data Platform.\"\"\"\n",
    "    dataplatform_base_url = DATA_PLATFORM_URL\n",
    "    name = stage[\"name\"]\n",
    "    url = f\"{dataplatform_base_url}/stages?name={name}\"\n",
    "\n",
    "    token = TOKEN\n",
    "    response = requests.get(\n",
    "        url=url, headers={\"Authorization\": f\"Bearer {token}\"}, verify=False\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    page = response.json()\n",
    "\n",
    "    if page[\"total\"] > 0:\n",
    "        return page[\"stages\"][0][\"stageId\"]\n",
    "    else:\n",
    "        url = f\"{dataplatform_base_url}/stages\"\n",
    "        response = requests.post(\n",
    "            url=url,\n",
    "            json=stage,\n",
    "            headers={\"Authorization\": f\"Bearer {token}\"},\n",
    "            verify=False,\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        stage_created = response.json()\n",
    "        return stage_created[\"stageId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup stage\n",
    "\n",
    "stage_payload = {\n",
    "    \"name\": STAGE_NAME,\n",
    "    \"triggers\": [\n",
    "        {\n",
    "            \"transformationName\": TRANSFORMATION_NAME,\n",
    "            \"destinationType\": \"DataPlatform:Repository\",\n",
    "            \"connectorType\": \"DocumentIndex:Collection\",\n",
    "            \"name\": TRIGGER_NAME,\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "stage_id = get_or_create_stage(stage_payload)\n",
    "print(f\"Stage ID: {stage_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 4. Create and assign a searchable index for documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An index enables efficient searching of your document content. The `create_index_and_assign_to_collection` function creates an index with specified parameters and assigns it to your collection.\n",
    "\n",
    "The key parameters include:\n",
    "- `chunk_size`: Controls how documents are divided into searchable segments (256 tokens)\n",
    "- `chunk_overlap`: Defines the overlap between chunks to maintain context (10 tokens)\n",
    "- `embedding_type`: Specifies the vector embedding approach (\"asymmetric\")\n",
    "\n",
    "Once the index is assigned to your collection, any ingested documents are automatically processed according to these settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX = os.getenv(\"INDEX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper function\n",
    "def create_index_and_assign_to_collection(index_name: str, collection_name: str, namespace: str, chunk_size: int = 256, chunk_overlap: int = 10, embedding_type: str = \"asymmetric\") -> str:\n",
    "    \"\"\"Create an index in the Document Index.\"\"\"\n",
    "    token = TOKEN\n",
    "    document_index_base_url = DOCUMENT_INDEX_API_URL\n",
    "    url = f\"{document_index_base_url}/indexes/{namespace}/{index_name}\"\n",
    "    payload = {\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"chunk_overlap\": chunk_overlap,\n",
    "        \"embedding_type\": embedding_type\n",
    "    }\n",
    "    response = requests.put(url, json=payload, headers={\"Authorization\": f\"Bearer {token}\"})\n",
    "    response.raise_for_status()\n",
    "    print(f\"Index created: {index_name}\")\n",
    "\n",
    "    # Assign the index to the collection\n",
    "    url = f\"{document_index_base_url}/collections/{namespace}/{collection_name}/indexes/{index_name}\"\n",
    "    response = requests.put(url, headers={\"Authorization\": f\"Bearer {token}\"})\n",
    "    response.raise_for_status()\n",
    "    print(f\"Index '{index_name}' assigned to collection '{collection_name}' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_index_and_assign_to_collection(index_name=INDEX, collection_name=COLLECTION, namespace=NAMESPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 5. Set up automated document processing\n",
    "\n",
    "The trigger configuration defines what happens when source documents are uploaded to the stage. The `ingestion_context` object combines three key elements:\n",
    "\n",
    "1. The trigger name that identifies which trigger to activate\n",
    "2. The destination repository where processed documents are stored\n",
    "3. The collection and namespace where processed documents are indexed\n",
    "\n",
    "This context is included with source document uploads to instruct the system on how to process each document. When a source document is uploaded, the specified trigger automatically applies the transformation and indexes the processed document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestion_context = {\n",
    "    \"triggerName\": TRIGGER_NAME,\n",
    "    \"destinationContext\": {\"repositoryId\": repository_id},\n",
    "    \"connectorContext\": {\n",
    "        \"collection\": COLLECTION,\n",
    "        \"namespace\": NAMESPACE,\n",
    "    },\n",
    "}\n",
    "print(f\"Ingestion context: {ingestion_context}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 6. Upload and process documents\n",
    "\n",
    "With our infrastructure set-up complete (repository, stage, index, and trigger), we can now upload source documents to the PhariaAI platform. This section demonstrates how to upload source documents and initiate the document ingestion process.\n",
    "\n",
    "The document ingestion workflow transforms source documents into searchable processed documents through several steps: uploading to the stage, applying transformations, storing in the repository, and indexing for search.\n",
    "\n",
    "The `ingest_all_documents` helper function returns a DataFrame with details on each upload attempt, making it easy to track successes and failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper functions\n",
    "\n",
    "@retry(\n",
    "    stop=stop_after_attempt(3),\n",
    "    wait=wait_exponential(multiplier=1, min=2, max=10),\n",
    "    retry=retry_if_exception_type(requests.RequestException),\n",
    ")\n",
    "def ingest_document(\n",
    "    document_path: str, ingestion_context: dict, name: str, stage_id: str\n",
    ") -> dict:\n",
    "    \"\"\"Attempts to ingest a document and returns the ingestion result.\"\"\"\n",
    "    with open(document_path, mode=\"rb\") as file_reader:\n",
    "        dataplatform_base_url = DATA_PLATFORM_URL\n",
    "        url = f\"{dataplatform_base_url}/stages/{stage_id}/files\"\n",
    "        token = TOKEN\n",
    "        response = requests.post(\n",
    "            url=url,\n",
    "            headers={\"Authorization\": f\"Bearer {token}\"},\n",
    "            verify=False,\n",
    "            files={\n",
    "                \"name\": name,\n",
    "                \"sourceData\": file_reader,\n",
    "                \"ingestionContext\": json.dumps(ingestion_context),\n",
    "            },\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "\n",
    "        file_uploaded = response.json()\n",
    "        return {\n",
    "            \"file_id\": file_uploaded[\"fileId\"],\n",
    "            \"status\": \"Success\",\n",
    "            \"error_type\": None,\n",
    "            \"error_message\": None,\n",
    "        }\n",
    "    \n",
    "\n",
    "\n",
    "def ingest_all_documents(\n",
    "    directory_path: str, ingestion_context: dict, stage_id: str, max_workers: int = 3\n",
    "):\n",
    "    \"\"\"Ingest all files in a directory concurrently and store results in a DataFrame.\"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_file = {\n",
    "            executor.submit(\n",
    "                ingest_document,\n",
    "                os.path.join(directory_path, file),\n",
    "                ingestion_context,\n",
    "                file,\n",
    "                stage_id,\n",
    "            ): file\n",
    "            for file in os.listdir(directory_path)\n",
    "        }\n",
    "\n",
    "        for future in concurrent.futures.as_completed(future_to_file):\n",
    "            file_name = future_to_file[future]\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"file_path\": file_path,\n",
    "                        \"file_id\": result[\"file_id\"],\n",
    "                        \"status\": result[\"status\"],\n",
    "                        \"error_type\": result[\"error_type\"],\n",
    "                        \"error_message\": result[\"error_message\"],\n",
    "                    }\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while ingesting {file_path}: {e}\")\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"file_path\": file_path,\n",
    "                        \"file_id\": None,\n",
    "                        \"status\": \"Ingestion Failed\",\n",
    "                        \"error\": str(e),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingesting the files\n",
    "directory_path = \"files_to_upload\"\n",
    "df_results = ingest_all_documents(directory_path, ingestion_context, stage_id)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 7. Monitor the source document processing status\n",
    "\n",
    "After uploading source documents, you need to verify their processing status. The code in this section does the following:\n",
    "\n",
    "1. Extracts IDs of successfully uploaded source documents\n",
    "2. Retrieves the transformation ID\n",
    "3. Checks the status of each source document's transformation\n",
    "4. Extracts dataset IDs from completed transformations\n",
    "\n",
    "The `check_files_status` function combines all this information into a comprehensive report that shows which files completed processing and which encountered errors. The dataset IDs are particularly important as they are used to access your processed documents in subsequent operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_successful_document_ids(df: pd.DataFrame) -> list:\n",
    "    \"\"\"Retrieve a list of successful file_ids from the DataFrame.\"\"\"\n",
    "    return df[df[\"status\"] == \"Success\"][\"file_id\"].tolist()\n",
    "\n",
    "@retry(\n",
    "    stop=stop_after_attempt(3),\n",
    "    wait=wait_exponential(multiplier=1, min=2, max=10),\n",
    "    retry=retry_if_exception_type(requests.RequestException),\n",
    ")\n",
    "def check_status_of_ingestion(transformation_id: str, file_id: str) -> dict:\n",
    "    \"\"\"Query the status of the ingestion for a given transformation and file_id.\"\"\"\n",
    "    dataplatform_base_url = DATA_PLATFORM_URL\n",
    "    url = f\"{dataplatform_base_url}/transformations/{transformation_id}/runs?file_id={file_id}\"\n",
    "\n",
    "    token = TOKEN\n",
    "    response = requests.get(\n",
    "        url=url, headers={\"Authorization\": f\"Bearer {token}\"}, verify=False\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    page = response.json()\n",
    "\n",
    "    assert page[\"total\"] > 0\n",
    "    return page[\"runs\"][0]\n",
    "\n",
    "def get_transformation_id(name: str) -> str:\n",
    "    \"\"\"Get the transformation ID from the Data Platform.\"\"\"\n",
    "    dataplatform_base_url = DATA_PLATFORM_URL\n",
    "    url = f\"{dataplatform_base_url}/transformations?name={name}\"\n",
    "\n",
    "    token = TOKEN\n",
    "    response = requests.get(\n",
    "        url=url, headers={\"Authorization\": f\"Bearer {token}\"}, verify=False\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    page = response.json()\n",
    "\n",
    "    assert page[\"total\"] > 0\n",
    "    return page[\"transformations\"][0][\"transformationId\"]\n",
    "\n",
    "def check_files_status(transformation_id: str, df: pd.DataFrame, max_workers: int = 3):\n",
    "    \"\"\"Check the status of ingested files and store the results in a DataFrame.\"\"\"\n",
    "\n",
    "    successful_file_ids = get_successful_document_ids(df)\n",
    "    status_results = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_file = {\n",
    "            executor.submit(\n",
    "                check_status_of_ingestion, transformation_id, file_id\n",
    "            ): file_id\n",
    "            for file_id in successful_file_ids\n",
    "        }\n",
    "\n",
    "        for future in concurrent.futures.as_completed(future_to_file):\n",
    "            file_id = future_to_file[future]\n",
    "            try:\n",
    "                run = future.result()\n",
    "                output = json.dumps(run.get(\"output\", {}), indent=4)\n",
    "                status_results.append(\n",
    "                    {\n",
    "                        \"file_id\": file_id,\n",
    "                        \"run_id\": run[\"runId\"],\n",
    "                        \"status\": run[\"status\"],\n",
    "                        \"output\": output,\n",
    "                        \"error\": run[\"errors\"],\n",
    "                    }\n",
    "                )\n",
    "            except Exception as e:\n",
    "                status_results.append(\n",
    "                    {\n",
    "                        \"file_id\": file_id,\n",
    "                        \"status\": run[\"status\"],\n",
    "                        \"output\": None,\n",
    "                        \"error\": str(e),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return df.merge(\n",
    "        pd.DataFrame(status_results),\n",
    "        on=\"file_id\",\n",
    "        how=\"left\",\n",
    "        suffixes=(\"_ingestion\", \"\"),\n",
    "    )\n",
    "\n",
    "def get_successful_dataset_ids(df: pd.DataFrame) -> list:\n",
    "    \"\"\"Retrieve a list of successful dataset_ids from the DataFrame.\"\"\"\n",
    "    dataset_ids_list = []\n",
    "    for i in range(len(df)):\n",
    "        dataset_ids_list.append(json.loads(df[\"output\"][i]).get(\"datasetId\"))\n",
    "    return dataset_ids_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation_id = get_transformation_id(TRANSFORMATION_NAME)\n",
    "status_df = check_files_status(transformation_id, df_results)\n",
    "status_df.to_csv(\"ingestion_status.csv\", index=False)\n",
    "successful_dataset_ids = get_successful_dataset_ids(status_df[status_df[\"status\"] == \"completed\"])\n",
    "status_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 8. Interact with processed documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With source documents ingested and processed, you can now interact with your data in various ways:\n",
    "\n",
    "1. **Search operation**: The `search_text` function demonstrates semantic search against your indexed processed documents, finding content based on meaning rather than exact keyword matches\n",
    "\n",
    "2. **Document and metadata retrieval**: The `get_document_from_document_index` function retrieves a complete processed document and its metadata using the dataset ID\n",
    "\n",
    "3. **Text display**: The `display_processed_document_text` function shows how to access the actual content extracted from your source documents, helping you verify the quality of text extraction\n",
    "\n",
    "These operations showcase the fundamental ways to interact with your processed documents in PhariaAI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1. Searching document content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After successfully ingesting documents, one of the most valuable operations is searching through your content. This section demonstrates how to perform semantic searches against your indexed documents.\n",
    "\n",
    "The `search_text` function sends a query to the PhariaDocument Index API, which uses vector embeddings to find semantically relevant content. Unlike traditional keyword search, this approach can identify conceptually related information even when exact terms do not match.\n",
    "\n",
    "In this example, we search for content related to \"what is attention?\" and retrieve matches ranked by relevance. The results include document chunks that semantically align with the query, along with confidence scores indicating match quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def search_text(namespace: str, collection: str, text: str, index: str) -> dict:\n",
    "    di_base_url = DOCUMENT_INDEX_API_URL\n",
    "    url = f\"{di_base_url}/collections/{namespace}/{collection}/indexes/{index}/search\"\n",
    "\n",
    "    token = TOKEN\n",
    "    payload = {\"query\": [{\"modality\": \"text\", \"text\": text}]}\n",
    "    response = requests.post(\n",
    "        url=url,\n",
    "        json=payload,\n",
    "        headers={\"Authorization\": f\"Bearer {token}\"},\n",
    "        verify=False,\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_search = \"what is attention?\"\n",
    "search_result = search_text(\n",
    "    NAMESPACE, COLLECTION, text_to_search, index=INDEX\n",
    ")\n",
    "print(json.dumps(search_result, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2. Retrieving complete documents and metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While searching helps find specific information, sometimes you need to retrieve a complete document along with its metadata. This operation is useful when you want to examine a document's full context or access its associated properties.\n",
    "\n",
    "The `get_document_from_document_index` function retrieves a document using its dataset ID (obtained during the ingestion process). The response includes both the document content and additional metadata such as creation time, source information, and any custom properties attached during processing.\n",
    "\n",
    "This example retrieves the fourth document from our previously ingested set, demonstrating how to access specific documents directly when you know their IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def get_document_from_document_index(namespace, collection, dataset_id) -> dict:\n",
    "    di_base_url = DOCUMENT_INDEX_API_URL\n",
    "    url = f\"{di_base_url}/collections/{namespace}/{collection}/docs/{dataset_id}\"\n",
    "\n",
    "    token = TOKEN\n",
    "    response = requests.get(\n",
    "        url=url, headers={\"Authorization\": f\"Bearer {token}\"}, verify=False\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_documents = []\n",
    "\n",
    "for id in successful_dataset_ids:\n",
    "    document_from_di = get_document_from_document_index(\n",
    "        NAMESPACE, COLLECTION, id\n",
    "    )\n",
    "    all_documents.append(document_from_di)\n",
    "\n",
    "print(json.dumps(all_documents[0], indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3. Viewing extracted document text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To inspect the actual content extracted from your documents, you can retrieve and display the text chunks stored in the repository. This is useful for verifying extraction quality and understanding how your documents were segmented.\n",
    "\n",
    "The `display_text_extracted` function connects to the PhariaData repository and retrieves text chunks from a specific document. It displays each chunk sequentially, showing how the document was divided during processing.\n",
    "\n",
    "This operation helps you validate that your documents were properly processed and that the extracted text accurately represents the original content. It can be particularly valuable when troubleshooting search issues or refining your ingestion parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def display_processed_document_text(repository_id: str, dataset_id: str) -> None:\n",
    "    dataplatform_base_url = DATA_PLATFORM_URL\n",
    "    url = f\"{dataplatform_base_url}/repositories/{repository_id}/datasets/{dataset_id}/datapoints\"\n",
    "\n",
    "    token = TOKEN\n",
    "    response = requests.get(\n",
    "        url=url, headers={\"Authorization\": f\"Bearer {token}\"}, verify=False, stream=True\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    for line in response.iter_lines():\n",
    "        datapoint = json.loads(line.decode())\n",
    "        print(datapoint)\n",
    "\n",
    "\n",
    "display_processed_document_text(repository_id, successful_dataset_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this section, you successfully set up the complete document ingestion pipeline:\n",
    "\n",
    "✅ **Configured the environment** with connections to both the PhariaData and PhariaDocument Index APIs\n",
    "\n",
    "✅ **Built the foundation infrastructure**:\n",
    "   - Created a repository for storing processed documents\n",
    "   - Set up a stage for temporary source document storage\n",
    "   - Configured an index for enabling semantic search\n",
    "   - Established triggers for automating document processing\n",
    "\n",
    "✅ **Implemented document operations** with:\n",
    "   - Concurrent source document uploads with error handling\n",
    "   - Status monitoring for transformation processes\n",
    "   - Multiple ways to interact with processed documents\n",
    "\n",
    "Your source document collection is now properly ingested, processed, and ready for semantic search operations. This data foundation will serve as the basis for retrieval-augmented generation in the subsequent sections of this tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
