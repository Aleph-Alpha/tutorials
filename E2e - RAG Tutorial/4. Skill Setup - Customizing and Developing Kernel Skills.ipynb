{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Skill Setup - Customizing and Developing Kernel Skills\n",
    "\n",
    "This section focuses on extending your RAG application by customiszing the kernel skill and integrating with Studio for development tracing. You'll learn how to customize the Q&A skill template, modify prompts to generate structured responses, and use development tracing to debug and improve your skills.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **Document Collection**: You have completed the previous section on document ingestion and have at least one indexed document collection\n",
    "- **Lorem Ipsum**: ...\n",
    "\n",
    "\n",
    "## Skill Components\n",
    "\n",
    "The skill development process involves several key elements:\n",
    "\n",
    "- **Kernel Integration**: Connects custom code to the Pharia kernel\n",
    "- **Studio Tracing**: Provides debugging and performance insights during development\n",
    "- **Prompt Customization**: Controls how responses are structured and formatted\n",
    "- **Development Workflow**: Process for testing and refining your skills\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "In this section, you'll learn how to:\n",
    "\n",
    "1. Understand the Q&A Kernel Skill template structure\n",
    "2. Explore document retrieval options in the intelligence layer\n",
    "3. Customize prompts to generate structured responses\n",
    "4. Set up development tracing with Studio for debugging and optimization\n",
    "\n",
    "<br>\n",
    "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Understanding the Q&A Kernel Skill Template\n",
    "\n",
    "The Q&A skill template provides a foundational structure for implementing Retrieval-Augmented Generation (RAG). Let's examine the core components of the template to understand how it works.\n",
    "\n",
    "The basic structure of a Q&A kernel skill includes:\n",
    "- Input and output models\n",
    "- Document retrieval using the Document Index\n",
    "- Context assembly from retrieved documents\n",
    "- LLM prompting for answer generation\n",
    "\n",
    "Here's the standard Q&A skill template that was generated when you created your application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pharia_skill import ChatParams, Csi, IndexPath, Message, skill\n",
    "from pydantic import BaseModel\n",
    "\n",
    "NAMESPACE = \"Studio\" #Document Index Namespace, won't change\n",
    "COLLECTION = \"papers\"\n",
    "INDEX = \"asym-64\"\n",
    "\n",
    "\n",
    "class Input(BaseModel):\n",
    "    question: str\n",
    "    namespace: str = NAMESPACE\n",
    "    collection: str = COLLECTION\n",
    "    index: str = INDEX\n",
    "\n",
    "\n",
    "class Output(BaseModel):\n",
    "    answer: str | None\n",
    "    sources: list[str] | None # Sources used to generate the answer\n",
    "\n",
    "\n",
    "@skill\n",
    "def custom_rag(csi: Csi, input: Input) -> Output:\n",
    "    index = IndexPath(\n",
    "        namespace=input.namespace,\n",
    "        collection=input.collection,\n",
    "        index=input.index,\n",
    "    )\n",
    "\n",
    "    if not (documents := csi.search(index, input.question, 3, 0.5)):\n",
    "        return Output(answer=None)\n",
    "\n",
    "\n",
    "    # Extract document names from the documents list\n",
    "    document_names = list(set([d.document_path.name for d in documents]))\n",
    "\n",
    "    context = \"\\n\".join([d.content for d in documents])\n",
    "    content = f\"\"\"Using the provided context documents below, answer the following question accurately and comprehensively. If the information is directly available in the context documents, cite it clearly. If not, use your knowledge to fill in the gaps while ensuring that the response is consistent with the given information. Do not fabricate facts or make assumptions beyond what the context or your knowledge base provides. Ensure that the response is structured, concise, and tailored to the specific question being asked.\n",
    "\n",
    "Input: {context}\n",
    "\n",
    "Question: {input.question}\n",
    "\"\"\"\n",
    "    message = Message.user(content)\n",
    "    params = ChatParams(max_tokens=512)\n",
    "    response = csi.chat(\"llama-3.1-8b-instruct\", [message], params)\n",
    "    return Output(answer=response.message.content, sources=document_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down the key components:\n",
    "\n",
    "1. **Configuration Constants**: The template defines default values for `NAMESPACE`, `COLLECTION`, and `INDEX`, which specify where to search for documents.\n",
    "\n",
    "2. **Input/Output Models**: The skill uses Pydantic models to define the expected input (a question and optional search parameters) and output (the answer).\n",
    "\n",
    "3. **Document Retrieval**: The `csi.search()` method retrieves relevant documents based on the user's question, limiting to 3 results with a minimum relevance score of 0.5.\n",
    "\n",
    "4. **Context Assembly**: Retrieved documents are combined into a single context string that will be sent to the LLM.\n",
    "\n",
    "5. **Prompt Construction**: The template includes a basic prompt that instructs the LLM how to use the context to answer the question.\n",
    "\n",
    "6. **Answer Generation**: The `csi.chat()` method sends the prompt to the LLM and returns the generated response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updating your skill (rebuild & publish)\n",
    "To make sure that your changes to the skill code are taken over, remember to rebuild and republish your skill after making changes, using the commands from the previous section from within your the ```<your-application>\\skill``` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "#To set the right .env\n",
    "set -a && source ../.env\n",
    "\n",
    "#Build & publish the skill\n",
    "uv run pharia-skill build qa\n",
    "uv run pharia-skill publish qa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Change the Collection the Skill is Referring to\n",
    "Before we explore advanced features, let's update our skill to use the document collection we created in the earlier Data Setup section. By default, the Q&A template is configured to use a collection called \"papers\", but we want to use our own collection.\n",
    "The collection name is defined as a constant at the top of the skill file:\n",
    "\n",
    "```python\n",
    "NAMESPACE = \"Studio\" #Document Index Namespace, won't change\n",
    "COLLECTION = \"papers\"\n",
    "INDEX = \"asym-64\"\n",
    "```\n",
    "\n",
    "We need to update the ```COLLECTION``` constant to point to our collection created during the Document Ingestion phase:\n",
    "\n",
    "```python\n",
    "NAMESPACE = \"Studio\" #Document Index Namespace, won't change\n",
    "COLLECTION = \"pharia-tutorial-full\"\n",
    "INDEX = \"asym-64\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple change redirects your skill to use the documents that you ingested earlier instead of the default example collection. Now when users ask questions, the skill will search within your custom document collection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Document Retrieval Options in the Intelligence Layer \"Work In Progress\"\n",
    "\n",
    "The Q&A template uses the `search` method, but there are other retrieval methods available that offer different capabilities from the Intelligence Layer SDK. We are still working on transferring all of thhe capabilites provided in the IL SDK in the kernel search implementation. (https://github.com/Aleph-Alpha/intelligence-layer-sdk/blob/main/src/documentation/document_index.ipynb)\n",
    "\n",
    "Some of the alternative retrieval methods include:\n",
    "\n",
    "- **Hybrid Search**: Combines semantic and keyword-based search for better precision\n",
    "- **Metadata Filtering**: Restricts search to documents with specific metadata properties\n",
    "- **Multi-Query Search**: Generates multiple search queries from a single user question\n",
    "- **Document Reranking**: Re-scores retrieved documents based on relevance to the question\n",
    "\n",
    "For most applications, the standard `search` method provides a good balance of performance and relevance. As your application becomes more sophisticated, you can explore these alternative methods to improve retrieval quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Customizing Prompts for Structured Responses\n",
    "\n",
    "One of the most effective ways to improve the quality of your RAG application is to customize the prompt that is sent to the LLM. By modifying the prompt, you can control the format, style, and content of the generated responses.\n",
    "\n",
    "Let's look at how you can modify the prompt to generate more structured responses. For example, you might want responses that:\n",
    "\n",
    "- Include specific sections like \"Summary\" and \"Details\"\n",
    "- Clearly indicate which parts are from the documents and which are from the LLM's knowledge\n",
    "- Follow a consistent formatting style\n",
    "\n",
    "Here's how you could modify the prompt in the Q&A skill template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a modified prompt for structured responses\n",
    "content = f\"\"\"Using the provided context documents below, answer the following question. Format your response with the following sections: 1. Summary: A brief 1-2 sentence answer to the question 2. Details: A comprehensive explanation with specific information from the context 3. Sources: References to the specific parts of the context you used, if applicable If the information is not available in the context documents, clearly state this and provide a general response based on your knowledge, marked as [GENERAL KNOWLEDGE]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement this change, you would replace the `content` variable in your skill function with the modified prompt above. This instructs the LLM to structure its response with specific sections, making the information more organized and easier to read.\n",
    "\n",
    "For even more control, you can use a system message to set persistent instructions for the LLM:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with different prompt variations to find the structure that works best for your specific use case. Keep in mind that the prompt should be clear and specific about what you want the LLM to do, while leaving enough flexibility for it to generate helpful responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Setting Up Development Tracing with Studio\n",
    "\n",
    "Development tracing is a powerful feature that allows you to debug and optimize your skills by sending execution data to PhariaStudio. This gives you visibility into each step of the RAG process, from document retrieval to answer generation.\n",
    "\n",
    "#### Understanding Tracing Benefits\n",
    "\n",
    "Tracing helps you:\n",
    "- Visualize the exact documents retrieved for each question\n",
    "- Evaluate the relevance of retrieved documents\n",
    "- Examine how the prompt is constructed\n",
    "- Measure performance metrics like retrieval and generation time\n",
    "- Identify bottlenecks in your RAG pipeline\n",
    "\n",
    "#### 5.1 Configuration Setup\n",
    "\n",
    "First, you need to configure the environment variables in the `<your-application>\\skill` folder. These variables enable your skill to interact with both the Kernel (for execution) and Studio (for tracing):\n",
    "\n",
    "```bash\n",
    "PHARIA_KERNEL_ADDRESS=https://pharia-kernel.your-deployment.pharia.com/\n",
    "PHARIA_AI_TOKEN=example-token-value\n",
    "PHARIA_STUDIO_ADDRESS=.https://pharia-studio.your-deployment.pharia.com\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Adding Dependencies for Tracing\n",
    "\n",
    "\n",
    "To enable tracing from within Jupyter notebooks, we need to add the `pharia-skill` dependency to our environment. Run this command in your terminal (in the same folder as this notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "poetry add pharia-skill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.A Creating a Test Function\n",
    "\n",
    "Now we'll define a function that calls our skill with tracing enabled. This allows us to test the skill independently and quickly iterate on improvements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper Function\n",
    "from pharia_skill.testing import DevCsi\n",
    "from rag_tutorial.skill.qa import IndexPath, Input, custom_rag\n",
    "\n",
    "def test_tracing():\n",
    "    csi = DevCsi().with_studio(\"rag-tutorial\")\n",
    "    index = IndexPath(\n",
    "        namespace=\"Studio\",\n",
    "        collection=\"papers\",\n",
    "        index=\"asym-64\",\n",
    "    )\n",
    "    input = Input(\n",
    "        question=\"What is an encoder?\",\n",
    "        namespace=index.namespace,\n",
    "        collection=index.collection,\n",
    "        index=index.index,\n",
    "    )\n",
    "    result = custom_rag(csi, input)\n",
    "    assert \"network\" in result.answer and \"layers\" in result.answer\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the test function, we need to load the environment variables from our skill's .env file and then call the function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"rag_tutorial/skill/.env\")\n",
    "test_tracing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.B (Alternative) Running Tests from Terminal\n",
    "\n",
    "Alternatively you can also add the test_tracing function to your qa_test.py file and invoke it from the terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "uv run pytest -k test_tracing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach could be useful for integrating tracing into your continuous integration workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 Viewing Traces in Studio\n",
    "After executing the skill with tracing enabled, you can view the detailed trace in PhariaStudio:\n",
    "\n",
    "1. Navigate to your PhariaStudio URL\n",
    "2. Go to the \"Traces\" section in the left sidebar\n",
    "3. Find your trace by name (in our example, \"rag-tutorial\")\n",
    "4. Click on the trace to view the detailed execution flow\n",
    "\n",
    "In the trace view, you'll see:\n",
    "\n",
    "- The input question\n",
    "- The documents retrieved from the index with their relevance scores\n",
    "- The exact prompt sent to the LLM\n",
    "- The generated response\n",
    "- Additional metrics for each step of the process\n",
    "\n",
    "This information is invaluable for understanding how your skill is performing and identifying opportunities for optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this section, you've learned how to customize and optimize your RAG application by working with the kernel skill:\n",
    "\n",
    "✅ **Explored the Q&A skill template** and its key components for implementing RAG\n",
    "\n",
    "✅ **Learned about different document retrieval options** available in the Intelligence Layer SDK\n",
    "\n",
    "✅ **Customized prompts** to generate more structured and informative responses\n",
    "\n",
    "✅ **Set up development tracing** with Studio to debug and optimize your skill\n",
    "\n",
    "With these skills, you can now create more sophisticated RAG applications that deliver high-quality, structured responses to user queries. In the next section, we'll look at how to evaluate and improve the performance of your RAG application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
