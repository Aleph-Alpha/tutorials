{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Skill setup - Customising and developing PhariaKernel Skills\n",
    "<a id=\"skill-setup\"></a>\n",
    "\n",
    "\n",
    "This section focuses on extending your RAG application by customising the PhariaKernel Skill and integrating with PhariaStudio for development tracing. You will learn how to customise the Q&A Skill template, modify prompts to generate structured responses, and use development tracing to debug and improve your Skills.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **Document collection**: You have completed the previous section on document ingestion and have at least one indexed document collection\n",
    "- **Permissions**: The *StudioUser* permission, as described in [User Setup](1.%20Introduction%20-%20Getting%20Started.ipynb#user-setup)\n",
    "\n",
    "\n",
    "## Skill components\n",
    "\n",
    "The Skill development process involves several key elements:\n",
    "\n",
    "- **PhariaKernel Integration**: Connects custom code to PhariaKernel\n",
    "- **PhariaStudio tracing**: Provides debugging and performance insights during development\n",
    "- **Prompt customisation**: Controls how responses are structured and formatted\n",
    "- **Development workflow**: The process for testing and refining your Skills\n",
    "\n",
    "## What you will learn\n",
    "\n",
    "1. How to understand the Q&A PhariaKernel Skill template structure\n",
    "2. How to explore document retrieval options in the intelligence layer\n",
    "3. How to customise prompts to generate structured responses\n",
    "4. How to set up development tracing with PhariaStudio for debugging and optimisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure\n",
    "### 1.1 Understand the Q&A PhariaKernel Skill template\n",
    "\n",
    "The Q&A Skill template provides a foundational structure for implementing Retrieval-Augmented Generation (RAG). We examine the core components of the template to understand how it works. You can see the standard template by navigating to ```<your-application>\\skill\\qa.py```\n",
    "\n",
    "The basic structure of a Q&A PhariaKernel Skill includes:\n",
    "- Input and output models\n",
    "- Document retrieval using PhariaDocument Index\n",
    "- Context assembly from retrieved documents\n",
    "- LLM prompting for answer generation\n",
    "\n",
    "The key components can be explained as follows:\n",
    "\n",
    "1. **Configuration constants**: The template defines default values for `NAMESPACE`, `COLLECTION`, and `INDEX`, which specify where to search for documents\n",
    "\n",
    "2. **Input/output models**: The Skill uses Pydantic models to define the expected input (a question and optional search parameters) and output (the answer)\n",
    "\n",
    "3. **Document retrieval**: The `csi.search()` method retrieves relevant documents based on the user's question, with a limit of three results with a minimum relevance score of 0.5\n",
    "\n",
    "4. **Context assembly**: Retrieved documents are combined into a single context string to be sent to the LLM\n",
    "\n",
    "5. **Prompt construction**: The template includes a basic prompt that instructs the LLM how to use the context to answer the question\n",
    "\n",
    "6. **Answer generation**: The `csi.chat()` method sends the prompt to the LLM and returns the generated response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Replace the Q&A PhariaKernel Skill template\n",
    "\n",
    "Before we explore advanced features, we will update our Skill to use the document collection we created in the [Data Setup](3.%20Data%20Setup%20-%20Setting%20Up%20the%20Document%20Ingestion%20Pipeline.ipynb#data-setup) section.\n",
    "\n",
    "By default, the Q&A template is configured to use a collection called \"papers\", but we want to use our own collection.\n",
    "The collection name is defined as a constant at the top of the Skill file:\n",
    "\n",
    "```python\n",
    "NAMESPACE = \"Studio\" # PhariaDocument Index namespace, will not change\n",
    "COLLECTION = \"papers\"\n",
    "INDEX = \"asym-64\"\n",
    "```\n",
    "\n",
    "We need to update the ```COLLECTION``` constant to point to our collection created during the document ingestion part of this tutorial:\n",
    "\n",
    "```python\n",
    "# TODO: collection and index name should be same as what was created in the previous notebook\n",
    "NAMESPACE = \"Studio\" #Document Index Namespace, won't change\n",
    "COLLECTION = \"pharia-tutorial-rag\"\n",
    "INDEX = \"rag-tutorial-index\"\n",
    "```\n",
    "\n",
    "This change will redirect your Skill to use the documents that you ingested earlier instead of the default example collection. Now when users ask questions, the Skill searches within your custom document collection.\n",
    "\n",
    "\n",
    "**Below we provide an updated QA skill that incorporates these changes. Further in the updated version we also return the sources with ```sources: list[str] | None``` and extract document names from the documents list with ```document_names = list(set([d.document_path.name for d in documents]))``` to later be able to point to our sources used**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pharia_skill import ChatParams, Csi, IndexPath, Message, skill\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# TODO: If you have used custom names for these values in the Data Setup step, please use these here\n",
    "NAMESPACE = \"Studio\" #Document Index Namespace, won't change\n",
    "COLLECTION = \"pharia-tutorial-rag\"\n",
    "INDEX = \"rag-tutorial-index\"\n",
    "\n",
    "\n",
    "class Input(BaseModel):\n",
    "    question: str\n",
    "    namespace: str = NAMESPACE\n",
    "    collection: str = COLLECTION\n",
    "    index: str = INDEX\n",
    "\n",
    "\n",
    "class Output(BaseModel):\n",
    "    answer: str | None = None\n",
    "    sources: list[str] | None = None # Sources used to generate the answer\n",
    "\n",
    "\n",
    "@skill\n",
    "def custom_rag(csi: Csi, input: Input) -> Output:\n",
    "    index = IndexPath(\n",
    "        namespace=input.namespace,\n",
    "        collection=input.collection,\n",
    "        index=input.index,\n",
    "    )\n",
    "\n",
    "    if not (documents := csi.search(index, input.question, 3, 0.5)):\n",
    "        return Output(answer=None, sources=None)\n",
    "\n",
    "    documents_metadata = [\n",
    "        csi.document_metadata(result.document_path) for result in documents\n",
    "    ]\n",
    "\n",
    "    document_names = list(set([Path(d.get(\"fileName\")).name for d in documents_metadata]))\n",
    "\n",
    "    context = \"\\n\".join([d.content for d in documents])\n",
    "    content = f\"\"\"Using the provided context documents below, answer the following question accurately and comprehensively. If the information is directly available in the context documents, cite it clearly. If not, use your knowledge to fill in the gaps while ensuring that the response is consistent with the given information. Do not fabricate facts or make assumptions beyond what the context or your knowledge base provides. Ensure that the response is structured, concise, and tailored to the specific question being asked.\n",
    "\n",
    "Input: {context}\n",
    "\n",
    "Question: {input.question}\n",
    "\"\"\"\n",
    "    message = Message.user(content)\n",
    "    params = ChatParams(max_tokens=512)\n",
    "    response = csi.chat(\"llama-3.1-8b-instruct\", [message], params)\n",
    "    return Output(answer=response.message.content, sources=document_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updating your Skill (rebuild and publish)\n",
    "To ensure that your changes to the Skill code are adopted, remember to rebuild and republish your Skill after making changes. Use the commands from the previous section from within the ```<your-application>\\skill``` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  If you are using linux based system, you can use the following command to source the .env file\n",
    "set -a && source ../.env\n",
    "\n",
    "# If you are using Windows Powershell, you can use the following command to source the .env file\n",
    "Get-Content \"../.env\" | ForEach-Object {\n",
    "    if ($_ -match '^\\s*([^#][^=]+?)\\s*=\\s*(.*)\\s*$') {\n",
    "        $key = $matches[1].Trim()\n",
    "        $value = $matches[2].Trim('\"')  # Optional: remove quotes\n",
    "        [System.Environment]::SetEnvironmentVariable($key, $value, \"Process\")\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# Build & publish the Skill\n",
    "uv run pharia-skill build qa\n",
    "uv run pharia-skill publish qa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Customise prompts for structured responses\n",
    "\n",
    "One of the most effective ways to improve the quality of your RAG application is to customise the prompt that is sent to the LLM. By modifying the prompt, you can control the format, style, and content of the generated responses.\n",
    "\n",
    "We will explore how to modify the prompt to generate more structured responses. For example, you might want responses that:\n",
    "\n",
    "- Include specific sections like \"Summary\" and \"Details\"\n",
    "- Clearly indicate which parts are from the documents and which are from the LLM's knowledge\n",
    "- Follow a consistent formatting style\n",
    "\n",
    "The following shows how you could modify the prompt in the Q&A Skill template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a modified prompt for structured responses\n",
    "content = \"\"\"\n",
    "    Using the provided context documents below, answer the following question. \n",
    "    Format your response with the following sections: \n",
    "        1. SUMMARY: A brief 1-2 sentence answer to the question \n",
    "        2. DETAILS: A comprehensive explanation with specific information from the context \n",
    "        3. SOURCES: References to the specific parts of the context you used, if applicable If the information is not available in the context documents, clearly state this and provide a general response based on your knowledge, marked as [GENERAL KNOWLEDGE].\n",
    "\n",
    "    Input: {context}\n",
    "\n",
    "    Question: {input.question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement this change, you replace the `content` variable in your Skill function with the modified prompt above. This instructs the LLM to structure its response with specific sections, making the information more organised and easier to read.\n",
    "\n",
    "For even more control, you can use a system message to set persistent instructions for the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with different prompt variations to find the structure that works best for your specific use case. Keep in mind that the prompt should be clear and specific about what you want the LLM to do, while leaving enough flexibility for it to generate helpful responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Set up development tracing with PhariaStudio\n",
    "\n",
    "Development tracing is a powerful feature that allows you to debug and optimise your Skills by sending execution data to PhariaStudio. This gives you visibility into each step of the RAG process, from document retrieval to answer generation.\n",
    "\n",
    "Tracing helps you to do the following:\n",
    "\n",
    "- Visualise the exact documents retrieved for each question\n",
    "- Evaluate the relevance of retrieved documents\n",
    "- Examine how the prompt is constructed\n",
    "- Measure performance metrics like retrieval and generation time\n",
    "- Identify bottlenecks in your RAG pipeline\n",
    "\n",
    "#### 4.1 Configuring your setup\n",
    "\n",
    "First, you need to create an `.env` file in the `<your-application>/skill` folder with the following variables. These variables enable your skill to interact with both the Kernel (for execution) and Studio (for tracing):\n",
    "\n",
    "```bash\n",
    "PHARIA_KERNEL_ADDRESS=https://pharia-kernel.your-deployment.pharia.com/\n",
    "PHARIA_AI_TOKEN=example-token-value\n",
    "PHARIA_STUDIO_ADDRESS=.https://pharia-studio.your-deployment.pharia.com\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.A Creating a test function\n",
    "\n",
    "Now we define a function that calls our Skill with tracing enabled. This allows us to test the Skill independently and quickly iterate on improvements The core of this testing is the **DevCsi().with_studio()** function.\n",
    "\n",
    "The **DevCsi** class serves as a local testing interface that connects your Skill to a running Pharia Kernel instance via HTTP. This enables testing against the same services used in production.\n",
    "\n",
    "The **with_studio()** function extends your DevCsi instance with tracing capabilities. It creates a StudioExporter that captures execution traces and registers it with the trace collector in Studio to monitor your skill's execution flow. The **project parameter (\"rag-tutorial\")** specifies the Studio project name where your traces will be stored. If the project doesn't exist, it will be automatically created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function\n",
    "from pharia_skill.testing import DevCsi\n",
    "from <your-app-name>.skill.qa import IndexPath, Input, custom_rag # Update this with the app name\n",
    "\n",
    "def test_tracing():\n",
    "    csi = DevCsi().with_studio(\"<your-project-name>\") #This creates a project in the Studio interface with the given name\n",
    "    index = IndexPath(\n",
    "        namespace=\"Studio\",\n",
    "        collection=\"pharia-tutorial-rag\",\n",
    "        index=\"rag-tutorial-index\",\n",
    "    )\n",
    "    input = Input(\n",
    "        question=\"What is an encoder?\",\n",
    "        namespace=index.namespace,\n",
    "        collection=index.collection,\n",
    "        index=index.index,\n",
    "    )\n",
    "    result = custom_rag(csi, input)\n",
    "    # Debug: Print the actual answer\n",
    "    print(\"ANSWER:\" + result.answer)\n",
    "    print(\"\\nContains word 'network':\", \"network\" in result.answer.lower())\n",
    "    print(\"\\nContains word 'layers':\", \"layers\" in result.answer.lower())\n",
    "    assert \"network\" in result.answer and \"layers\" in result.answer #Make the error explicit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the test function, we need to load the environment variables from our Skill's .env file and then call the function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"<your-app-name>/skill/.env\")\n",
    "test_tracing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.B (Alternative) Running tests from the terminal\n",
    "\n",
    "Alternatively you can also add the `test_tracing` function to your qa_test.py file and invoke it from the terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "uv run pytest -k test_tracing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach can be useful for integrating tracing into your continuous integration workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Viewing traces in PhariaStudio\n",
    "\n",
    "After executing the Skill with tracing enabled, you can view the detailed trace in PhariaStudio:\n",
    "\n",
    "1. Navigate to your PhariaStudio URL\n",
    "2. Go to the \"Traces\" section in the left sidebar\n",
    "3. Find your trace by name (in our example, \"rag-tutorial\")\n",
    "4. Click on the trace to view the detailed execution flow\n",
    "\n",
    "The trace view displays:\n",
    "\n",
    "- The input question\n",
    "- The documents retrieved from the index with their relevance scores\n",
    "- The exact prompt sent to the LLM\n",
    "- The generated response\n",
    "- Additional metrics for each step of the process\n",
    "\n",
    "This information is invaluable to understand how your Skill is performing and to identify opportunities for optimisation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this section, you customised and optimised your RAG application by working with the PhariaKernel Skill:\n",
    "\n",
    "✅ **Explored the Q&A Skill template** and its key components for implementing RAG\n",
    "\n",
    "✅ **Learned about different document retrieval options** available in the Intelligence Layer SDK\n",
    "\n",
    "✅ **Customised prompts** to generate more structured and informative responses\n",
    "\n",
    "✅ **Set up development tracing** with PhariaStudio to debug and optimise your Skill\n",
    "\n",
    "After learning the above, you can now create more sophisticated RAG applications that deliver high-quality, structured responses to user queries. In the next section, we look at how to evaluate and improve the performance of your RAG application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
