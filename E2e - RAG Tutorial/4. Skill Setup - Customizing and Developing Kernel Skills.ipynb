{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Skill setup - Customising and developing PhariaKernel Skills\n",
    "<a id=\"skill-setup\"></a>\n",
    "\n",
    "\n",
    "This section focuses on extending your RAG application by customising the PhariaKernel Skill and integrating with PhariaStudio for development tracing. You will learn how to customise the Q&A Skill template, modify prompts to generate structured responses, and use development tracing to debug and improve your Skills.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **Document collection**: You have completed the previous section on document ingestion and have at least one indexed document collection\n",
    "- **Permissions**: The *StudioUser* permission, as described in [User Setup](1.%20Introduction%20-%20Getting%20Started.ipynb#user-setup)\n",
    "\n",
    "\n",
    "## Skill components\n",
    "\n",
    "The Skill development process involves several key elements:\n",
    "\n",
    "- **PhariaKernel Integration**: Connects custom code to PhariaKernel\n",
    "- **PhariaStudio tracing**: Provides debugging and performance insights during development\n",
    "- **Prompt customisation**: Controls how responses are structured and formatted\n",
    "- **Development workflow**: The process for testing and refining your Skills\n",
    "\n",
    "## What you will learn\n",
    "\n",
    "1. How to understand the Q&A PhariaKernel Skill template structure\n",
    "2. How to explore document retrieval options in the intelligence layer\n",
    "3. How to customise prompts to generate structured responses\n",
    "4. How to set up development tracing with PhariaStudio for debugging and optimisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure\n",
    "### 1. Understand the Q&A PhariaKernel Skill template\n",
    "\n",
    "The Q&A Skill template provides a foundational structure for implementing Retrieval-Augmented Generation (RAG). We examine the core components of the template to understand how it works.\n",
    "\n",
    "The basic structure of a Q&A PhariaKernel Skill includes:\n",
    "- Input and output models\n",
    "- Document retrieval using PhariaDocument Index\n",
    "- Context assembly from retrieved documents\n",
    "- LLM prompting for answer generation\n",
    "\n",
    "The following is the standard Q&A Skill template that was generated when you created your application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pharia_skill import ChatParams, Csi, IndexPath, Message, skill\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# TODO: Change these values to what shows in the 3rd notebook\n",
    "NAMESPACE = \"customer-playground\" #Document Index Namespace, won't change\n",
    "COLLECTION = \"pharia-tutorial-rag\"\n",
    "INDEX = \"pharia-tutorial-index\"\n",
    "\n",
    "\n",
    "class Input(BaseModel):\n",
    "    question: str\n",
    "    namespace: str = NAMESPACE\n",
    "    collection: str = COLLECTION\n",
    "    index: str = INDEX\n",
    "\n",
    "\n",
    "class Output(BaseModel):\n",
    "    answer: str | None\n",
    "    sources: list[str] | None # Sources used to generate the answer\n",
    "\n",
    "\n",
    "@skill\n",
    "def custom_rag(csi: Csi, input: Input) -> Output:\n",
    "    index = IndexPath(\n",
    "        namespace=input.namespace,\n",
    "        collection=input.collection,\n",
    "        index=input.index,\n",
    "    )\n",
    "\n",
    "    if not (documents := csi.search(index, input.question, 3, 0.5)):\n",
    "        return Output(answer=None)\n",
    "\n",
    "\n",
    "    # Extract document names from the documents list\n",
    "    document_names = list(set([d.document_path.name for d in documents]))\n",
    "\n",
    "    context = \"\\n\".join([d.content for d in documents])\n",
    "    content = f\"\"\"Using the provided context documents below, answer the following question accurately and comprehensively. If the information is directly available in the context documents, cite it clearly. If not, use your knowledge to fill in the gaps while ensuring that the response is consistent with the given information. Do not fabricate facts or make assumptions beyond what the context or your knowledge base provides. Ensure that the response is structured, concise, and tailored to the specific question being asked.\n",
    "\n",
    "Input: {context}\n",
    "\n",
    "Question: {input.question}\n",
    "\"\"\"\n",
    "    message = Message.user(content)\n",
    "    params = ChatParams(max_tokens=512)\n",
    "    response = csi.chat(\"llama-3.1-8b-instruct\", [message], params)\n",
    "    return Output(answer=response.message.content, sources=document_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key components can be explained as follows:\n",
    "\n",
    "1. **Configuration constants**: The template defines default values for `NAMESPACE`, `COLLECTION`, and `INDEX`, which specify where to search for documents\n",
    "\n",
    "2. **Input/output models**: The Skill uses Pydantic models to define the expected input (a question and optional search parameters) and output (the answer)\n",
    "\n",
    "3. **Document retrieval**: The `csi.search()` method retrieves relevant documents based on the user's question, with a limit of three results with a minimum relevance score of 0.5\n",
    "\n",
    "4. **Context assembly**: Retrieved documents are combined into a single context string to be sent to the LLM\n",
    "\n",
    "5. **Prompt construction**: The template includes a basic prompt that instructs the LLM how to use the context to answer the question\n",
    "\n",
    "6. **Answer generation**: The `csi.chat()` method sends the prompt to the LLM and returns the generated response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updating your Skill (rebuild and publish)\n",
    "To ensure that your changes to the Skill code are adopted, remember to rebuild and republish your Skill after making changes. Use the commands from the previous section from within the ```<your-application>\\skill``` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# To set the right .env\n",
    "set -a && source ../.env\n",
    "\n",
    "# Build & publish the Skill\n",
    "uv run pharia-skill build qa\n",
    "uv run pharia-skill publish qa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Change the collection the Skill refers to\n",
    "\n",
    "Before we explore advanced features, we will update our Skill to use the document collection we created in the [Data Setup](3.%20Data%20Setup%20-%20Setting%20Up%20the%20Document%20Ingestion%20Pipeline.ipynb#data-setup) section.\n",
    "\n",
    "By default, the Q&A template is configured to use a collection called \"papers\", but we want to use our own collection.\n",
    "The collection name is defined as a constant at the top of the Skill file:\n",
    "\n",
    "```python\n",
    "NAMESPACE = \"Studio\" # PhariaDocument Index namespace, will not change\n",
    "COLLECTION = \"papers\"\n",
    "INDEX = \"asym-64\"\n",
    "```\n",
    "\n",
    "We need to update the ```COLLECTION``` constant to point to our collection created during the document ingestion part of this tutorial:\n",
    "\n",
    "```python\n",
    "# TODO: collection and index name should be same as what was created in the previous notebook\n",
    "NAMESPACE = \"Studio\" #Document Index Namespace, won't change\n",
    "COLLECTION = \"pharia-tutorial-full\"\n",
    "INDEX = \"asym-64\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple change redirects your Skill to use the documents that you ingested earlier instead of the default example collection. Now when users ask questions, the Skill searches within your custom document collection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Document retrieval options in the Intelligence Layer \"Work In Progress\"\n",
    "\n",
    "The Q&A template uses the `search` method, but there are other retrieval methods available that offer different capabilities from the Intelligence Layer SDK. We are still working on transferring all of thhe capabilites provided in the IL SDK in the PhariaKernel search implementation. (https://github.com/Aleph-Alpha/intelligence-layer-sdk/blob/main/src/documentation/document_index.ipynb)\n",
    "\n",
    "Some of the alternative retrieval methods include:\n",
    "\n",
    "- **Hybrid search**: Combines semantic and keyword-based search for better precision\n",
    "- **Metadata filtering**: Restricts search to documents with specific metadata properties\n",
    "- **Multi-query search**: Generates multiple search queries from a single user question\n",
    "- **Document reranking**: Re-scores retrieved documents based on relevance to the question\n",
    "\n",
    "For most applications, the standard `search` method provides a good balance of performance and relevance. As your application becomes more sophisticated, you can explore these alternative methods to improve retrieval quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Customise prompts for structured responses\n",
    "\n",
    "One of the most effective ways to improve the quality of your RAG application is to customise the prompt that is sent to the LLM. By modifying the prompt, you can control the format, style, and content of the generated responses.\n",
    "\n",
    "We will explore how to modify the prompt to generate more structured responses. For example, you might want responses that:\n",
    "\n",
    "- Include specific sections like \"Summary\" and \"Details\"\n",
    "- Clearly indicate which parts are from the documents and which are from the LLM's knowledge\n",
    "- Follow a consistent formatting style\n",
    "\n",
    "The following shows how you could modify the prompt in the Q&A Skill template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a modified prompt for structured responses\n",
    "content = f\"\"\"Using the provided context documents below, answer the following question. Format your response with the following sections: 1. Summary: A brief 1-2 sentence answer to the question 2. Details: A comprehensive explanation with specific information from the context 3. Sources: References to the specific parts of the context you used, if applicable If the information is not available in the context documents, clearly state this and provide a general response based on your knowledge, marked as [GENERAL KNOWLEDGE]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement this change, you replace the `content` variable in your Skill function with the modified prompt above. This instructs the LLM to structure its response with specific sections, making the information more organised and easier to read.\n",
    "\n",
    "For even more control, you can use a system message to set persistent instructions for the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with different prompt variations to find the structure that works best for your specific use case. Keep in mind that the prompt should be clear and specific about what you want the LLM to do, while leaving enough flexibility for it to generate helpful responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Set up development tracing with PhariaStudio\n",
    "\n",
    "Development tracing is a powerful feature that allows you to debug and optimise your Skills by sending execution data to PhariaStudio. This gives you visibility into each step of the RAG process, from document retrieval to answer generation.\n",
    "\n",
    "Tracing helps you to do the following:\n",
    "\n",
    "- Visualise the exact documents retrieved for each question\n",
    "- Evaluate the relevance of retrieved documents\n",
    "- Examine how the prompt is constructed\n",
    "- Measure performance metrics like retrieval and generation time\n",
    "- Identify bottlenecks in your RAG pipeline\n",
    "\n",
    "#### 5.1 Configuring your setup\n",
    "\n",
    "First, you need to create an `.env` file in the `<your-application>/skill` folder with the following variables. These variables enable your skill to interact with both the Kernel (for execution) and Studio (for tracing):\n",
    "\n",
    "```bash\n",
    "PHARIA_KERNEL_ADDRESS=https://pharia-kernel.your-deployment.pharia.com/\n",
    "PHARIA_AI_TOKEN=example-token-value\n",
    "PHARIA_STUDIO_ADDRESS=.https://pharia-studio.your-deployment.pharia.com\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Adding dependencies for tracing\n",
    "\n",
    "\n",
    "To enable tracing from within Jupyter notebooks, we need to add the `pharia-skill` dependency to our environment. Run this command in your terminal (in the same folder as this notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "poetry add pharia-skill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.A Creating a test function\n",
    "\n",
    "Now we define a function that calls our Skill with tracing enabled. This allows us to test the Skill independently and quickly iterate on improvements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function\n",
    "from pharia_skill.testing import DevCsi\n",
    "from rag_tutorial.skill.qa import IndexPath, Input, custom_rag\n",
    "\n",
    "def test_tracing():\n",
    "    # TODO: explain what with_studio does and that it is the project name\n",
    "    csi = DevCsi().with_studio(\"rag-tutorial\")\n",
    "    index = IndexPath(\n",
    "        namespace=\"Studio\",\n",
    "        collection=\"papers\",\n",
    "        index=\"asym-64\",\n",
    "    )\n",
    "    input = Input(\n",
    "        question=\"What is an encoder?\",\n",
    "        namespace=index.namespace,\n",
    "        collection=index.collection,\n",
    "        index=index.index,\n",
    "    )\n",
    "    result = custom_rag(csi, input)\n",
    "    assert \"network\" in result.answer and \"layers\" in result.answer\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the test function, we need to load the environment variables from our Skill's .env file and then call the function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# TODO: Fix path \n",
    "load_dotenv(\"../rag_tutorial/skill/.env\")\n",
    "test_tracing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.B (Alternative) Running tests from the terminal\n",
    "\n",
    "Alternatively you can also add the `test_tracing` function to your qa_test.py file and invoke it from the terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "uv run pytest -k test_tracing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach can be useful for integrating tracing into your continuous integration workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 Viewing traces in PhariaStudio\n",
    "\n",
    "After executing the Skill with tracing enabled, you can view the detailed trace in PhariaStudio:\n",
    "\n",
    "1. Navigate to your PhariaStudio URL\n",
    "2. Go to the \"Traces\" section in the left sidebar\n",
    "3. Find your trace by name (in our example, \"rag-tutorial\")\n",
    "4. Click on the trace to view the detailed execution flow\n",
    "\n",
    "The trace view displays:\n",
    "\n",
    "- The input question\n",
    "- The documents retrieved from the index with their relevance scores\n",
    "- The exact prompt sent to the LLM\n",
    "- The generated response\n",
    "- Additional metrics for each step of the process\n",
    "\n",
    "This information is invaluable to understand how your Skill is performing and to identify opportunities for optimisation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this section, you customised and optimised your RAG application by working with the PhariaKernel Skill:\n",
    "\n",
    "✅ **Explored the Q&A Skill template** and its key components for implementing RAG\n",
    "\n",
    "✅ **Learned about different document retrieval options** available in the Intelligence Layer SDK\n",
    "\n",
    "✅ **Customised prompts** to generate more structured and informative responses\n",
    "\n",
    "✅ **Set up development tracing** with PhariaStudio to debug and optimise your Skill\n",
    "\n",
    "After learning the above, you can now create more sophisticated RAG applications that deliver high-quality, structured responses to user queries. In the next section, we look at how to evaluate and improve the performance of your RAG application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
