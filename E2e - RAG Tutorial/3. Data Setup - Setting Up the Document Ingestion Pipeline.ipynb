{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data setup - Setting up the document ingestion pipeline\n",
    "<a id=\"data-setup\"></a>\n",
    "\n",
    "This section describes how to establish a complete document ingestion pipeline in PhariaAI. The ingestion pipeline is a crucial foundation for RAG applications, as it transforms source documents into searchable, AI-ready processed documents.\n",
    "\n",
    "## Pipeline components\n",
    "\n",
    "The pipeline consists of several interconnected components:\n",
    "\n",
    "- **Repository**: Stores the source documents and processed documents\n",
    "- **Collection**: Groups processed documents in a searchable container with unified access patterns and shared indexes\n",
    "- **Stage**: Provides temporary storage for source documents\n",
    "- **Transformation**: Converts raw files into structured, searchable content\n",
    "- **Index**: Enables efficient semantic search across your documents\n",
    "- **Trigger**: Automates the processing workflow when documents are uploaded\n",
    "\n",
    "The document ingestion workflow we will build transforms source documents into searchable processed documents in the following steps:\n",
    "- uploading to the stage\n",
    "- applying transformations\n",
    "- storing in the repository\n",
    "- indexing for search\n",
    "\n",
    "\n",
    "## What you will learn\n",
    "\n",
    "1. How to configure your environment and connection parameters\n",
    "2. How to create an ingestion pipeline with the PhariaData API\n",
    "3. How to upload documents and monitor their processing\n",
    "4. How to interact with your processed content through search and retrieval\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting, ensure you have the following:\n",
    "\n",
    "- **API token**: A valid Aleph Alpha API token with appropriate permissions\n",
    "- **API URLs**: Access to running instances of `pharia-data-api` and `document-index-api`\n",
    "- **Permissions**: The *StudioUser* permission, as described in [User Setup](1.%20Introduction%20-%20Getting%20Started.ipynb#user-setup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation options\n",
    "\n",
    "This tutorial uses various Python packages for API interaction and data processing. You have several options for setting up your environment:\n",
    "\n",
    "### Using Poetry (recommended)\n",
    "\n",
    "If you're working with the complete project repository that includes the `pyproject.toml` file:\n",
    "\n",
    "1. Install [poetry](https://python-poetry.org/docs/#installing-with-pipx) using `pipx` following the official instructions\n",
    "2. Run `poetry install` in the project directory to set up all dependencies automatically\n",
    "\n",
    "<br>\n",
    "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure\n",
    "\n",
    "This section guides you through setting up your document ingestion pipeline. You will first import necessary libraries, configure your environment, and then build the essential components for document processing. The workflow follows a systematic approach of creating a repository, setting up a document staging area, configuring an index, and establishing triggers for automated document transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import dependencies and configure the environment\n",
    "\n",
    "We begin by importing necessary dependencies and setting up the environment. We use standard Python libraries such as `requests` for API communication, `pandas` for data handling, as well as specialised libraries such as `tenacity` for robust error handling with retry mechanisms.\n",
    "\n",
    "The environment configuration establishes connections to two key PhariaAI services:\n",
    "- The PhariaData API for managing document transformations and storage\n",
    "- The PhariaDocument Index API for creating searchable indexes\n",
    "\n",
    "We use several key libraries for our document processing workflow. The code below imports all of these libraries and disables warnings to keep our notebook output clean:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `.env` using the following command and add your PhariaAI Token\n",
    "\n",
    "```bash\n",
    "cp .env.sample .env\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import concurrent.futures\n",
    "from dotenv import load_dotenv\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_exponential,\n",
    "    retry_if_exception_type,\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we configure the essential parameters that provide authentication and identify your workspace:\n",
    "\n",
    "- **TOKEN**: Your Aleph Alpha API authentication token, loaded from your environment file\n",
    "- **NAMESPACE**: The organisational namespace where your collections are stored (\"Studio\")\n",
    "- **COLLECTION**: The name of the document collection for this tutorial (\"pharia-tutorial-rag\")\n",
    "\n",
    "**Note:** The namespace identifier depends on your specific PhariaAI setup and permission level. The collection name can be freely chosen to help you organise and separate different RAG projects. Using descriptive collection names (like \"legal-contracts\" or \"product-documentation\") can help you manage multiple document sets within the same namespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setups\n",
    "load_dotenv(\".env\", override=True)\n",
    "\n",
    "TOKEN = os.getenv(\"PHARIA_AI_TOKEN\") #<your-token>\n",
    "NAMESPACE = os.getenv(\"PHARIA_DATA_NAMESPACE\")\n",
    "COLLECTION = os.getenv(\"PHARIA_DATA_COLLECTION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define the API endpoints that connect to PhariaAI's core document services:\n",
    "\n",
    "- **DATA_PLATFORM_URL**: The endpoint for the PhariaData API service that manages document storage and transformations\n",
    "- **DOCUMENT_INDEX_API_URL**: The endpoint for the PhariaDocument Index service that enables vector search capabilities\n",
    "\n",
    "These endpoints are stored as environment variables, making them accessible to all the helper functions we create throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PLATFORM_URL = os.getenv(\"PHARIA_DATA_PLATFORM_URL\")\n",
    "DOCUMENT_INDEX_API_URL = os.getenv(\"DOCUMENT_INDEX_API_URL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a document repository\n",
    "\n",
    "A repository in PhariaData is a storage container that organises processed documents. In this tutorial, we create a repository named \"DocumentSearch\".\n",
    "\n",
    "The `get_or_create_repository` function checks if a repository with the specified name already exists and creates one if it does not. The function returns the repository ID, which is referenced in later steps when configuring the ingestion pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPOSITORY_NAME = os.getenv(\"REPOSITORY_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper function \n",
    "\n",
    "def get_or_create_repository(repository: dict) -> str:\n",
    "    \"\"\"Get or create a repository in the Data Platform.\"\"\"\n",
    "    dataplatform_base_url = DATA_PLATFORM_URL\n",
    "    name = repository[\"name\"]\n",
    "    url = f\"{dataplatform_base_url}/api/v1/repositories?name={name}\"\n",
    "\n",
    "    token = TOKEN\n",
    "    response = requests.get(\n",
    "        url=url, headers={\"Authorization\": f\"Bearer {token}\"}, verify=False\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    page = response.json()\n",
    "\n",
    "    if page[\"total\"] > 0:\n",
    "        return page[\"repositories\"][0][\"repositoryId\"]\n",
    "    else:\n",
    "        url = f\"{dataplatform_base_url}/api/v1/repositories\"\n",
    "        response = requests.post(\n",
    "            url=url,\n",
    "            json=repository,\n",
    "            headers={\"Authorization\": f\"Bearer {token}\"},\n",
    "            verify=False,\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        repo_created = response.json()\n",
    "        return repo_created[\"repositoryId\"]\n",
    "    \n",
    "def get_or_create_collection(namespace: str, collection: str) -> str:\n",
    "    \"\"\"Get or create a collection in the Document Index.\"\"\"\n",
    "    try:\n",
    "        di_base_url = DOCUMENT_INDEX_API_URL\n",
    "        url = f\"{di_base_url}/collections/{namespace}\"\n",
    "        token = TOKEN\n",
    "        response = requests.get(\n",
    "            url=url, headers={\"Authorization\": f\"Bearer {token}\"}, verify=False\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        collections_list = response.json()\n",
    "        \n",
    "        if len(collections_list) == 0 or collection not in collections_list:\n",
    "            url = f\"{di_base_url}/collections/{namespace}/{collection}\"\n",
    "            response = requests.put(\n",
    "                url=url, headers={\"Authorization\": f\"Bearer {token}\"}, verify=False\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return f\"{collection} created\"\n",
    "        else:\n",
    "            return f\"{collection} exists\"\n",
    "    except Exception as e:\n",
    "        return f\"{e}, Response: {response}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository ID: 2dda3c31-178a-4bef-af21-33186619337d\n",
      "Collection: pharia-tutorial-rag created\n"
     ]
    }
   ],
   "source": [
    "## Create the repository\n",
    "\n",
    "repository_payload = {\n",
    "    \"name\": REPOSITORY_NAME,\n",
    "    \"mediaType\": \"jsonlines\",\n",
    "    \"modality\": \"text\",\n",
    "    \"schema\": None,\n",
    "}\n",
    "\n",
    "repository_id = get_or_create_repository(repository_payload)\n",
    "print(f\"Repository ID: {repository_id}\")\n",
    "\n",
    "collection_id = get_or_create_collection(NAMESPACE, COLLECTION)\n",
    "print(f\"Collection: {collection_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Configure a document upload stage\n",
    "\n",
    "A stage provides temporary storage for source documents before they are processed. In this step, we create a stage named \"DocumentStorageTutorialTest\" that uses the \"DocumentToMarkdown\" transformation to convert source documents.\n",
    "\n",
    "The stage configuration includes a trigger that defines what happens when source documents are uploaded. This trigger specifies the transformation to apply and where to store the results.\n",
    "\n",
    "The `get_or_create_stage` function returns a stage ID that is used when uploading documents in later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Environment Variables\n",
    "STAGE_NAME = os.getenv(\"STAGE_NAME\")\n",
    "TRANSFORMATION_NAME = os.getenv(\"TRANSFORMATION_NAME\")\n",
    "TRIGGER_NAME = os.getenv(\"TRIGGER_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper function \n",
    "\n",
    "def get_or_create_stage(stage: dict) -> str:\n",
    "    \"\"\"Get or create a stage in the Data Platform.\"\"\"\n",
    "    dataplatform_base_url = DATA_PLATFORM_URL\n",
    "    name = stage[\"name\"]\n",
    "    url = f\"{dataplatform_base_url}/api/v1/stages?name={name}\"\n",
    "\n",
    "    token = TOKEN\n",
    "    response = requests.get(\n",
    "        url=url, headers={\"Authorization\": f\"Bearer {token}\"}, verify=False\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    page = response.json()\n",
    "\n",
    "    if page[\"total\"] > 0:\n",
    "        return page[\"stages\"][0][\"stageId\"]\n",
    "    else:\n",
    "        url = f\"{dataplatform_base_url}/api/v1/stages\"\n",
    "        response = requests.post(\n",
    "            url=url,\n",
    "            json=stage,\n",
    "            headers={\"Authorization\": f\"Bearer {token}\"},\n",
    "            verify=False,\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        stage_created = response.json()\n",
    "        return stage_created[\"stageId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage ID: 6649f6f1-66c7-4688-b372-7beed1913ce4\n"
     ]
    }
   ],
   "source": [
    "## Setup stage\n",
    "\n",
    "stage_payload = {\n",
    "    \"name\": STAGE_NAME,\n",
    "    \"triggers\": [\n",
    "        {\n",
    "            \"transformationName\": TRANSFORMATION_NAME,\n",
    "            \"destinationType\": \"DataPlatform:Repository\",\n",
    "            \"connectorType\": \"DocumentIndex:Collection\",\n",
    "            \"name\": TRIGGER_NAME,\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "stage_id = get_or_create_stage(stage_payload)\n",
    "print(f\"Stage ID: {stage_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create and assign a searchable index for documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An index enables efficient searching of your document content. The `create_index_and_assign_to_collection` function creates an index with specified parameters and assigns it to your collection.\n",
    "\n",
    "The key parameters include:\n",
    "- `chunk_size`: Controls how documents are divided into searchable segments (256 tokens)\n",
    "- `chunk_overlap`: Defines the overlap between chunks to maintain context (10 tokens)\n",
    "- `embedding_type`: Specifies the vector embedding approach (\"asymmetric\")\n",
    "\n",
    "Once the index is assigned to your collection, any ingested documents are automatically processed according to these settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX = os.getenv(\"INDEX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper function\n",
    "# TODO: Change default embedding type and point to documentation of different types\n",
    "def create_index_and_assign_to_collection(index_name: str, collection_name: str, namespace: str, chunk_size: int = 256, chunk_overlap: int = 10, embedding_type: str = \"asymmetric\") -> str:\n",
    "    \"\"\"Create an index in the Document Index.\"\"\"\n",
    "    token = TOKEN\n",
    "    document_index_base_url = DOCUMENT_INDEX_API_URL\n",
    "    url = f\"{document_index_base_url}/indexes/{namespace}/{index_name}\"\n",
    "    payload = {\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"chunk_overlap\": chunk_overlap,\n",
    "        \"embedding_type\": embedding_type\n",
    "    }\n",
    "    response = requests.put(url, json=payload, headers={\"Authorization\": f\"Bearer {token}\"})\n",
    "    response.raise_for_status()\n",
    "    print(f\"Index created: {index_name}\")\n",
    "\n",
    "    # Assign the index to the collection\n",
    "    url = f\"{document_index_base_url}/collections/{namespace}/{collection_name}/indexes/{index_name}\"\n",
    "    response = requests.put(url, headers={\"Authorization\": f\"Bearer {token}\"})\n",
    "    response.raise_for_status()\n",
    "    print(f\"Index '{index_name}' assigned to collection '{collection_name}' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index created: rag-tutorial-index\n",
      "Index 'rag-tutorial-index' assigned to collection 'pharia-tutorial-rag' \n"
     ]
    }
   ],
   "source": [
    "create_index_and_assign_to_collection(index_name=INDEX, collection_name=COLLECTION, namespace=NAMESPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Set up automated document processing\n",
    "\n",
    "The trigger configuration defines what happens when source documents are uploaded to the stage. The `ingestion_context` object combines three key elements:\n",
    "\n",
    "1. The trigger name that identifies which trigger to activate\n",
    "2. The destination repository where processed documents are stored\n",
    "3. The collection and namespace where processed documents are indexed\n",
    "\n",
    "This context is included with source document uploads to instruct the system on how to process each document. When a source document is uploaded, the specified trigger automatically applies the transformation and indexes the processed document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion context: {'triggerName': 'testTrigger - DocumentStorageTutorial', 'destinationContext': {'repositoryId': '2dda3c31-178a-4bef-af21-33186619337d'}, 'connectorContext': {'collection': 'pharia-tutorial-rag', 'namespace': 'Assistant'}}\n"
     ]
    }
   ],
   "source": [
    "ingestion_context = {\n",
    "    \"triggerName\": TRIGGER_NAME,\n",
    "    \"destinationContext\": {\"repositoryId\": repository_id},\n",
    "    \"connectorContext\": {\n",
    "        \"collection\": COLLECTION,\n",
    "        \"namespace\": NAMESPACE,\n",
    "    },\n",
    "}\n",
    "print(f\"Ingestion context: {ingestion_context}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Upload and process documents\n",
    "\n",
    "With our infrastructure set-up complete (repository, stage, index, and trigger), we can now upload source documents to the PhariaAI platform. This section demonstrates how to upload source documents and initiate the document ingestion process.\n",
    "\n",
    "The document ingestion workflow transforms source documents into searchable processed documents through several steps: uploading to the stage, applying transformations, storing in the repository, and indexing for search.\n",
    "\n",
    "The `ingest_all_documents` helper function returns a DataFrame with details on each upload attempt, making it easy to track successes and failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper functions\n",
    "\n",
    "@retry(\n",
    "    stop=stop_after_attempt(3),\n",
    "    wait=wait_exponential(multiplier=1, min=2, max=10),\n",
    "    retry=retry_if_exception_type(requests.RequestException),\n",
    ")\n",
    "def ingest_document(\n",
    "    document_path: str, ingestion_context: dict, name: str, stage_id: str\n",
    ") -> dict:\n",
    "    \"\"\"Attempts to ingest a document and returns the ingestion result.\"\"\"\n",
    "    with open(document_path, mode=\"rb\") as file_reader:\n",
    "        dataplatform_base_url = DATA_PLATFORM_URL\n",
    "        url = f\"{dataplatform_base_url}/api/v1/stages/{stage_id}/files\"\n",
    "        token = TOKEN\n",
    "        response = requests.post(\n",
    "            url=url,\n",
    "            headers={\"Authorization\": f\"Bearer {token}\"},\n",
    "            verify=False,\n",
    "            files={\n",
    "                \"name\": name,\n",
    "                \"sourceData\": file_reader,\n",
    "                \"ingestionContext\": json.dumps(ingestion_context),\n",
    "            },\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "\n",
    "        file_uploaded = response.json()\n",
    "        return {\n",
    "            \"file_id\": file_uploaded[\"fileId\"],\n",
    "            \"status\": \"Success\",\n",
    "            \"error_type\": None,\n",
    "            \"error_message\": None,\n",
    "        }\n",
    "    \n",
    "\n",
    "\n",
    "def ingest_all_documents(\n",
    "    directory_path: str, ingestion_context: dict, stage_id: str, max_workers: int = 3\n",
    "):\n",
    "    \"\"\"Ingest all files in a directory concurrently and store results in a DataFrame.\"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_file = {\n",
    "            executor.submit(\n",
    "                ingest_document,\n",
    "                os.path.join(directory_path, file),\n",
    "                ingestion_context,\n",
    "                file,\n",
    "                stage_id,\n",
    "            ): file\n",
    "            for file in os.listdir(directory_path)\n",
    "        }\n",
    "\n",
    "        for future in concurrent.futures.as_completed(future_to_file):\n",
    "            file_name = future_to_file[future]\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"file_path\": file_path,\n",
    "                        \"file_id\": result[\"file_id\"],\n",
    "                        \"status\": result[\"status\"],\n",
    "                        \"error_type\": result[\"error_type\"],\n",
    "                        \"error_message\": result[\"error_message\"],\n",
    "                    }\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while ingesting {file_path}: {e}\")\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"file_path\": file_path,\n",
    "                        \"file_id\": None,\n",
    "                        \"status\": \"Ingestion Failed\",\n",
    "                        \"error\": str(e),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>file_id</th>\n",
       "      <th>status</th>\n",
       "      <th>error_type</th>\n",
       "      <th>error_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>files_to_upload/What is RAG_ - Retrieval-Augme...</td>\n",
       "      <td>3a52f1e5-9780-42b2-9c8e-3ec8fc72a40b</td>\n",
       "      <td>Success</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>files_to_upload/RAG.pdf</td>\n",
       "      <td>6b1721d1-b99b-4749-bb87-fcf9564aa448</td>\n",
       "      <td>Success</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>files_to_upload/paper.pdf</td>\n",
       "      <td>234826d1-4f60-4e69-a8fa-7eabd7d48b75</td>\n",
       "      <td>Success</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>files_to_upload/Azure Cognitive Search_ Outper...</td>\n",
       "      <td>748aa0b2-c7be-402f-bcb6-25aeb4ab30da</td>\n",
       "      <td>Success</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_path  \\\n",
       "0  files_to_upload/What is RAG_ - Retrieval-Augme...   \n",
       "1                            files_to_upload/RAG.pdf   \n",
       "2                          files_to_upload/paper.pdf   \n",
       "3  files_to_upload/Azure Cognitive Search_ Outper...   \n",
       "\n",
       "                                file_id   status error_type error_message  \n",
       "0  3a52f1e5-9780-42b2-9c8e-3ec8fc72a40b  Success       None          None  \n",
       "1  6b1721d1-b99b-4749-bb87-fcf9564aa448  Success       None          None  \n",
       "2  234826d1-4f60-4e69-a8fa-7eabd7d48b75  Success       None          None  \n",
       "3  748aa0b2-c7be-402f-bcb6-25aeb4ab30da  Success       None          None  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ingesting the files\n",
    "directory_path = \"files_to_upload\"\n",
    "df_results = ingest_all_documents(directory_path, ingestion_context, stage_id)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Monitor the source document processing status\n",
    "\n",
    "After uploading source documents, you need to verify their processing status. The code in this section does the following:\n",
    "\n",
    "1. Extracts IDs of successfully uploaded source documents\n",
    "2. Retrieves the transformation ID\n",
    "3. Checks the status of each source document's transformation\n",
    "4. Extracts dataset IDs from completed transformations\n",
    "\n",
    "The `check_files_status` function combines all this information into a comprehensive report that shows which files completed processing and which encountered errors. The dataset IDs are particularly important as they are used to access your processed documents in subsequent operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_successful_document_ids(df: pd.DataFrame) -> list:\n",
    "    \"\"\"Retrieve a list of successful file_ids from the DataFrame.\"\"\"\n",
    "    return df[df[\"status\"] == \"Success\"][\"file_id\"].tolist()\n",
    "\n",
    "@retry(\n",
    "    stop=stop_after_attempt(3),\n",
    "    wait=wait_exponential(multiplier=1, min=2, max=10),\n",
    "    retry=retry_if_exception_type(requests.RequestException),\n",
    ")\n",
    "def check_status_of_ingestion(transformation_id: str, file_id: str) -> dict:\n",
    "    \"\"\"Query the status of the ingestion for a given transformation and file_id.\"\"\"\n",
    "    dataplatform_base_url = DATA_PLATFORM_URL\n",
    "    url = f\"{dataplatform_base_url}/api/v1/transformations/{transformation_id}/runs?file_id={file_id}\"\n",
    "\n",
    "    token = TOKEN\n",
    "    response = requests.get(\n",
    "        url=url, headers={\"Authorization\": f\"Bearer {token}\"}, verify=False\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    page = response.json()\n",
    "\n",
    "    assert page[\"total\"] > 0\n",
    "    return page[\"runs\"][0]\n",
    "\n",
    "def get_transformation_id(name: str) -> str:\n",
    "    \"\"\"Get the transformation ID from the Data Platform.\"\"\"\n",
    "    dataplatform_base_url = DATA_PLATFORM_URL\n",
    "    url = f\"{dataplatform_base_url}/api/v1/transformations?name={name}\"\n",
    "\n",
    "    token = TOKEN\n",
    "    response = requests.get(\n",
    "        url=url, headers={\"Authorization\": f\"Bearer {token}\"}, verify=False\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    page = response.json()\n",
    "\n",
    "    assert page[\"total\"] > 0\n",
    "    return page[\"transformations\"][0][\"transformationId\"]\n",
    "\n",
    "def check_files_status(transformation_id: str, df: pd.DataFrame, max_workers: int = 3):\n",
    "    \"\"\"Check the status of ingested files and store the results in a DataFrame.\"\"\"\n",
    "\n",
    "    successful_file_ids = get_successful_document_ids(df)\n",
    "    status_results = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_file = {\n",
    "            executor.submit(\n",
    "                check_status_of_ingestion, transformation_id, file_id\n",
    "            ): file_id\n",
    "            for file_id in successful_file_ids\n",
    "        }\n",
    "\n",
    "        for future in concurrent.futures.as_completed(future_to_file):\n",
    "            file_id = future_to_file[future]\n",
    "            try:\n",
    "                run = future.result()\n",
    "                output = json.dumps(run.get(\"output\", {}), indent=4)\n",
    "                status_results.append(\n",
    "                    {\n",
    "                        \"file_id\": file_id,\n",
    "                        \"run_id\": run[\"runId\"],\n",
    "                        \"status\": run[\"status\"],\n",
    "                        \"output\": output,\n",
    "                        \"error\": run[\"errors\"],\n",
    "                    }\n",
    "                )\n",
    "            except Exception as e:\n",
    "                status_results.append(\n",
    "                    {\n",
    "                        \"file_id\": file_id,\n",
    "                        \"status\": run[\"status\"],\n",
    "                        \"output\": None,\n",
    "                        \"error\": str(e),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return df.merge(\n",
    "        pd.DataFrame(status_results),\n",
    "        on=\"file_id\",\n",
    "        how=\"left\",\n",
    "        suffixes=(\"_ingestion\", \"\"),\n",
    "    )\n",
    "\n",
    "def get_successful_dataset_ids(df: pd.DataFrame) -> list:\n",
    "    \"\"\"Retrieve a list of successful dataset_ids from the DataFrame.\"\"\"\n",
    "    dataset_ids_list = []\n",
    "    for i in range(len(df)):\n",
    "        dataset_ids_list.append(json.loads(df[\"output\"][i]).get(\"datasetId\"))\n",
    "    return dataset_ids_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>file_id</th>\n",
       "      <th>status_ingestion</th>\n",
       "      <th>error_type</th>\n",
       "      <th>error_message</th>\n",
       "      <th>run_id</th>\n",
       "      <th>status</th>\n",
       "      <th>output</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>files_to_upload/What is RAG_ - Retrieval-Augme...</td>\n",
       "      <td>3a52f1e5-9780-42b2-9c8e-3ec8fc72a40b</td>\n",
       "      <td>Success</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>08132b6d-6c47-4119-b1a1-65c1ff359d18</td>\n",
       "      <td>completed</td>\n",
       "      <td>{\\n    \"type\": \"DataPlatform:Repository:Datase...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>files_to_upload/RAG.pdf</td>\n",
       "      <td>6b1721d1-b99b-4749-bb87-fcf9564aa448</td>\n",
       "      <td>Success</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>310c3d5c-b3e6-4ed3-8052-8daa3f4d984d</td>\n",
       "      <td>completed</td>\n",
       "      <td>{\\n    \"type\": \"DataPlatform:Repository:Datase...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>files_to_upload/paper.pdf</td>\n",
       "      <td>234826d1-4f60-4e69-a8fa-7eabd7d48b75</td>\n",
       "      <td>Success</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>b9a818ec-02c2-4a83-8399-9bb474d083e6</td>\n",
       "      <td>completed</td>\n",
       "      <td>{\\n    \"type\": \"DataPlatform:Repository:Datase...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>files_to_upload/Azure Cognitive Search_ Outper...</td>\n",
       "      <td>748aa0b2-c7be-402f-bcb6-25aeb4ab30da</td>\n",
       "      <td>Success</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>80631c8f-52fe-4be1-afb6-59e43dd304af</td>\n",
       "      <td>completed</td>\n",
       "      <td>{\\n    \"type\": \"DataPlatform:Repository:Datase...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_path  \\\n",
       "0  files_to_upload/What is RAG_ - Retrieval-Augme...   \n",
       "1                            files_to_upload/RAG.pdf   \n",
       "2                          files_to_upload/paper.pdf   \n",
       "3  files_to_upload/Azure Cognitive Search_ Outper...   \n",
       "\n",
       "                                file_id status_ingestion error_type  \\\n",
       "0  3a52f1e5-9780-42b2-9c8e-3ec8fc72a40b          Success       None   \n",
       "1  6b1721d1-b99b-4749-bb87-fcf9564aa448          Success       None   \n",
       "2  234826d1-4f60-4e69-a8fa-7eabd7d48b75          Success       None   \n",
       "3  748aa0b2-c7be-402f-bcb6-25aeb4ab30da          Success       None   \n",
       "\n",
       "  error_message                                run_id     status  \\\n",
       "0          None  08132b6d-6c47-4119-b1a1-65c1ff359d18  completed   \n",
       "1          None  310c3d5c-b3e6-4ed3-8052-8daa3f4d984d  completed   \n",
       "2          None  b9a818ec-02c2-4a83-8399-9bb474d083e6  completed   \n",
       "3          None  80631c8f-52fe-4be1-afb6-59e43dd304af  completed   \n",
       "\n",
       "                                              output error  \n",
       "0  {\\n    \"type\": \"DataPlatform:Repository:Datase...  None  \n",
       "1  {\\n    \"type\": \"DataPlatform:Repository:Datase...  None  \n",
       "2  {\\n    \"type\": \"DataPlatform:Repository:Datase...  None  \n",
       "3  {\\n    \"type\": \"DataPlatform:Repository:Datase...  None  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformation_id = get_transformation_id(TRANSFORMATION_NAME)\n",
    "status_df = check_files_status(transformation_id, df_results)\n",
    "status_df.to_csv(\"ingestion_status.csv\", index=False)\n",
    "successful_dataset_ids = get_successful_dataset_ids( status_df[status_df[\"status\"] == \"completed\"])\n",
    "status_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Interact with processed documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With source documents ingested and processed, you can now interact with your data in various ways:\n",
    "\n",
    "1. **Search operation**: The `search_text` function demonstrates semantic search against your indexed processed documents, finding content based on meaning rather than exact keyword matches\n",
    "\n",
    "2. **Document and metadata retrieval**: The `get_document_from_document_index` function retrieves a complete processed document and its metadata using the dataset ID\n",
    "\n",
    "3. **Text display**: The `display_processed_document_text` function shows how to access the actual content extracted from your source documents, helping you verify the quality of text extraction\n",
    "\n",
    "These operations showcase the fundamental ways to interact with your processed documents in PhariaAI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1. Searching document content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After successfully ingesting documents, one of the most valuable operations is searching through your content. This section demonstrates how to perform semantic searches against your indexed documents.\n",
    "\n",
    "The `search_text` function sends a query to the PhariaDocument Index API, which uses vector embeddings to find semantically relevant content. Unlike traditional keyword search, this approach can identify conceptually related information even when exact terms do not match.\n",
    "\n",
    "In this example, we search for content related to \"what is attention?\" and retrieve matches ranked by relevance. The results include document chunks that semantically align with the query, along with confidence scores indicating match quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def search_text(namespace: str, collection: str, text: str, index: str) -> dict:\n",
    "    di_base_url = DOCUMENT_INDEX_API_URL\n",
    "    url = f\"{di_base_url}/collections/{namespace}/{collection}/indexes/{index}/search\"\n",
    "\n",
    "    token = TOKEN\n",
    "    payload = {\"query\": [{\"modality\": \"text\", \"text\": text}]}\n",
    "    response = requests.post(\n",
    "        url=url,\n",
    "        json=payload,\n",
    "        headers={\"Authorization\": f\"Bearer {token}\"},\n",
    "        verify=False,\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"document_path\": {\n",
      "            \"namespace\": \"Assistant\",\n",
      "            \"collection\": \"pharia-tutorial-rag\",\n",
      "            \"name\": \"1a592e70-271d-4982-9b7e-798af170d7dd\"\n",
      "        },\n",
      "        \"section\": [\n",
      "            {\n",
      "                \"modality\": \"text\",\n",
      "                \"text\": \"Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2 , 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network.\\n\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n\\n## 2 Background\"\n",
      "            }\n",
      "        ],\n",
      "        \"start\": {\n",
      "            \"modality\": \"text\",\n",
      "            \"item\": 0,\n",
      "            \"position\": 3957\n",
      "        },\n",
      "        \"end\": {\n",
      "            \"modality\": \"text\",\n",
      "            \"item\": 0,\n",
      "            \"position\": 4705\n",
      "        },\n",
      "        \"score\": 0.4305724\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "text_to_search = \"what is attention?\"\n",
    "search_result = search_text(\n",
    "    NAMESPACE, COLLECTION, text_to_search, index=INDEX\n",
    ")\n",
    "print(json.dumps(search_result, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2. Retrieving complete documents and metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While searching helps find specific information, sometimes you need to retrieve a complete document along with its metadata. This operation is useful when you want to examine a document's full context or access its associated properties.\n",
    "\n",
    "The `get_document_from_document_index` function retrieves a document using its dataset ID (obtained during the ingestion process). The response includes both the document content and additional metadata such as creation time, source information, and any custom properties attached during processing.\n",
    "\n",
    "This example retrieves the fourth document from our previously ingested set, demonstrating how to access specific documents directly when you know their IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def get_document_from_document_index(namespace, collection, dataset_id) -> dict:\n",
    "    di_base_url = DOCUMENT_INDEX_API_URL\n",
    "    url = f\"{di_base_url}/collections/{namespace}/{collection}/docs/{dataset_id}\"\n",
    "\n",
    "    token = TOKEN\n",
    "    response = requests.get(\n",
    "        url=url, headers={\"Authorization\": f\"Bearer {token}\"}, verify=False\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"schema_version\": \"V1\",\n",
      "    \"contents\": [\n",
      "        {\n",
      "            \"modality\": \"text\",\n",
      "            \"text\": \"## Get Started for Free\\n\\nContact Us\\n\\nWhat is Cloud Computing? / Cloud Computing Concepts Hub / Generative AI\\n\\n/ Machine Learning &amp; AI\\n\\n## What Is RAG?\\n\\n## Create an AWS Account\\n\\n\\n\\n\\n## Explore Free Machine Learning Offers\\n\\nBuild , deploy, y, and run machine learning applications in the cloud for free\\n\\n\\n\\n\\n## Check out Machine Learning Services\\n\\nInnovate faster with the most comprehensive set of AI and ML services\\n\\n\\n\\n\\n## Browse Machine Learning Trainings\\n\\nGet started on machine learning training with content built by AWS experts\\n\\n\\n\\n\\n\\n Read Machine Learning Blogs\\n\\nRead about the latest AWS Machine Learning product news and best practices\\n\\nGet Started for Free\\n\\nContact Us\\n\\n## What is Retrieval-Augmented Generation?\\n\\nRetrieval -Augmented Generation (RAG) is the process of optimizing the output of a large language model , so it references an authoritative knowledge base outside of its training data sources before generating a response . Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions , translating languages , and completing sentences . RAG extends the already powerful capabilities of LLMs to specific domains or an organization ' s internal knowledge base , all without the need to retrain the model . It is a cost -effective approach to improving LLM output so it remains relevant , accurate , and useful in various contexts .\\n\\n## Why is Retrieval-Augmented Generation important?\\n\\nLLMs are a key artificial intelligence (AI) technology powering intelligent chatbots and other natural language processing (NLP) applications . The goal is to create bots that can answer user questions in various contexts by cross-referencing authoritative knowledge sources . Unfortunately, y, the nature of LLM technology introduces unpredictability in LLM responses . Additionally, y, LLM training data is static and introduces a cut-off date on the knowledge it has .\\n\\nKnown challenges of LLMs include:\\n\\n- Presenting false information when it does not have the answer .\\n- Presenting out-of-date or generic information when the user expects a specific , current response .\\n- Creating a response from non-authoritative sources .\\n- Creating inaccurate responses due to terminology confusion , wherein different training sources use the same terminology to talk about different things .\\n\\nYou can think of the Large Language Model as an over-enthusiastic new employee who refuses to stay informed with current events but will always answer every question with absolute confidence . Unfortunately, y, such an attitude can negatively impact user trust and is not something you want your chatbots to emulate!\\n\\nRAG is one approach to solving some of these challenges . It redirects the LLM to retrieve relevant information from authoritative , pre -determined knowledge sources . Organizations have greater control over the generated text output , and users gain insights into how the LLM generates the response .\\n\\n\\n What are the benefits of Retrieval-Augmented\\r\\nGt Sttd fFCtt U\\n\\n## Get Started for Free\\n\\ng\\r\\nContact Us\\n\\nGeneration?\\n\\nRAG technology brings several benefits to an organization ' s generative AI efforts .\\n\\n## Cost -effective implementation\\n\\nChatbot development typically begins using a foundation model . Foundation models (FMs) are API -accessible LLMs trained on a broad spectrum of generalized and unlabeled data . The computational and financial costs of retraining FMs for organization or domain-specific information are high . RAG is a more cost -effective approach to introducing new data to the LLM . It makes generative artificial intelligence (generative AI) technology more broadly accessible and usable .\\n\\n## Current information\\n\\nEven if the original training data sources for an LLM are suitable for your needs , it is challenging to maintain relevancy . RAG allows developers to provide the latest research , statistics , or news to the generative models . They can use RAG to connect the LLM directly to live social media feeds , news sites , or other frequently-updated information sources . The LLM can then provide the latest information to the users .\\n\\n## Enhanced user trust\\n\\nRAG allows the LLM to present accurate information with source attribution . The output can include citations or references to sources . Users can also look up source documents themselves if they require further clarification or more detail . This can increase trust and confidence in your generative AI solution .\\n\\n## More developer control\\n\\nWith RAG , developers can test and improve their chat applications more efficiently . They can control and change the LLM ' s information sources to adapt to changing requirements or crossfunctional usage . Developers can also restrict sensitive information retrieval to different authorization levels and ensure the LLM generates appropriate responses . In addition , they can also troubleshoot and make fixes if the LLM references incorrect information sources for specific questions . Organizations can implement generative AI technology more confidently for a broader range of applications .\\n\\n## How does Retrieval-Augmented Generation work?\\n\\nWithout RAG , the LLM takes the user input and creates a response based on information it was trained on\\u2014or what it already knows . With RAG , an information retrieval component is\\n\\n\\n introduced that utilizes the user input to first pull information from a new data source query and the relevant information are both given to the LLM\\n\\n.\\n\\nThe user\\r\\n\\uf002\\n\\n.\\n\\nThe LLM uses the new\\n\\n\\n\\n\\nknowledge and its training data to create better responses\\n\\n.\\n\\nThe following sections provide an\\n\\n## overview of the processGet Started fo .o of the process.Get Started for Free\\n\\nContact Us\\n\\n## Create external data\\n\\nThe new data outside of the LLM ' s original training data set is called external data. a. It can come from multiple data sources , such as a APIs , databases , or document repositories . The data may exist in various formats like files , database records , or long-form text . Another AI technique , called embedding language models, s, converts data into numerical representations and stores it in a vector database . This process creates a knowledge library that the generative AI models can understand .\\n\\n## Retrieve relevant information\\n\\nThe next step is to perform a relevancy search . The user query is converted to a vector representation and matched with the vector databases . For example , consider a smart chatbot that can answer human resource questions for an organization . If an employee searches , \\\"How much annual leave do I have? \\\" the system will retrieve annual leave policy documents alongside the individual employee ' s past leave record . These specific documents will be returned because they are highly-relevant to what the employee has input . The relevancy was calculated and established using mathematical vector calculations and representations .\\n\\n## Augment the LLM prompt\\n\\nNext , the RAG model augments the user input (or prompts) by adding the relevant retrieved data in context . This step uses prompt engineering techniques to communicate effectively with the LLM . The augmented prompt allows the large language models to generate an accurate answer to user queries .\\n\\n## Update external data\\n\\nThe next question may be\\u2014what if the external data becomes stale? To maintain current information for retrieval , asynchronously update the documents and update embedding representation of the documents . You can do this through automated real-time processes or periodic batch processing . This is a common challenge in data analytics\\u2014different data-science approaches to change management can be used .\\n\\nThe following diagram shows the conceptual flow of using RAG with LLMs . \\n\\n\\n\\n\\n\\n## What is the difference between RetrievalAugmented Generation and semantic search?\\n\\nSemantic search enhances RAG results for organizations wanting to add vast external knowledge sources to their LLM applications . Modern enterprises store vast amounts of information like manuals , FAQs , research reports , customer service guides , and human resource document repositories across various systems . Context retrieval is challenging at scale and consequently lowers generative output quality .\\n\\nSemantic search technologies can scan large databases of disparate information and retrieve data more accurately . For example , they can answer questions such as , \\\"How much was spent on machinery repairs last year? \\\" by mapping the question to the relevant documents and returning specific text instead of search results . Developers can then use that answer to provide more context to the LLM .\\n\\nConventional or keyword search solutions in RAG produce limited results for knowledgeintensive tasks . Developers must also deal with word embeddings , document chunking , and other complexities as they manually prepare their data . In contrast , semantic search technologies do all the work of knowledge base preparation so developers don ' t have to . They also generate semantically relevant passages and token words ordered by relevance to maximize the quality of the RAG payload . \\n\\n\\nHow can AWS support your\\u00a0Retrieval-Augmented\\r\\n\\uf002\\n\\nGeneration requirements?\\r\\nGet Started for Free\\n\\n## ration requ\\r\\nGet Started for Free\\n\\n## Contact Us\\n\\nAmazon Bedrock is a fully-managed service that offers a choice of high-performing foundation models\\u2014along with a broad set of capabilities\\u2014to build generative AI applications while simplifying development and maintaining privacy and security . With knowledge bases for Amazon Bedrock , you can connect FMs to your data sources for RAG in just a few clicks . Vector conversions , retrievals , and improved output generation are all handled automatically .\\n\\nFor organizations managing their own RAG , Amazon Kendra is a highly-accurate enterprise search service powered by machine learning . It provides an optimized Kendra Retrieve API that you can use with Amazon Kendra ' s high-accuracy semantic ranker as an enterprise retriever for your RAG workflows . For example , with the Retrieve API , you can:\\n\\n- Retrieve up to 100 semantically-relevant passages of up to 200 token words each , ordered by relevance .\\n- Use pre-built connectors to popular data technologies like Amazon Simple Storage Service , SharePoint , Confluence , and other websites .\\n- Support a wide range of document formats such as HTML , Word , PowerPoint , PDF, F, Excel , and text files .\\n- Filter responses based on those documents that the end-user permissions allow .\\n\\nAmazon also offers options for organizations who want to build more custom generative AI solutions . Amazon SageMaker JumpStart is a ML hub with FMs , built -in algorithms , and prebuilt ML solutions that you can deploy with just a few clicks . You can speed up RAG implementation by referring to existing SageMaker notebooks and code examples .\\n\\nGet started with Retrieval -Augmented Generation on AWS by creating a free account today\\n\\n## Next Steps on AWS\\n\\n\\n\\n\\n## Check out additional product-related resources\\n\\nInnovate faster with the most comprehensive set of Machine Learning services \\uf101 ## Get Started for Free\\n\\n## Sign up for a free account\\n\\nInstant get access to the AWS Free Tier .\\n\\n## Sign up \\uf101\\n\\n## Sign In to the Console\\n\\n## Learn About AWS\\n\\nWhat Is AWS?\\n\\nWhat Is Cloud Computing?\\n\\nAWS Inclusion , Diversity &amp; Equity\\n\\nWhat Is DevOps?\\n\\nWhat Is a Container?\\n\\nWhat Is a Data Lake?\\n\\nWhat is Generative AI?\\n\\nAWS Cloud Security\\n\\nWhat ' s New\\n\\nBlogs\\n\\nPress Releases\\n\\n## Help\\n\\nContact Us\\n\\nGet Expert Help\\n\\nFile a Support Ticket\\n\\nAWS re:Post\\n\\n\\n\\n\\n## Start building in the console\\n\\nGet started building in the AWS management console .\\n\\n## Sign in \\uf101\\n\\n## Resources for AWS\\n\\n## Developers on AWS\\n\\nGetting Started\\n\\nTraining and Certification\\n\\nAWS Solutions Library\\n\\nArchitecture Center\\n\\nProduct and Technical FAQs\\n\\nAnalyst Reports\\n\\nAWS Partners\\n\\nDeveloper Center\\n\\nSDKs &amp; Tools\\n\\n. NET on AWS\\n\\nPython on AWS\\n\\nJava on AWS\\n\\nPHP on AWS\\n\\nJavaScript on AWS\\n\\nContact Us\\n\\n\\n Knowledge Center\\n\\nAWS Support Overview\\n\\nLegal\\n\\nGet Started for Free\\n\\nAWS Careers\\n\\n\\n\\n\\n\\uf099 \\uf09a \\uf0e1 \\uf16d\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\uf1e8 \\uf167 \\uf2ce \\uf003\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAmazon is an Equal Opportunity Employer: Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age. e.\\n\\n## Language\\n\\n| \\u0639\\u0631\\u0628\\u064a \\u0628\\u064a\\n\\nBahasa Indonesia |\\n\\nDeutsch |\\n\\nEnglish |\\n\\nEspa\\u00f1ol |\\n\\nFran\\u00e7ais |\\n\\nItaliano |\\n\\nPortugu\\u00eas |\\n\\nTi\\u1ebf \\u0302\\u0301 ng Vi\\u04abt |\\n\\nT\\u00fcrk\\u00e7e |\\n\\n\\u03a1\\u0443\\u0441\\u0441\\u043a\\u0438\\u0439 |\\n\\n\\u0e44\\u0e17\\u0e22 |\\n\\n|\\n\\n|\\n\\n( ) |\\n\\n( )\\n\\n## Privacy\\n\\n|\\n\\nSite Terms\\n\\n|\\n\\nCookie Preferences\\n\\n|\\n\\n\\u00a9 2023 , Amazon Web Services , Inc . or its affiliates . All rights reserved .\\n\\n## Create an AWS Account\\n\\nContact Us\\n\\n\\n\"\n",
      "        }\n",
      "    ],\n",
      "    \"metadata\": {\n",
      "        \"fileId\": \"3a52f1e5-9780-42b2-9c8e-3ec8fc72a40b\",\n",
      "        \"fileName\": \"What is RAG_ - Retrieval-Augmented Generation Explained - AWS.pdf\",\n",
      "        \"fileSize\": 503489,\n",
      "        \"stageId\": \"6649f6f1-66c7-4688-b372-7beed1913ce4\",\n",
      "        \"totalPages\": 8,\n",
      "        \"totalTokenEstimationCount\": 3161,\n",
      "        \"wordcount\": 2139\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "all_documents = []\n",
    "\n",
    "for id in successful_dataset_ids:\n",
    "    document_from_di = get_document_from_document_index(\n",
    "        NAMESPACE, COLLECTION, id\n",
    "    )\n",
    "    all_documents.append(document_from_di)\n",
    "\n",
    "print(json.dumps(all_documents[0], indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3. Viewing extracted document text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To inspect the actual content extracted from your documents, you can retrieve and display the text chunks stored in the repository. This is useful for verifying extraction quality and understanding how your documents were segmented.\n",
    "\n",
    "The `display_text_extracted` function connects to the PhariaData repository and retrieves text chunks from a specific document. It displays each chunk sequentially, showing how the document was divided during processing.\n",
    "\n",
    "This operation helps you validate that your documents were properly processed and that the extracted text accurately represents the original content. It can be particularly valuable when troubleshooting search issues or refining your ingestion parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '## Get Started for Free\\n\\nContact Us\\n\\nWhat is Cloud Computing? / Cloud Computing Concepts Hub / Generative AI\\n\\n/ Machine Learning &amp; AI\\n\\n## What Is RAG?\\n\\n## Create an AWS Account\\n\\n\\n\\n\\n## Explore Free Machine Learning Offers\\n\\nBuild , deploy, y, and run machine learning applications in the cloud for free\\n\\n\\n\\n\\n## Check out Machine Learning Services\\n\\nInnovate faster with the most comprehensive set of AI and ML services\\n\\n\\n\\n\\n## Browse Machine Learning Trainings\\n\\nGet started on machine learning training with content built by AWS experts\\n\\n\\n\\n\\n\\n', 'wordcount': 93, 'tokenEstimationCount': 135}\n",
      "{'text': \"Read Machine Learning Blogs\\n\\nRead about the latest AWS Machine Learning product news and best practices\\n\\nGet Started for Free\\n\\nContact Us\\n\\n## What is Retrieval-Augmented Generation?\\n\\nRetrieval -Augmented Generation (RAG) is the process of optimizing the output of a large language model , so it references an authoritative knowledge base outside of its training data sources before generating a response . Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions , translating languages , and completing sentences . RAG extends the already powerful capabilities of LLMs to specific domains or an organization ' s internal knowledge base , all without the need to retrain the model . It is a cost -effective approach to improving LLM output so it remains relevant , accurate , and useful in various contexts .\\n\\n## Why is Retrieval-Augmented Generation important?\\n\\nLLMs are a key artificial intelligence (AI) technology powering intelligent chatbots and other natural language processing (NLP) applications . The goal is to create bots that can answer user questions in various contexts by cross-referencing authoritative knowledge sources . Unfortunately, y, the nature of LLM technology introduces unpredictability in LLM responses . Additionally, y, LLM training data is static and introduces a cut-off date on the knowledge it has .\\n\\nKnown challenges of LLMs include:\\n\\n- Presenting false information when it does not have the answer .\\n- Presenting out-of-date or generic information when the user expects a specific , current response .\\n- Creating a response from non-authoritative sources .\\n- Creating inaccurate responses due to terminology confusion , wherein different training sources use the same terminology to talk about different things .\\n\\nYou can think of the Large Language Model as an over-enthusiastic new employee who refuses to stay informed with current events but will always answer every question with absolute confidence . Unfortunately, y, such an attitude can negatively impact user trust and is not something you want your chatbots to emulate!\\n\\nRAG is one approach to solving some of these challenges . It redirects the LLM to retrieve relevant information from authoritative , pre -determined knowledge sources . Organizations have greater control over the generated text output , and users gain insights into how the LLM generates the response .\\n\\n\\n\", 'wordcount': 409, 'tokenEstimationCount': 619}\n",
      "{'text': \"What are the benefits of Retrieval-Augmented\\r\\nGt Sttd fFCtt U\\n\\n## Get Started for Free\\n\\ng\\r\\nContact Us\\n\\nGeneration?\\n\\nRAG technology brings several benefits to an organization ' s generative AI efforts .\\n\\n## Cost -effective implementation\\n\\nChatbot development typically begins using a foundation model . Foundation models (FMs) are API -accessible LLMs trained on a broad spectrum of generalized and unlabeled data . The computational and financial costs of retraining FMs for organization or domain-specific information are high . RAG is a more cost -effective approach to introducing new data to the LLM . It makes generative artificial intelligence (generative AI) technology more broadly accessible and usable .\\n\\n## Current information\\n\\nEven if the original training data sources for an LLM are suitable for your needs , it is challenging to maintain relevancy . RAG allows developers to provide the latest research , statistics , or news to the generative models . They can use RAG to connect the LLM directly to live social media feeds , news sites , or other frequently-updated information sources . The LLM can then provide the latest information to the users .\\n\\n## Enhanced user trust\\n\\nRAG allows the LLM to present accurate information with source attribution . The output can include citations or references to sources . Users can also look up source documents themselves if they require further clarification or more detail . This can increase trust and confidence in your generative AI solution .\\n\\n## More developer control\\n\\nWith RAG , developers can test and improve their chat applications more efficiently . They can control and change the LLM ' s information sources to adapt to changing requirements or crossfunctional usage . Developers can also restrict sensitive information retrieval to different authorization levels and ensure the LLM generates appropriate responses . In addition , they can also troubleshoot and make fixes if the LLM references incorrect information sources for specific questions . Organizations can implement generative AI technology more confidently for a broader range of applications .\\n\\n## How does Retrieval-Augmented Generation work?\\n\\nWithout RAG , the LLM takes the user input and creates a response based on information it was trained onor what it already knows . With RAG , an information retrieval component is\\n\\n\\n\", 'wordcount': 385, 'tokenEstimationCount': 591}\n",
      "{'text': 'introduced that utilizes the user input to first pull information from a new data source query and the relevant information are both given to the LLM\\n\\n.\\n\\nThe user\\r\\n\\uf002\\n\\n.\\n\\nThe LLM uses the new\\n\\n\\n\\n\\nknowledge and its training data to create better responses\\n\\n.\\n\\nThe following sections provide an\\n\\n## overview of the processGet Started fo .o of the process.Get Started for Free\\n\\nContact Us\\n\\n## Create external data\\n\\nThe new data outside of the LLM \\' s original training data set is called external data. a. It can come from multiple data sources , such as a APIs , databases , or document repositories . The data may exist in various formats like files , database records , or long-form text . Another AI technique , called embedding language models, s, converts data into numerical representations and stores it in a vector database . This process creates a knowledge library that the generative AI models can understand .\\n\\n## Retrieve relevant information\\n\\nThe next step is to perform a relevancy search . The user query is converted to a vector representation and matched with the vector databases . For example , consider a smart chatbot that can answer human resource questions for an organization . If an employee searches , \"How much annual leave do I have? \" the system will retrieve annual leave policy documents alongside the individual employee \\' s past leave record . These specific documents will be returned because they are highly-relevant to what the employee has input . The relevancy was calculated and established using mathematical vector calculations and representations .\\n\\n## Augment the LLM prompt\\n\\nNext , the RAG model augments the user input (or prompts) by adding the relevant retrieved data in context . This step uses prompt engineering techniques to communicate effectively with the LLM . The augmented prompt allows the large language models to generate an accurate answer to user queries .\\n\\n## Update external data\\n\\nThe next question may bewhat if the external data becomes stale? To maintain current information for retrieval , asynchronously update the documents and update embedding representation of the documents . You can do this through automated real-time processes or periodic batch processing . This is a common challenge in data analyticsdifferent data-science approaches to change management can be used .\\n\\nThe following diagram shows the conceptual flow of using RAG with LLMs .', 'wordcount': 413, 'tokenEstimationCount': 604}\n",
      "{'text': '\\n\\n\\n\\n\\n\\n## What is the difference between RetrievalAugmented Generation and semantic search?\\n\\nSemantic search enhances RAG results for organizations wanting to add vast external knowledge sources to their LLM applications . Modern enterprises store vast amounts of information like manuals , FAQs , research reports , customer service guides , and human resource document repositories across various systems . Context retrieval is challenging at scale and consequently lowers generative output quality .\\n\\nSemantic search technologies can scan large databases of disparate information and retrieve data more accurately . For example , they can answer questions such as , \"How much was spent on machinery repairs last year? \" by mapping the question to the relevant documents and returning specific text instead of search results . Developers can then use that answer to provide more context to the LLM .\\n\\nConventional or keyword search solutions in RAG produce limited results for knowledgeintensive tasks . Developers must also deal with word embeddings , document chunking , and other complexities as they manually prepare their data . In contrast , semantic search technologies do all the work of knowledge base preparation so developers don \\' t have to . They also generate semantically relevant passages and token words ordered by relevance to maximize the quality of the RAG payload .', 'wordcount': 218, 'tokenEstimationCount': 346}\n",
      "{'text': \"\\n\\n\\nHow can AWS support your\\xa0Retrieval-Augmented\\r\\n\\uf002\\n\\nGeneration requirements?\\r\\nGet Started for Free\\n\\n## ration requ\\r\\nGet Started for Free\\n\\n## Contact Us\\n\\nAmazon Bedrock is a fully-managed service that offers a choice of high-performing foundation modelsalong with a broad set of capabilitiesto build generative AI applications while simplifying development and maintaining privacy and security . With knowledge bases for Amazon Bedrock , you can connect FMs to your data sources for RAG in just a few clicks . Vector conversions , retrievals , and improved output generation are all handled automatically .\\n\\nFor organizations managing their own RAG , Amazon Kendra is a highly-accurate enterprise search service powered by machine learning . It provides an optimized Kendra Retrieve API that you can use with Amazon Kendra ' s high-accuracy semantic ranker as an enterprise retriever for your RAG workflows . For example , with the Retrieve API , you can:\\n\\n- Retrieve up to 100 semantically-relevant passages of up to 200 token words each , ordered by relevance .\\n- Use pre-built connectors to popular data technologies like Amazon Simple Storage Service , SharePoint , Confluence , and other websites .\\n- Support a wide range of document formats such as HTML , Word , PowerPoint , PDF, F, Excel , and text files .\\n- Filter responses based on those documents that the end-user permissions allow .\\n\\nAmazon also offers options for organizations who want to build more custom generative AI solutions . Amazon SageMaker JumpStart is a ML hub with FMs , built -in algorithms , and prebuilt ML solutions that you can deploy with just a few clicks . You can speed up RAG implementation by referring to existing SageMaker notebooks and code examples .\\n\\nGet started with Retrieval -Augmented Generation on AWS by creating a free account today\\n\\n## Next Steps on AWS\\n\\n\\n\\n\\n## Check out additional product-related resources\\n\\nInnovate faster with the most comprehensive set of Machine Learning services \\uf101\", 'wordcount': 341, 'tokenEstimationCount': 497}\n",
      "{'text': \"## Get Started for Free\\n\\n## Sign up for a free account\\n\\nInstant get access to the AWS Free Tier .\\n\\n## Sign up \\uf101\\n\\n## Sign In to the Console\\n\\n## Learn About AWS\\n\\nWhat Is AWS?\\n\\nWhat Is Cloud Computing?\\n\\nAWS Inclusion , Diversity &amp; Equity\\n\\nWhat Is DevOps?\\n\\nWhat Is a Container?\\n\\nWhat Is a Data Lake?\\n\\nWhat is Generative AI?\\n\\nAWS Cloud Security\\n\\nWhat ' s New\\n\\nBlogs\\n\\nPress Releases\\n\\n## Help\\n\\nContact Us\\n\\nGet Expert Help\\n\\nFile a Support Ticket\\n\\nAWS re:Post\\n\\n\\n\\n\\n## Start building in the console\\n\\nGet started building in the AWS management console .\\n\\n## Sign in \\uf101\\n\\n## Resources for AWS\\n\\n## Developers on AWS\\n\\nGetting Started\\n\\nTraining and Certification\\n\\nAWS Solutions Library\\n\\nArchitecture Center\\n\\nProduct and Technical FAQs\\n\\nAnalyst Reports\\n\\nAWS Partners\\n\\nDeveloper Center\\n\\nSDKs &amp; Tools\\n\\n. NET on AWS\\n\\nPython on AWS\\n\\nJava on AWS\\n\\nPHP on AWS\\n\\nJavaScript on AWS\\n\\nContact Us\\n\\n\\n\", 'wordcount': 163, 'tokenEstimationCount': 218}\n",
      "{'text': 'Knowledge Center\\n\\nAWS Support Overview\\n\\nLegal\\n\\nGet Started for Free\\n\\nAWS Careers\\n\\n\\n\\n\\n\\uf099 \\uf09a \\uf0e1 \\uf16d\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\uf1e8 \\uf167 \\uf2ce \\uf003\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAmazon is an Equal Opportunity Employer: Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age. e.\\n\\n## Language\\n\\n|  \\n\\nBahasa Indonesia |\\n\\nDeutsch |\\n\\nEnglish |\\n\\nEspaol |\\n\\nFranais |\\n\\nItaliano |\\n\\nPortugus |\\n\\nTi  ng Vit |\\n\\nTrke |\\n\\n |\\n\\n |\\n\\n|\\n\\n|\\n\\n( ) |\\n\\n( )\\n\\n## Privacy\\n\\n|\\n\\nSite Terms\\n\\n|\\n\\nCookie Preferences\\n\\n|\\n\\n 2023 , Amazon Web Services , Inc . or its affiliates . All rights reserved .\\n\\n## Create an AWS Account\\n\\nContact Us\\n\\n\\n', 'wordcount': 117, 'tokenEstimationCount': 151}\n"
     ]
    }
   ],
   "source": [
    "# Helper function\n",
    "def display_processed_document_text(repository_id: str, dataset_id: str) -> None:\n",
    "    dataplatform_base_url = DATA_PLATFORM_URL\n",
    "    url = f\"{dataplatform_base_url}/api/v1/repositories/{repository_id}/datasets/{dataset_id}/datapoints\"\n",
    "\n",
    "    token = TOKEN\n",
    "    response = requests.get(\n",
    "        url=url, headers={\"Authorization\": f\"Bearer {token}\"}, verify=False, stream=True\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    for line in response.iter_lines():\n",
    "        datapoint = json.loads(line.decode())\n",
    "        print(datapoint)\n",
    "\n",
    "\n",
    "display_processed_document_text(repository_id, successful_dataset_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this section, you successfully set up the complete document ingestion pipeline:\n",
    "\n",
    " **Configured the environment** with connections to both the PhariaData and PhariaDocument Index APIs\n",
    "\n",
    " **Built the foundation infrastructure**:\n",
    "   - Created a repository for storing processed documents\n",
    "   - Set up a stage for temporary source document storage\n",
    "   - Configured an index for enabling semantic search\n",
    "   - Established triggers for automating document processing\n",
    "\n",
    " **Implemented document operations** with:\n",
    "   - Concurrent source document uploads with error handling\n",
    "   - Status monitoring for transformation processes\n",
    "   - Multiple ways to interact with processed documents\n",
    "\n",
    "Your source document collection is now properly ingested, processed, and ready for semantic search operations. This data foundation will serve as the basis for retrieval-augmented generation in the subsequent sections of this tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
