{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Setup - Setting up the document ingestion pipeline\n",
    "\n",
    "This section shows you how to establish a complete document ingestion pipeline in PhariaAI. The ingestion pipeline is a crucial foundation for RAG applications, as it transforms source documents into searchable, AI-ready processed documents.\n",
    "\n",
    "## Pipeline components\n",
    "\n",
    "The pipeline consists of several interconnected components:\n",
    "\n",
    "- **Repository**: Stores the source documents & processed documents\n",
    "- **Collection**: Groups processed documents in a searchable container with unified access patterns and shared indexes\n",
    "- **Stage**: Provides temporary storage for source documents\n",
    "- **Transformation**: Converts raw files into structured, searchable content\n",
    "- **Index**: Enables efficient semantic search across your documents\n",
    "- **Trigger**: Automates the processing workflow when documents are uploaded\n",
    "\n",
    "The document ingestion workflow we will be building transforms source documents into searchable processed documents through several steps: uploading to the stage, applying transformations, storing in the repository, and indexing for search.\n",
    "\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "In this section, you'll learn how to:\n",
    "\n",
    "1. Configure your environment and connection parameters\n",
    "2. Create an ingestion pipeline in the Data Platform\n",
    "3. Upload documents and monitor their processing\n",
    "4. Interact with your processed content through search and retrieval\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting, ensure you have the following:\n",
    "\n",
    "- **API Token**: A valid Aleph Alpha API token with appropriate permissions\n",
    "- **API URL**: Access to running instances of pharia-data-api and document-index-api\n",
    "- **Permissions**: StudioUser permission as described in the User Setup section\n",
    "\n",
    "<br>\n",
    "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation Options\n",
    "\n",
    "This tutorial uses various Python packages for API interaction and data processing. You have two options for setting up your environment:\n",
    "\n",
    "### Using Poetry (Recommended)\n",
    "\n",
    "If you're working with the complete project repository that includes the `pyproject.toml` file:\n",
    "\n",
    "1. Install [poetry](https://python-poetry.org/docs/#installing-with-pipx) using `pipx` following the official instructions\n",
    "2. Run `poetry install` in the project directory to automatically set up all dependencies\n",
    "\n",
    "### Custom Installation with Poetry\n",
    "\n",
    "If you prefer using your own virtual environment manager instead of poetry's default:\n",
    "\n",
    "```\n",
    "      poetry config virtualenvs.prefer-active-python true\n",
    "```\n",
    "\n",
    "You can append `--local` or `--global` to this command to apply the setting locally or globally.\n",
    "\n",
    "#### Manual Installation\n",
    "\n",
    "If you're just using this notebook without the full project structure, you can install the required packages manually:\n",
    "\n",
    "- python = \"~=3.11\"\n",
    "- requests = \"^2.32.3\"\n",
    "- aiohttp = \"^3.10.5\"\n",
    "- urllib3 = \"^2.2.2\"\n",
    "- pandas = \"2.1.4\"\n",
    "- tenacity = \"^8.2.2\"\n",
    "- python-dotenv = \"^1.0.0\"\n",
    "- ipykernel = \"^6.29.5\"\n",
    "\n",
    "Will be needed later, so lets already install it:\n",
    "- pharia-skill = \"^0.14.0\"\n",
    "\n",
    "\n",
    "TODO: Simplify the whole process here and give only one option\n",
    "\n",
    "<br>\n",
    "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "This section guides you through setting up your document ingestion pipeline. You'll first import necessary libraries, configure your environment, and then build the essential components for document processing. The workflow follows a systematic approach of creating a repository, setting up a document staging area, configuring an index, and establishing triggers for automated document transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import dependencies & configure the Environment\n",
    "\n",
    "Let's begin by importing the necessary dependencies and setting up our environment. We'll use standard Python libraries like `requests` for API communication, `pandas` for data handling, and specialized libraries like `tenacity` for robust error handling with retry mechanisms.\n",
    "\n",
    "The environment configuration establishes connections to Pharia's two key services:\n",
    "- The Data Platform API for managing document transformations and storage\n",
    "- The Document Index API for creating searchable indexes\n",
    "\n",
    "We'll use several key libraries for our document processing workflow, The code below imports all of these libraries and disables warnings to keep our notebook output clean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import concurrent.futures\n",
    "from dotenv import load_dotenv\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_exponential,\n",
    "    retry_if_exception_type,\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll configure the essential parameters that provide authentication and identify your workspace:\n",
    "\n",
    "- **TOKEN**: Your Pharia API authentication token, loaded from your environment file\n",
    "- **NAMESPACE**: The organizational namespace where your collections are stored (\"Studio\")\n",
    "- **COLLECTION**: The name of the document collection for this tutorial (\"pharia-tutorial-rag\")\n",
    "\n",
    "> **Note:** The namespace identifier depends on your specific Pharia setup and permission level. The collection name can be freely chosen to help you organize and separate different RAG projects. Using descriptive collection names (like \"legal-contracts\" or \"product-documentation\") helps you manage multiple document sets within the same namespace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setups\n",
    "load_dotenv(\"rag-tutorial/.env\", override=True)\n",
    "\n",
    "TOKEN = os.getenv(\"PHARIA_AI_TOKEN\", \"QydIbwy17Agq7LBKE9XXQPx_OYoGmFAQuRCUilC2Ul8QVnWksHh897b3DNMYxbxhp0hotUw\") #<your-token>\n",
    "NAMESPACE = \"Studio\"\n",
    "COLLECTION = \"pharia-tutorial-rag\"\n",
    "\n",
    "os.environ[\"NAMESPACE\"] = NAMESPACE\n",
    "os.environ[\"COLLECTION\"] = COLLECTION\n",
    "os.environ[\"TOKEN\"] = TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define the API endpoints that connect to Pharia's core document services:\n",
    "\n",
    "- **DATA_PLATFORM_URL**: The endpoint for the Data Platform service that manages document storage and transformations\n",
    "- **DOCUMENT_INDEX_API_URL**: The endpoint for the Document Index service that enables vector search capabilities\n",
    "\n",
    "These endpoints are stored as environment variables, making them accessible to all the helper functions we'll create throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## URLS\n",
    "DATA_PLATFORM_URL = \"https://pharia-data-api.<your-deployment>.pharia.com\"\n",
    "DOCUMENT_INDEX_API_URL = \"https://document-index.<your-deployment>.pharia.com\"\n",
    "\n",
    "os.environ[\"DATA_PLATFORM_URL\"] = DATA_PLATFORM_URL\n",
    "os.environ[\"DOCUMENT_INDEX_API_URL\"] = DOCUMENT_INDEX_API_URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creating a Document Repository\n",
    "\n",
    "A repository in Pharia's Data Platform is a storage container that organizes processed documents. In this tutorial, we create a repository named \"DocumentSearch\"\n",
    "\n",
    "The `get_or_create_repository` function checks if a repository with the specified name already exists and creates one if needed. The function returns the repository ID, which will be referenced in later steps when configuring the ingestion pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Environment Variables\n",
    "\n",
    "REPOSITORY_NAME = \"RAG_Tutorial_Repository\"\n",
    "os.environ[\"REPOSITORY_NAME\"] = REPOSITORY_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper fucntion \n",
    "\n",
    "def get_or_create_repository(repository: dict) -> str:\n",
    "    \"\"\"Get or create a repository in the Data Platform.\"\"\"\n",
    "    dataplatform_base_url = os.getenv(\"DATA_PLATFORM_URL\")\n",
    "    name = repository[\"name\"]\n",
    "    url = f\"{dataplatform_base_url}/api/v1/repositories?name={name}\"\n",
    "\n",
    "    token = os.getenv(\"TOKEN\")\n",
    "    response = requests.get(\n",
    "        url=url, headers={\"Authorization\": f\"Bearer {token}\"}, verify=False\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    page = response.json()\n",
    "\n",
    "    if page[\"total\"] > 0:\n",
    "        return page[\"repositories\"][0][\"repositoryId\"]\n",
    "    else:\n",
    "        url = f\"{dataplatform_base_url}/api/v1/repositories\"\n",
    "        response = requests.post(\n",
    "            url=url,\n",
    "            json=repository,\n",
    "            headers={\"Authorization\": f\"Bearer {token}\"},\n",
    "            verify=False,\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        repo_created = response.json()\n",
    "        return repo_created[\"repositoryId\"]\n",
    "    \n",
    "def get_or_create_collection(namespace: str, collection: str) -> str:\n",
    "    \"\"\"Get or create a collection in the Document Index.\"\"\"\n",
    "    try:\n",
    "        di_base_url = os.getenv(\"DOCUMENT_INDEX_API_URL\")\n",
    "        url = f\"{di_base_url}/collections/{namespace}\"\n",
    "        token = os.getenv(\"TOKEN\")\n",
    "        response = requests.get(\n",
    "            url=url, headers={\"Authorization\": f\"Bearer {token}\"}, verify=False\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        collections_list = response.json()\n",
    "        \n",
    "        if len(collections_list) == 0 or collection not in collections_list:\n",
    "            url = f\"{di_base_url}/collections/{namespace}/{collection}\"\n",
    "            response = requests.put(\n",
    "                url=url, headers={\"Authorization\": f\"Bearer {token}\"}, verify=False\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return f\"{collection} created\"\n",
    "        else:\n",
    "            return f\"{collection} exists\"\n",
    "    except Exception as e:\n",
    "        return f\"{e}, Response: {response}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the Repository\n",
    "\n",
    "repository_payload = {\n",
    "    \"name\": os.getenv(\"REPOSITORY_NAME\"),\n",
    "    \"mediaType\": \"jsonlines\",\n",
    "    \"modality\": \"text\",\n",
    "    \"schema\": None,\n",
    "}\n",
    "\n",
    "repository_id = get_or_create_repository(repository_payload)\n",
    "print(f\"Repository ID: {repository_id}\")\n",
    "\n",
    "collection_id = get_or_create_collection(os.getenv(\"NAMESPACE\"), os.getenv(\"COLLECTION\"))\n",
    "print(f\"Collection: {collection_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Configuring a Document Upload Stage\n",
    "\n",
    "A stage provides temporary storage for source documents before they're processed. In this step, we create a stage named \"DocumentStorageTutotialTest\" that will use the \"DocumentToMarkdown\" transformation to convert source documents.\n",
    "\n",
    "The stage configuration includes a trigger that defines what happens when source documents are uploaded. This trigger specifies the transformation to apply and where to store the results.\n",
    "\n",
    "The `get_or_create_stage` function returns a stage ID that will be used when uploading documents in later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Environment Variables\n",
    "STAGE_NAME = \"DocumentStorageTutotialTest\"\n",
    "TRANSFORMATION_NAME = \"DocumentToMarkdown\" # You can check the other transformations in the documentation https://pharia-data-api.<your-deployment>.pharia.com/api/v1/transformations\n",
    "TRIGGER_NAME = \"testTrigger - DocumentStorageTutorial\"\n",
    "\n",
    "os.environ[\"STAGE_NAME\"] = STAGE_NAME\n",
    "os.environ[\"TRANSFORMATION_NAME\"] = TRANSFORMATION_NAME\n",
    "os.environ[\"TRIGGER_NAME\"] = TRIGGER_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper fucntion \n",
    "\n",
    "def get_or_create_stage(stage: dict) -> str:\n",
    "    \"\"\"Get or create a stage in the Data Platform.\"\"\"\n",
    "    dataplatform_base_url = os.getenv(\"DATA_PLATFORM_URL\")\n",
    "    name = stage[\"name\"]\n",
    "    url = f\"{dataplatform_base_url}/api/v1/stages?name={name}\"\n",
    "\n",
    "    token = os.getenv(\"TOKEN\")\n",
    "    response = requests.get(\n",
    "        url=url, headers={\"Authorization\": f\"Bearer {token}\"}, verify=False\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    page = response.json()\n",
    "\n",
    "    if page[\"total\"] > 0:\n",
    "        return page[\"stages\"][0][\"stageId\"]\n",
    "    else:\n",
    "        url = f\"{dataplatform_base_url}/api/v1/stages\"\n",
    "        response = requests.post(\n",
    "            url=url,\n",
    "            json=stage,\n",
    "            headers={\"Authorization\": f\"Bearer {token}\"},\n",
    "            verify=False,\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        stage_created = response.json()\n",
    "        return stage_created[\"stageId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup Stage\n",
    "\n",
    "stage_payload = {\n",
    "    \"name\": os.getenv(\"STAGE_NAME\"),\n",
    "    \"triggers\": [\n",
    "        {\n",
    "            \"transformationName\": os.getenv(\"TRANSFORMATION_NAME\"),\n",
    "            \"destinationType\": \"DataPlatform:Repository\",\n",
    "            \"connectorType\": \"DocumentIndex:Collection\",\n",
    "            \"name\": TRIGGER_NAME,\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "stage_id = get_or_create_stage(stage_payload)\n",
    "print(f\"Stage ID: {stage_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create & Assign a Searchable Index for Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An index enables efficient searching of your document content. The `create_index_and_assign_to_collection` function creates an index with specified parameters and assigns it to your collection.\n",
    "\n",
    "The key parameters include:\n",
    "- `chunk_size`: Controls how documents are divided into searchable segments (256 tokens)\n",
    "- `chunk_overlap`: Defines overlap between chunks to maintain context (10 tokens)\n",
    "- `embedding_type`: Specifies the vector embedding approach (\"asymmetric\")\n",
    "\n",
    "Once the index is assigned to your collection, any ingested documents will be automatically processed according to these settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper function\n",
    "# TODO: Change default embedding type and point to documentation of different types\n",
    "def create_index_and_assign_to_collection(index_name: str, collection_name: str, namespace: str = \"Studio\", chunk_size: int = 256, chunk_overlap: int = 10, embedding_type: str = \"asymmetric\") -> str:\n",
    "    \"\"\"Create an index in the Document Index.\"\"\"\n",
    "    token = os.getenv(\"TOKEN\")\n",
    "    document_index_base_url = os.getenv(\"DOCUMENT_INDEX_API_URL\")\n",
    "    url = f\"{document_index_base_url}/indexes/{namespace}/{index_name}\"\n",
    "    payload = {\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"chunk_overlap\": chunk_overlap,\n",
    "        \"embedding_type\": embedding_type\n",
    "    }\n",
    "    response = requests.put(url, json=payload, headers={\"Authorization\": f\"Bearer {token}\"})\n",
    "    response.raise_for_status()\n",
    "    print(f\"Index created: {index_name}\")\n",
    "\n",
    "    # Assign the index to the collection\n",
    "    url = f\"{document_index_base_url}/collections/{namespace}/{collection_name}/indexes/{index_name}\"\n",
    "    response = requests.put(url, headers={\"Authorization\": f\"Bearer {token}\"})\n",
    "    response.raise_for_status()\n",
    "    print(f\"Index '{index_name}' assigned to collection '{collection_name}' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_index_and_assign_to_collection(index_name=\"studio-tutorial-index\", collection_name=os.getenv(\"COLLECTION\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Setting Up Automated Document Processing\n",
    "\n",
    "The trigger configuration defines what happens when source documents are uploaded to the stage. The `ingestion_context` object combines three key elements:\n",
    "\n",
    "1. The trigger name that identifies which trigger to activate\n",
    "2. The destination repository where processed documents will be stored\n",
    "3. The collection and namespace where processed documents will be indexed\n",
    "\n",
    "This context will be included with source document uploads to instruct the system on how to process each document. When a source document is uploaded, the specified trigger automatically applies the transformation and indexes the processed document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Environment Varaibles\n",
    "\n",
    "TEST_TRIGGER = os.environ[\"TRIGGER_NAME\"]\n",
    "os.environ[\"TEST_TRIGGER\"] = TEST_TRIGGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestion_context = {\n",
    "    \"triggerName\": os.getenv(\"TEST_TRIGGER\"),\n",
    "    \"destinationContext\": {\"repositoryId\": repository_id},\n",
    "    \"connectorContext\": {\n",
    "        \"collection\": os.getenv(\"COLLECTION\"),\n",
    "        \"namespace\": os.getenv(\"NAMESPACE\"),\n",
    "    },\n",
    "}\n",
    "print(f\"Ingestion context: {ingestion_context}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Uploading and Processing Documents\n",
    "\n",
    "With our infrastructure set up (repository, stage, index, and trigger), we can now upload source documents to the Pharia platform. This section demonstrates how to upload source documents and initiate the document ingestion process.\n",
    "\n",
    "The document ingestion workflow transforms source documents into searchable processed documents through several steps: uploading to the stage, applying transformations, storing in the repository, and indexing for search.\n",
    "\n",
    "The `ingest_all_documents` helper function returns a DataFrame with details on each upload attempt, making it easy to track successes and failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper fucntions\n",
    "\n",
    "@retry(\n",
    "    stop=stop_after_attempt(3),\n",
    "    wait=wait_exponential(multiplier=1, min=2, max=10),\n",
    "    retry=retry_if_exception_type(requests.RequestException),\n",
    ")\n",
    "def ingest_document(\n",
    "    document_path: str, ingestion_context: dict, name: str, stage_id: str\n",
    ") -> dict:\n",
    "    \"\"\"Attempts to ingest a document and returns the ingestion result.\"\"\"\n",
    "    with open(document_path, mode=\"rb\") as file_reader:\n",
    "        dataplatform_base_url = os.getenv(\"DATA_PLATFORM_URL\")\n",
    "        url = f\"{dataplatform_base_url}/api/v1/stages/{stage_id}/files\"\n",
    "        token = os.getenv(\"TOKEN\")\n",
    "        response = requests.post(\n",
    "            url=url,\n",
    "            headers={\"Authorization\": f\"Bearer {token}\"},\n",
    "            verify=False,\n",
    "            files={\n",
    "                \"name\": name,\n",
    "                \"sourceData\": file_reader,\n",
    "                \"ingestionContext\": json.dumps(ingestion_context),\n",
    "            },\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "\n",
    "        file_uploaded = response.json()\n",
    "        return {\n",
    "            \"file_id\": file_uploaded[\"fileId\"],\n",
    "            \"status\": \"Success\",\n",
    "            \"error_type\": None,\n",
    "            \"error_message\": None,\n",
    "        }\n",
    "    \n",
    "\n",
    "\n",
    "def ingest_all_documents(\n",
    "    directory_path: str, ingestion_context: dict, stage_id: str, max_workers: int = 3\n",
    "):\n",
    "    \"\"\"Ingest all files in a directory concurrently and store results in a DataFrame.\"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_file = {\n",
    "            executor.submit(\n",
    "                ingest_document,\n",
    "                os.path.join(directory_path, file),\n",
    "                ingestion_context,\n",
    "                file,\n",
    "                stage_id,\n",
    "            ): file\n",
    "            for file in os.listdir(directory_path)\n",
    "        }\n",
    "\n",
    "        for future in concurrent.futures.as_completed(future_to_file):\n",
    "            file_name = future_to_file[future]\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"file_path\": file_path,\n",
    "                        \"file_id\": result[\"file_id\"],\n",
    "                        \"status\": result[\"status\"],\n",
    "                        \"error_type\": result[\"error_type\"],\n",
    "                        \"error_message\": result[\"error_message\"],\n",
    "                    }\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while ingesting {file_path}: {e}\")\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"file_path\": file_path,\n",
    "                        \"file_id\": None,\n",
    "                        \"status\": \"Ingestion Failed\",\n",
    "                        \"error\": str(e),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingesting the files\n",
    "directory_path = \"files_to_upload\"\n",
    "df_results = ingest_all_documents(directory_path, ingestion_context, stage_id)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Monitoring Source Document Processing Status\n",
    "\n",
    "After uploading source documents, you need to verify their processing status. The code in this section:\n",
    "\n",
    "1. Extracts IDs of successfully uploaded source documents\n",
    "2. Retrieves the transformation ID\n",
    "3. Checks the status of each source document's transformation\n",
    "4. Extracts dataset IDs from completed transformations\n",
    "\n",
    "The `check_files_status` function combines all this information into a comprehensive report that shows which files completed processing and which encountered errors. The dataset IDs are particularly important as they're used to access your processed documents in subsequent operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_successful_document_ids(df: pd.DataFrame) -> list:\n",
    "    \"\"\"Retrieve a list of successful file_ids from the DataFrame.\"\"\"\n",
    "    return df[df[\"status\"] == \"Success\"][\"file_id\"].tolist()\n",
    "\n",
    "@retry(\n",
    "    stop=stop_after_attempt(3),\n",
    "    wait=wait_exponential(multiplier=1, min=2, max=10),\n",
    "    retry=retry_if_exception_type(requests.RequestException),\n",
    ")\n",
    "def check_status_of_ingestion(transformation_id: str, file_id: str) -> dict:\n",
    "    \"\"\"Query the status of the ingestion for a given transformation and file_id.\"\"\"\n",
    "    dataplatform_base_url = os.getenv(\"DATA_PLATFORM_URL\")\n",
    "    url = f\"{dataplatform_base_url}/api/v1/transformations/{transformation_id}/runs?file_id={file_id}\"\n",
    "\n",
    "    token = os.getenv(\"TOKEN\")\n",
    "    response = requests.get(\n",
    "        url=url, headers={\"Authorization\": f\"Bearer {token}\"}, verify=False\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    page = response.json()\n",
    "\n",
    "    assert page[\"total\"] > 0\n",
    "    return page[\"runs\"][0]\n",
    "\n",
    "def get_transformation_id(name: str) -> str:\n",
    "    \"\"\"Get the transformation ID from the Data Platform.\"\"\"\n",
    "    dataplatform_base_url = os.getenv(\"DATA_PLATFORM_URL\")\n",
    "    url = f\"{dataplatform_base_url}/api/v1/transformations?name={name}\"\n",
    "\n",
    "    token = os.getenv(\"TOKEN\")\n",
    "    response = requests.get(\n",
    "        url=url, headers={\"Authorization\": f\"Bearer {token}\"}, verify=False\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    page = response.json()\n",
    "\n",
    "    assert page[\"total\"] > 0\n",
    "    return page[\"transformations\"][0][\"transformationId\"]\n",
    "\n",
    "def check_files_status(transformation_id: str, df: pd.DataFrame, max_workers: int = 3):\n",
    "    \"\"\"Check the status of ingested files and store the results in a DataFrame.\"\"\"\n",
    "\n",
    "    successful_file_ids = get_successful_document_ids(df)\n",
    "    status_results = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_file = {\n",
    "            executor.submit(\n",
    "                check_status_of_ingestion, transformation_id, file_id\n",
    "            ): file_id\n",
    "            for file_id in successful_file_ids\n",
    "        }\n",
    "\n",
    "        for future in concurrent.futures.as_completed(future_to_file):\n",
    "            file_id = future_to_file[future]\n",
    "            try:\n",
    "                run = future.result()\n",
    "                output = json.dumps(run.get(\"output\", {}), indent=4)\n",
    "                status_results.append(\n",
    "                    {\n",
    "                        \"file_id\": file_id,\n",
    "                        \"run_id\": run[\"runId\"],\n",
    "                        \"status\": run[\"status\"],\n",
    "                        \"output\": output,\n",
    "                        \"error\": run[\"errors\"],\n",
    "                    }\n",
    "                )\n",
    "            except Exception as e:\n",
    "                status_results.append(\n",
    "                    {\n",
    "                        \"file_id\": file_id,\n",
    "                        \"status\": run[\"status\"],\n",
    "                        \"output\": None,\n",
    "                        \"error\": str(e),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return df.merge(\n",
    "        pd.DataFrame(status_results),\n",
    "        on=\"file_id\",\n",
    "        how=\"left\",\n",
    "        suffixes=(\"_ingestion\", \"\"),\n",
    "    )\n",
    "\n",
    "def get_successful_dataset_ids(df: pd.DataFrame) -> list:\n",
    "    \"\"\"Retrieve a list of successful dataset_ids from the DataFrame.\"\"\"\n",
    "    dataset_ids_list = []\n",
    "    for i in range(len(df)):\n",
    "        dataset_ids_list.append(json.loads(df[\"output\"][i]).get(\"datasetId\"))\n",
    "    return dataset_ids_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation_id = get_transformation_id(os.getenv(\"TRANSFORMATION_NAME\"))\n",
    "status_df = check_files_status(transformation_id, df_results)\n",
    "status_df.to_csv(\"ingestion_status.csv\", index=False)\n",
    "successful_dataset_ids = get_successful_dataset_ids( status_df[status_df[\"status\"] == \"completed\"])\n",
    "status_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Working with Processed Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With source documents ingested and processed, you can now interact with your data in various ways:\n",
    "\n",
    "1. **Search Operation**: The `search_text` function demonstrates semantic search against your indexed processed documents, finding content based on meaning rather than exact keyword matches.\n",
    "\n",
    "2. **Document & Metadata Retrieval**: The `get_document_from_document_index` function retrieves a complete processed document and its metadata using the dataset ID.\n",
    "\n",
    "3. **Text Display**: The `display_processed_document_text` function shows how to access the actual content extracted from your source documents, helping you verify the quality of text extraction.\n",
    "\n",
    "These operations showcase the fundamental ways to interact with your processed documents in the Pharia platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1. Searching Document Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After successfully ingesting documents, one of the most valuable operations is searching through your content. This section demonstrates how to perform semantic searches against your indexed documents.\n",
    "\n",
    "The `search_text` function sends a query to the Document Index API, which uses vector embeddings to find semantically relevant content. Unlike traditional keyword search, this approach can identify conceptually related information even when exact terms don't match.\n",
    "\n",
    "In this example, we search for content related to \"what is attention?\" and retrieve matches ranked by relevance. The results include document chunks that semantically align with the query, along with confidence scores indicating match quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def search_text(namespace: str, collection: str, text: str, index: str) -> dict:\n",
    "    di_base_url = os.getenv(\"DOCUMENT_INDEX_API_URL\")\n",
    "    url = f\"{di_base_url}/collections/{namespace}/{collection}/indexes/{index}/search\"\n",
    "\n",
    "    token = os.getenv(\"TOKEN\")\n",
    "    payload = {\"query\": [{\"modality\": \"text\", \"text\": text}]}\n",
    "    response = requests.post(\n",
    "        url=url,\n",
    "        json=payload,\n",
    "        headers={\"Authorization\": f\"Bearer {token}\"},\n",
    "        verify=False,\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_search = \"what is attention?\"\n",
    "search_result = search_text(\n",
    "    os.getenv(\"NAMESPACE\"), os.getenv(\"COLLECTION\"), text_to_search, index=\"studio-tutorial-index\"\n",
    ")\n",
    "print(json.dumps(search_result, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2. Retrieving Complete Documents and Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While searching helps find specific information, sometimes you need to retrieve a complete document along with its metadata. This operation is useful when you want to examine a document's full context or access its associated properties.\n",
    "\n",
    "The `get_document_from_document_index` function retrieves a document using its dataset ID (obtained during the ingestion process). The response includes both the document content and additional metadata such as creation time, source information, and any custom properties attached during processing.\n",
    "\n",
    "This example retrieves the fourth document from our previously ingested set, demonstrating how to access specific documents directly when you know their IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def get_document_from_document_index(namespace, collection, dataset_id) -> dict:\n",
    "    di_base_url = os.getenv(\"DOCUMENT_INDEX_API_URL\")\n",
    "    url = f\"{di_base_url}/collections/{namespace}/{collection}/docs/{dataset_id}\"\n",
    "\n",
    "    token = os.getenv(\"TOKEN\")\n",
    "    response = requests.get(\n",
    "        url=url, headers={\"Authorization\": f\"Bearer {token}\"}, verify=False\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_documents = []\n",
    "\n",
    "for id in successful_dataset_ids:\n",
    "    document_from_di = get_document_from_document_index(\n",
    "        os.getenv(\"NAMESPACE\"), os.getenv(\"COLLECTION\"), id\n",
    "    )\n",
    "    all_documents.append(document_from_di)\n",
    "\n",
    "print(json.dumps(all_documents[0], indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3. Viewing Extracted Document Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To inspect the actual content extracted from your documents, you can retrieve and display the text chunks stored in the repository. This is useful for verifying extraction quality and understanding how your documents were segmented.\n",
    "\n",
    "The `display_text_extracted` function connects to the Data Platform repository and retrieves text chunks from a specific document. It displays each chunk sequentially, showing how the document was divided during processing.\n",
    "\n",
    "This operation helps you validate that your documents were properly processed and that the extracted text accurately represents the original content. It can be particularly valuable when troubleshooting search issues or refining your ingestion parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function\n",
    "def display_processed_document_text(repository_id: str, dataset_id: str) -> None:\n",
    "    number_of_pages = 10\n",
    "    dataplatform_base_url = os.getenv(\"DATA_PLATFORM_URL\")\n",
    "    url = f\"{dataplatform_base_url}/api/v1/repositories/{repository_id}/datasets/{dataset_id}/datapoints?size={number_of_pages}\"\n",
    "\n",
    "    token = os.getenv(\"TOKEN\", \"QydIbwy17Agq7LBKE9XXQPx_OYoGmFAQuRCUilC2Ul8QVnWksHh897b3DNMYxbxhp0hotUw\")\n",
    "    response = requests.get(\n",
    "        url=url, headers={\"Authorization\": f\"Bearer {token}\"}, verify=False, stream=True\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    for line in response.iter_lines():\n",
    "        print(\"# starting new page ...\")\n",
    "        datapoint = json.loads(line.decode())\n",
    "        # print(f\"{datapoint['text'][:100]}...\")\n",
    "        print(datapoint)\n",
    "\n",
    "\n",
    "display_processed_document_text(repository_id, successful_dataset_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this section, you've successfully set up the complete document ingestion pipeline:\n",
    "\n",
    "✅ **Configured the environment** with connections to both the Data Platform and Document Index APIs\n",
    "\n",
    "✅ **Built the foundation infrastructure**:\n",
    "   - Created a repository for storing processed documents\n",
    "   - Set up a stage for temporary source document storage\n",
    "   - Configured an index for enabling semantic search\n",
    "   - Established triggers for automating document processing\n",
    "\n",
    "✅ **Implemented document operations** with:\n",
    "   - Concurrent source document uploads with error handling\n",
    "   - Status monitoring for transformation processes\n",
    "   - Multiple ways to interact with processed documents\n",
    "\n",
    "Your source document collection is now properly ingested, processed, and ready for semantic search operations. This data foundation will serve as the basis for retrieval-augmented generation in the subsequent sections of this tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
