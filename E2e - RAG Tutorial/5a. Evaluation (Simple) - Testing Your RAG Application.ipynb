{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5a. Evaluation - Testing your RAG application (simple)\n",
    "<a id=\"evaluation1\"></a>\n",
    "\n",
    "\n",
    "This section helps you assess the quality and performance of your RAG application through objective, automated testing. Effective evaluation is crucial for ensuring that your application accurately retrieves relevant information and generates helpful responses based on your document collection.\n",
    "\n",
    "## Evaluation components\n",
    "\n",
    "The evaluation framework we will build examines several critical aspects:\n",
    "\n",
    "- **Answer quality**: Assesses the completeness of generated responses against expected content\n",
    "- **Performance metrics**: Quantifies the system's effectiveness through pass rates and match scores\n",
    "- **Test dataset**: Provides a diverse set of questions with expected keywords for consistent evaluation\n",
    "\n",
    "**Note:** While this tutorial uses simple keyword matching for clarity and ease of implementation, your evaluation approach should be tailored to your specific use case. Consider alternatives such as *LLM as a judge*, *BERTScores*, or other domain-specific evaluation criteria for specialised applications\n",
    "\n",
    "## What you will learn\n",
    "\n",
    "1. How to wrap your PhariaKernel Skill in an Intelligence Layer task for consistent testing\n",
    "2. How to create and register a test dataset with questions and expected keywords\n",
    "3. How to implement evaluation logic for measuring keyword presence in answers\n",
    "4. How to create aggregation metrics to analyse overall system performance\n",
    "5. How to run benchmarks and interpret the results\n",
    "6. How to identify targeted improvement opportunities based on evaluation insights\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting, ensure you have the following:\n",
    "\n",
    "- **Completed Skills**: You have finished the [Skill Setup](4.%20Skill%20Setup%20-%20Customizing%20and%20Developing%20Kernel%20Skills.ipynb#evaluation1) section and have a working RAG Skill deployed in PhariaKernel\n",
    "- **Intelligence Layer**: Access to the PhariaAI Intelligence Layer SDK for structured evaluation\n",
    "- **Studio access**: Permissions to create datasets and benchmarks in PhariaStudio\n",
    "- **Authentication**: A valid API token with permissions to access your deployed skill\n",
    "- **Document collection**: The same document collection used by your RAG application\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the evaluation environment\n",
    "\n",
    "To effectively evaluate your RAG application, we use the PhariaAI Intelligence Layer framework. However, we are actively working on creating an integrated experience and we will improve on the ease of use of the following tasks in a new SDK soon!\n",
    "\n",
    "First, make sure to add the Intelligence Layer and Aleph Alpha client dependencies. Run these commands in your terminal:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "poetry add git+https://github.com/Aleph-Alpha/intelligence-layer-sdk.git --python \">=3.10,<3.13\" \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "pip install aleph-alpha-client\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now import the necessary libraries and set up your environment. We start by importing components from the Intelligence Layer framework that will help us create and run our evaluations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "from collections.abc import Iterable\n",
    "from typing import Iterable\n",
    "from statistics import mean\n",
    "from uuid import uuid4\n",
    "import requests\n",
    "import os\n",
    "\n",
    "from intelligence_layer.connectors import StudioClient\n",
    "\n",
    "from intelligence_layer.core import NoOpTracer, Task, TaskSpan\n",
    "\n",
    "from intelligence_layer.evaluation import (\n",
    "    Example,\n",
    "    StudioDatasetRepository,\n",
    "    AggregationLogic,\n",
    "    StudioBenchmarkRepository,\n",
    "    SingleOutputEvaluationLogic,\n",
    ")\n",
    "\n",
    "from intelligence_layer.evaluation.dataset.domain import Example\n",
    "\n",
    "\n",
    "from rag_tutorial.skill.qa import Input, Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have named your project differently, replace the last line with ```from <your-application>.skill.qa import Input, Output```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure\n",
    "\n",
    "### 1. Connect to PhariaStudio\n",
    "\n",
    "First, we need to establish a connection to PhariaStudio, which will be used to store our evaluation datasets, benchmarks, and traces. The StudioClient provides an interface for creating and managing these resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"rag_tutorial/skill/.env\")\n",
    "PHARIA_STUDIO_PROJECT_NAME = \"rag-tutorial\"\n",
    "\n",
    "studio_client = StudioClient(\n",
    "    project=PHARIA_STUDIO_PROJECT_NAME,\n",
    "    studio_url=os.getenv(\"PHARIA_STUDIO_ADDRESS\"),\n",
    "    auth_token=os.getenv(\"PHARIA_AI_TOKEN\"),\n",
    "    create_project=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a task wrapper for your RAG Skill\n",
    "\n",
    "To evaluate our RAG Skill, we need to wrap it in an Intelligence Layer task. This wrapper serves as an adapter between your Skill implementation and the evaluation framework. The task abstraction allows the framework to:\n",
    "\n",
    "1. Execute your Skill with different inputs\n",
    "2. Capture outputs systematically\n",
    "3. Track performance metrics and execution details\n",
    "4. Integrate with PhariaStudio's visualisation tools\n",
    "\n",
    "The `QATask` class below implements the Intelligence Layer's task interface, providing a standardised way to invoke our PhariaKernel Skill, as follows:\n",
    "\n",
    "- It connects to PhariaKernel using a REST API\n",
    "- It forwards input questions to our deployed Skill\n",
    "- It transforms API responses into a structured output format\n",
    "\n",
    "This abstraction allows us to easily test our Skill with different evaluation datasets and compare performance across different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QATask(Task[Input, Output]):\n",
    "    def __init__(self) -> None:\n",
    "        self.token = os.getenv(\"PHARIA_AI_TOKEN\")\n",
    "        self.kernel_url = os.getenv(\"PHARIA_KERNEL_ADDRESS\")\n",
    "        self.skill_namespace = \"playground\"\n",
    "        self.skill_name = \"qa-rag-tutorial\"\n",
    "\n",
    "    def do_run(self, input: Input, task_span: TaskSpan) -> Output:\n",
    "        try:\n",
    "            headers = {\"Authorization\": f\"Bearer {self.token}\"}\n",
    "            url = f\"{self.kernel_url}/v1/skills/{self.skill_namespace}/{self.skill_name}/run\"\n",
    "            response = requests.post(\n",
    "                url,\n",
    "                json=input.model_dump() if isinstance(input, BaseModel) else input,\n",
    "                headers=headers,\n",
    "            )\n",
    "            response = response.json()\n",
    "            return Output(answer=response[\"answer\"], output=response[\"output\"])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return Output(answer=None, output=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, verify that your task wrapper correctly interfaces with the deployed Skill:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = Input(question=\"What is a transformer?\")\n",
    "\n",
    "task = QATask()\n",
    "task.run(test_input, NoOpTracer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create an evaluation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Create a test dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create an example test dataset with questions that cover different topics in our document collection. For each question, we specify keywords that should appear in a well-informed answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = [\n",
    "    {\n",
    "        \"question\": \"What is mixture-of-experts?\",\n",
    "        \"keywords\": [\"experts\", \"gating\", \"combine\"],\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is an Large Language Model\",\n",
    "        \"keywords\": [\"corpus\", \"parameters\", \"generation\"],\n",
    "    },\n",
    "    {\"question\": \"What is a Sequence?\", \"keywords\": [\"order\", \"elements\", \"series\"]},\n",
    "    {\n",
    "        \"question\": \"What is translation?\",\n",
    "        \"keywords\": [\"language\", \"meaning\", \"convert\"],\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the difference between GRNN and RNN?\",\n",
    "        \"keywords\": [\"gates\", \"general\", \"specific\"],\n",
    "    },\n",
    "    {\"question\": \"What is LSTM?\", \"keywords\": [\"memory\", \"gates\", \"vanishing\"]},\n",
    "    {\"question\": \"What is are RNNs?\", \"keywords\": [\"feedback\", \"sequential\", \"state\"]},\n",
    "    {\n",
    "        \"question\": \"What is self-attention?\",\n",
    "        \"keywords\": [\"positions\", \"sequence\", \"relate\"],\n",
    "    },\n",
    "    {\"question\": \"What is Attention?\", \"keywords\": [\"focus\", \"weighting\", \"context\"]},\n",
    "    {\n",
    "        \"question\": \"What is a transformer\",\n",
    "        \"keywords\": [\"attention\", \"parallel\", \"encoder\"],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Pydantic model for expected output\n",
    "\n",
    "Next, we define a Pydantic model to establish a clear structure for test output:\n",
    "\n",
    "1. `EvaluationExpectedOutput`: Defines what keywords we expect in a good answer\n",
    "\n",
    "These models provide type safety and ensure consistent evaluation across multiple benchmark runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationExpectedOutput(BaseModel):\n",
    "    keywords: list[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our test examples prepared, we now need to register them in PhariaStudio. This step creates a persistent, versioned dataset that can be referenced in benchmarks and used for repeated evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Upload the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "studio_dataset_repo = StudioDatasetRepository(studio_client=studio_client)\n",
    "\n",
    "examples = [\n",
    "    Example(\n",
    "        input=Input(question=example[\"question\"]),\n",
    "        expected_output=EvaluationExpectedOutput(keywords=example[\"keywords\"]),\n",
    "    )\n",
    "    for example in test_set\n",
    "]\n",
    "\n",
    "studio_dataset = studio_dataset_repo.create_dataset(\n",
    "    examples=examples, dataset_name=\"demo-dataset\"\n",
    ")\n",
    "\n",
    "studio_dataset.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has now been created and registered in PhariaStudio with the ID shown above. To verify and inspect your dataset:\n",
    "\n",
    "1. Navigate to your PhariaStudio interface in your web browser\n",
    "2. Select the \"Evaluate\" section from the main navigation\n",
    "3. Choose \"Datasets\" from the evaluation options\n",
    "4. Find your \"demo-dataset\" in the list of available datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define evaluation logic\n",
    "\n",
    "Now we implement the core logic for evaluating our RAG system. Our evaluation approach consists of two main components:\n",
    "\n",
    "1. **EvaluationLogic**: Evaluates individual examples by comparing the system's output with the expected output\n",
    "2. **AggregationLogic**: Aggregates individual evaluation results into overall metrics\n",
    "\n",
    "For each of these, we define the expected return values as Pydantic models to ensure type safety and define their logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 EvaluationLogic \n",
    "\n",
    "First, we set up the evaluation logic that is used for each individual example. Our `QaEvaluationLogic` class implements this assessment strategy by extending the Intelligence Layer's `SingleOutputEvaluationLogic` interface, allowing it to integrate with the broader evaluation framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QaEvaluation(BaseModel):\n",
    "    matched_keywords: list[str]\n",
    "    missing_keywords: list[str]\n",
    "    match_score: float\n",
    "    passed: bool\n",
    "\n",
    "\n",
    "class QaEvaluationLogic(\n",
    "    SingleOutputEvaluationLogic[Input, Output, EvaluationExpectedOutput, QaEvaluation]\n",
    "):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.threshold = 0.8  ## Threshold to define when an evaluation is passed\n",
    "\n",
    "    def do_evaluate_single_output(\n",
    "        self, example: Example[Input, EvaluationExpectedOutput], output: Output\n",
    "    ) -> QaEvaluation:\n",
    "        required_keywords = example.expected_output.keywords\n",
    "        output_text = output.answer.lower()\n",
    "\n",
    "        matched_keywords = []\n",
    "        missing_keywords = []\n",
    "\n",
    "        for keyword in required_keywords:\n",
    "            if keyword.lower() in output_text:\n",
    "                matched_keywords.append(keyword)\n",
    "            else:\n",
    "                missing_keywords.append(keyword)\n",
    "\n",
    "        match_score = (\n",
    "            len(matched_keywords) / len(required_keywords) if required_keywords else 1.0\n",
    "        )\n",
    "\n",
    "        passed = match_score >= self.threshold\n",
    "\n",
    "        return QaEvaluation(\n",
    "            matched_keywords=matched_keywords,\n",
    "            missing_keywords=missing_keywords,\n",
    "            match_score=match_score,\n",
    "            passed=passed,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure our evaluation logic works correctly, we test it with a sample question and answer about neural network encoders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = Input(question=\"What is an encoder?\")\n",
    "output = Output(\n",
    "    answer=\"**1. SUMMARY:** An encoder is a component of a neural network model, specifically composed of a stack of identical layers, each containing a multi-head self-attention mechanism and a position-wise fully connected feed-forward network.\\n\\n**2. DETAILS:** The encoder is a crucial part of a neural network architecture, particularly in transformer models. It is composed of a stack of N identical layers, where each layer consists of two sub-layers. The first sub-layer is a multi-head self-attention mechanism, which allows the model to attend to different parts of the input sequence simultaneously. The second sub-layer is a simple, position-wise fully connected feed-forward network, which applies a linear transformation to the input. In the context of encoder-decoder attention, the encoder takes in an input sequence and generates a set of output vectors that are used as memory keys and values for the decoder to attend to. This allows the decoder to attend to all positions in the input sequence, enabling the model to generate coherent and contextually relevant outputs.\\n\\n**3. SOURCES:** The provided context documents do not explicitly define what an encoder is, but rather describe its composition and functionality. However, based on the description, it is clear that the encoder is a key component of the neural network model.\"\n",
    ")\n",
    "example = Example(\n",
    "    input=input,\n",
    "    expected_output=EvaluationExpectedOutput(\n",
    "        keywords=[\"encoder\", \"neural network\", \"transformer\"]\n",
    "    ),\n",
    ")\n",
    "evaluation_logic = QaEvaluationLogic()\n",
    "evaluation = evaluation_logic.do_evaluate_single_output(example, output)\n",
    "evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 AggregationLogic\n",
    "\n",
    "To assess overall system performance, we need to aggregate individual evaluation results into meaningful metrics. This is defined in the `QaAggregationLogic` class:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QaAggregatedEvaluation(BaseModel):\n",
    "    pass_rate: float\n",
    "    average_match_score: float\n",
    "\n",
    "\n",
    "class QaAggregationLogic(\n",
    "    AggregationLogic[\n",
    "        QaEvaluation,\n",
    "        QaAggregatedEvaluation,\n",
    "    ]\n",
    "):\n",
    "    def aggregate(self, evaluations: Iterable[QaEvaluation]) -> QaAggregatedEvaluation:\n",
    "        evaluation_list = list(evaluations)\n",
    "        if len(evaluation_list) == 0:\n",
    "            return QaAggregatedEvaluation(\n",
    "                pass_rate=0.0,\n",
    "                average_match_score=0.0,\n",
    "            )\n",
    "\n",
    "        passed_count = sum(1 for eval in evaluation_list if eval.passed)\n",
    "        pass_rate = passed_count / len(evaluation_list)\n",
    "\n",
    "        average_match_score = mean(eval.match_score for eval in evaluation_list)\n",
    "\n",
    "        return QaAggregatedEvaluation(\n",
    "            pass_rate=pass_rate,\n",
    "            average_match_score=average_match_score,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify our aggregation mechanism, we test it by aggregating two examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = Input(question=\"What is an encoder?\")\n",
    "output_1 = Output(\n",
    "    answer=\"**1. SUMMARY:** An encoder is a component of a neural network model, specifically composed of a stack of identical layers, each containing a multi-head self-attention mechanism and a position-wise fully connected feed-forward network.\\n\\n**2. DETAILS:** The encoder is a crucial part of a neural network architecture, particularly in transformer models. It is composed of a stack of N identical layers, where each layer consists of two sub-layers. The first sub-layer is a multi-head self-attention mechanism, which allows the model to attend to different parts of the input sequence simultaneously. The second sub-layer is a simple, position-wise fully connected feed-forward network, which applies a linear transformation to the input. In the context of encoder-decoder attention, the encoder takes in an input sequence and generates a set of output vectors that are used as memory keys and values for the decoder to attend to. This allows the decoder to attend to all positions in the input sequence, enabling the model to generate coherent and contextually relevant outputs.\\n\\n**3. SOURCES:** The provided context documents do not explicitly define what an encoder is, but rather describe its composition and functionality. However, based on the description, it is clear that the encoder is a key component of the neural network model.\"\n",
    ")\n",
    "example_1 = Example(\n",
    "    input=input_1,\n",
    "    expected_output=EvaluationExpectedOutput(\n",
    "        keywords=[\"encoder\", \"neural network\", \"transformer\"]\n",
    "    ),\n",
    ")\n",
    "input_2 = Input(question=\"What is a transformer?\")\n",
    "output_2 = Output(\n",
    "    answer='**1. SUMMARY:** A transformer is a new simple network architecture that uses attention mechanisms, eliminating the need for recurrence and convolutions.\\n\\n**2. DETAILS:** The transformer is a proposed network architecture that relies solely on attention mechanisms, which allow the model to focus on specific parts of the input data when generating output. This approach is different from traditional recurrent neural networks (RNNs) and convolutional neural networks (CNNs), which use recurrence and convolutions, respectively. The transformer architecture is designed to be more parallelizable and requires less training time, making it a more efficient alternative to traditional models.\\n\\n**3. SOURCES:** The information about the transformer architecture is based on the provided context document, which states: \"We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\"'\n",
    ")\n",
    "example_2 = Example(\n",
    "    input=input_2,\n",
    "    expected_output=EvaluationExpectedOutput(\n",
    "        keywords=[\n",
    "            \"transformer\",\n",
    "            \"architecture\",\n",
    "            \"attention\",\n",
    "            \"pickle\",\n",
    "        ]  ## Add a random keyword to test the evaluation\n",
    "    ),\n",
    ")\n",
    "\n",
    "aggregation_logic = QaAggregationLogic()\n",
    "evaluation_logic = QaEvaluationLogic()\n",
    "\n",
    "evaluation_1 = evaluation_logic.do_evaluate_single_output(example_1, output_1)\n",
    "evaluation_2 = evaluation_logic.do_evaluate_single_output(example_2, output_2)\n",
    "aggregation = aggregation_logic.aggregate([evaluation_1, evaluation_2])\n",
    "\n",
    "print(evaluation_1)\n",
    "print(evaluation_2)\n",
    "print(f\"Aggregation: {aggregation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create and run a benchmark\n",
    "\n",
    "With our evaluation components ready, we can now create a benchmark in PhariaStudio and run our evaluation on the test dataset. Benchmarks provide a structured way to do the following:\n",
    "\n",
    "1. Store evaluation datasets and logic for reuse\n",
    "2. Track performance across multiple runs and configurations\n",
    "3. Visualise results through the PhariaStudio interface\n",
    "4. Share insights with team members\n",
    "\n",
    "First, we create a benchmark repository that stores our evaluation configuration and results. This repository is linked to our PhariaStudio project, making it easy to access evaluation results through the web interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_repository = StudioBenchmarkRepository(studio_client=studio_client)\n",
    "\n",
    "benchmark = benchmark_repository.create_benchmark(\n",
    "    dataset_id=studio_dataset.id,\n",
    "    eval_logic=evaluation_logic,\n",
    "    aggregation_logic=aggregation_logic,\n",
    "    name=\"keyword-matching-benchmark\",\n",
    "    description=\"This benchmark evaluates the keyword matching between the model's output and the expected output.\",\n",
    ")\n",
    "\n",
    "benchmark.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we trigger the becnhmark to execute. When we run the benchmark, the following process takes place:\n",
    "\n",
    "1. Each example from our dataset is sent to the RAG task\n",
    "2. The task generates an answer for each question\n",
    "3. Our evaluation logic assesses each answer against expected keywords\n",
    "4. Results are aggregated and stored in PhariaStudio\n",
    "5. A benchmark execution ID is returned for tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = benchmark_repository.get_benchmark(\n",
    "    benchmark_id=benchmark.id,\n",
    "    eval_logic=evaluation_logic,\n",
    "    aggregation_logic=aggregation_logic,\n",
    ")\n",
    "\n",
    "benchmark_execution_id = benchmark.execute(\n",
    "    task=task,\n",
    "    name=str(uuid4()),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the benchmark completes, you can view detailed results in the PhariaStudio interface under Evaluate/Benchmarks. This visualisation helps identify patterns in system performance and can illuminate areas for improvement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improving your RAG application\n",
    "\n",
    "Based on the evaluation results, you can identify areas for improvement in your RAG application. Common improvements include:\n",
    "\n",
    "1. **Refining the prompt**: Adjust the prompt to encourage more precise reference citation\n",
    "2. **Adjusting retrieval parameters**: Modify the number of retrieved documents or relevance thresholds\n",
    "3. **Enhancing document chunking**: Change how documents are split and indexed\n",
    "4. **Implementing better ranking**: Add reranking steps to prioritise the most relevant documents\n",
    "\n",
    "To modify your RAG Skill, see the [Skill Setup](4.%20Skill%20Setup%20-%20Customizing%20and%20Developing%20Kernel%20Skills.ipynb#evaluation1) part of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this section, you established a comprehensive evaluation framework for your RAG application:\n",
    "\n",
    "✅ **Created structured evaluation components** including a task wrapper, evaluation logic, and aggregation metrics\n",
    "\n",
    "✅ **Built a diverse test dataset** covering key topics in your document collection\n",
    "\n",
    "✅ **Established objective evaluation criteria** based on keyword presence in generated answers\n",
    "\n",
    "✅ **Implemented aggregation metrics** to track overall system performance\n",
    "\n",
    "✅ **Integrated with PhariaStudio's visualisation tools** for intuitive result analysis\n",
    "\n",
    "✅ **Developed strategies for targeted improvements** based on evaluation insights\n",
    "\n",
    "This evaluation framework provides you with a systematic way to measure RAG performance and identify areas for improvement. As you refine your application, regular evaluation runs help ensure that changes positively impact system quality.\n",
    "\n",
    "Remember that different use cases may require different evaluation approaches. Consider expanding your evaluation framework with:\n",
    "\n",
    "- Human evaluation for subjective aspects of answer quality\n",
    "- Task-specific metrics for particular domains\n",
    "- *LLM as a judge* for large scale semantic tests\n",
    "\n",
    "In the next section, we explore how to deploy your fully evaluated RAG application for production use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
