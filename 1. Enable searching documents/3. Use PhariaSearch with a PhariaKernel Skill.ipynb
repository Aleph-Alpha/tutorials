{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use PhariaSearch with PhariaKernel Skills\n",
    "\n",
    "### Load the Enviroment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validation_utils import validate_environment\n",
    "\n",
    "# Run environment validation\n",
    "validate_environment()\n",
    "\n",
    "# If validation fails, DO NOT proceed with the rest of the tutorial\n",
    "# Fix the issues identified above first!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use PhariaSearch with PhariaKernel Skills\n",
    "\n",
    "### Load Environment Variables\n",
    "\n",
    "Now we'll load all the configuration parameters from your `.env` file. These include API endpoints, authentication tokens, and the names of resources we'll use for the PhariaKernel skill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getenv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "PHARIA_AI_TOKEN = getenv(\"PHARIA_AI_TOKEN\")\n",
    "PHARIA_KERNEL_ADDRESS = getenv(\"PHARIA_KERNEL_ADDRESS\")\n",
    "NAMESPACE = getenv(\"PHARIA_DATA_NAMESPACE\")\n",
    "COLLECTION = getenv(\"PHARIA_DATA_COLLECTION\")\n",
    "INDEX = getenv(\"INDEX\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"\\033[92m‚úÖ SUCCESS: Environment variables loaded\\033[0m\")\n",
    "print(f\"   ‚Ä¢ Namespace: {NAMESPACE}\")\n",
    "print(f\"   ‚Ä¢ Collection: {COLLECTION}\")\n",
    "print(f\"   ‚Ä¢ Index: {INDEX}\")\n",
    "print(f\"   ‚Ä¢ Kernel Address: {PHARIA_KERNEL_ADDRESS}\")\n",
    "print(f\"   ‚Ä¢ Token: {'‚úì Set' if PHARIA_AI_TOKEN else '‚úó Missing'}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Skill\n",
    "\n",
    "PhariaSearch is natively integrated within PhariaKernel to reduce the effort necessary to develop a RAG (Retrieval-Augmented Generation) based system.\n",
    "\n",
    "The PhariaSkill SDK is available on PyPI (https://pypi.org/project/pharia-skill/) and can be installed with `pip install pharia-skill`. Let's start by importing the necessary dependencies to build our RAG skill.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "PhariaKernel supports only semantic search and not filter indexes or hybrid indexes. This will be supported in the near future.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pharia_skill import ChatParams, Csi, IndexPath, Message, skill\n",
    "from pydantic import BaseModel\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"\\033[92m‚úÖ SUCCESS: Dependencies imported\\033[0m\")\n",
    "print(\"   ‚Ä¢ PhariaSkill SDK components loaded\")\n",
    "print(\"   ‚Ä¢ Pydantic for data validation\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Input and Output Models\n",
    "\n",
    "Let's define the basic Input and Output structure to make our RAG skill work. These Pydantic models ensure type safety and validation for our skill's interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Input(BaseModel):\n",
    "    question: str\n",
    "    namespace: str = NAMESPACE\n",
    "    collection: str = COLLECTION\n",
    "    index: str = INDEX\n",
    "\n",
    "class Output(BaseModel):\n",
    "    answer: str | None\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"\\033[92m‚úÖ SUCCESS: Data models defined\\033[0m\")\n",
    "print(\"   ‚Ä¢ Input model: question + search parameters\")\n",
    "print(\"   ‚Ä¢ Output model: answer (nullable)\")\n",
    "print(\"   ‚Ä¢ Default values from environment variables\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement RAG Skill Logic\n",
    "\n",
    "Finally, let's define our AI logic within the skill. This function will:\n",
    "1. Search for relevant documents using PhariaSearch\n",
    "2. Use the found documents as context for the LLM\n",
    "3. Generate an answer using the context and question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@skill\n",
    "def rag_skill(csi: Csi, input: Input) -> Output:\n",
    "    index = IndexPath(\n",
    "        namespace=input.namespace,\n",
    "        collection=input.collection,\n",
    "        index=input.index,\n",
    "    )\n",
    "\n",
    "    if not (documents := csi.search(index, input.question, 3, 0.5)):\n",
    "        return Output(answer=None)\n",
    "\n",
    "    context = \"\\n\".join([d.content for d in documents])\n",
    "    content = f\"\"\"Using the provided context documents below, answer the following question accurately and comprehensively. If the information is directly available in the context documents, cite it clearly. If not, use your knowledge to fill in the gaps while ensuring that the response is consistent with the given information. Do not fabricate facts or make assumptions beyond what the context or your knowledge base provides. Ensure that the response is structured, concise, and tailored to the specific question being asked.\n",
    "\n",
    "Input: {context}\n",
    "\n",
    "Question: {input.question}\n",
    "\"\"\"\n",
    "    message = Message.user(content)\n",
    "    params = ChatParams(max_tokens=512)\n",
    "    response = csi.chat(\"llama-3.1-8b-instruct\", [message], params)\n",
    "    return Output(answer=response.message.content)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"\\033[92m‚úÖ SUCCESS: PhariaSearch RAG skill function defined\\033[0m\")\n",
    "print(\"   ‚Ä¢ Function: rag_skill() - retrieves documents and generates answers\")\n",
    "print(\"   ‚Ä¢ Search: Retrieves top 3 documents with similarity > 0.5\")\n",
    "print(\"   ‚Ä¢ Context: Combines retrieved documents for LLM context\")\n",
    "print(\"   ‚Ä¢ Generation: Uses llama-3.1-8b-instruct with 512 token limit\")\n",
    "print(\"   ‚Ä¢ Returns: Structured answer or None if no documents found\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Your Skill with DevCSI\n",
    "\n",
    "Now that the Skill is defined, it is possible to test it locally using the DevCSI provided by PhariaKernel. This creates a development environment that connects to your PhariaSearch collection and allows you to test the RAG functionality.\n",
    "\n",
    "The DevCSI requires the environment variables we loaded earlier:\n",
    "- `PHARIA_AI_TOKEN` - Authentication token for API access\n",
    "- `PHARIA_KERNEL_ADDRESS` - PhariaKernel service endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pharia_skill.testing import DevCsi\n",
    "\n",
    "# Initialize DevCSI for testing\n",
    "csi = DevCsi()\n",
    "\n",
    "print(\"\\033[92m‚úÖ SUCCESS: DevCSI initialized\\033[0m\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Test the skill with a sample question\n",
    "question = \"What is the influence of Robert Moses?\"\n",
    "result = rag_skill(csi, Input(question=question))\n",
    "\n",
    "# Display results with beautiful formatting\n",
    "print(\"\\033[96m\" + \"=\"*60 + \"\\033[0m\")\n",
    "print(f\"\\033[96müìã QUESTION:\\033[0m {question}\")\n",
    "print(\"\\033[96m\" + \"=\"*60 + \"\\033[0m\")\n",
    "print()\n",
    "\n",
    "if result.answer:\n",
    "    print(\"\\033[92mü§ñ ANSWER:\\033[0m\")\n",
    "    print(result.answer)\n",
    "    print()\n",
    "    print(\"\\033[92m‚úÖ SUCCESS: Skill execution completed\\033[0m\")\n",
    "else:\n",
    "    print(\"\\033[93m‚ö†Ô∏è  WARNING: No relevant documents found\\033[0m\")\n",
    "\n",
    "print(\"\\033[96m\" + \"=\"*60 + \"\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move the skill to production\n",
    "\n",
    "Once the Skill is ready, it is possible to deploy it to production by following the steps in the <a href=\"https://docs.aleph-alpha.com/products/pharia-ai/pharia-studio/pharia-kernel/quick_start\">Quick Start with Kernel skills</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
