{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup collections and indexes with PhariaSearch\n",
    "\n",
    "While large language models come equipped with extensive built-in knowledge, answering questions given a known text may not be sufficient for your use case. At some point, you will probably want to search through, or answer questions about, your own knowledge base.\n",
    "\n",
    "You can leverage Aleph Alpha's PhariaSearch (previously known as DocumentIndex) – a robust semantic search tool – to pinpoint sections in documents that align closely with your query.\n",
    "\n",
    "In this tutorial, we will go through the creation of Collections and Index configurations, and the upload of text directly to PhariaSearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Validation\n",
    "\n",
    "⚠️ **CRITICAL: Validate your environment before proceeding**\n",
    "\n",
    "This step ensures your `.env` file is properly configured with valid API endpoints, authentication tokens, and unique resource names.\n",
    "\n",
    "**DO NOT SKIP!** If validation fails:\n",
    "- Check your `.env` file for missing values\n",
    "- Contact your infrastructure administrator if errors persist\n",
    "\n",
    "Only proceed after ALL checks pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validation_utils import validate_environment\n",
    "\n",
    "# Run environment validation\n",
    "validate_environment()\n",
    "\n",
    "# If validation fails, DO NOT proceed with the rest of the tutorial\n",
    "# Fix the issues identified above first!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Environment Variables\n",
    "\n",
    "  Now we'll load all the configuration parameters from your `.env` file. These include API endpoints, authentication tokens, and the names of resources\n",
    "  we'll create in PhariaSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getenv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Please make sure to include the /v1 in the api base url\n",
    "PHARIA_API_BASE_URL = getenv(\"PHARIA_API_BASE_URL\")\n",
    "PHARIA_SEARCH_API_URL = f\"{PHARIA_API_BASE_URL}/v1/studio/search\"\n",
    "TOKEN = getenv(\"PHARIA_AI_TOKEN\")\n",
    "\n",
    "NAMESPACE = getenv(\"PHARIA_DATA_NAMESPACE\")\n",
    "COLLECTION = getenv(\"PHARIA_DATA_COLLECTION\")\n",
    "INDEX = getenv(\"INDEX\")\n",
    "HYBRID_INDEX = getenv(\"HYBRID_INDEX\")\n",
    "FILTER_INDEX = getenv(\"FILTER_INDEX\")\n",
    "\n",
    "EMBEDDING_MODEL_NAME = getenv(\"EMBEDDING_MODEL_NAME\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"\\033[92m✅ SUCCESS: Environment variables loaded\\033[0m\")\n",
    "print(f\"   • Namespace: {NAMESPACE}\")\n",
    "print(f\"   • Collection: {COLLECTION}\")\n",
    "print(f\"   • API Base URL: {PHARIA_API_BASE_URL}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup a Search Client with Pharia Data SDK\n",
    "\n",
    "To search through PhariaSearch, you'll first need to setup the Search Client using the environment variables we specified at the beginning of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pharia_data_sdk.connectors import DocumentIndexClient\n",
    "\n",
    "search_client = DocumentIndexClient(\n",
    "    token=TOKEN,\n",
    "    base_url=PHARIA_SEARCH_API_URL,\n",
    ")\n",
    "\n",
    "print(\"\\n\")\n",
    "try:\n",
    "    print(f\"\\033[92m✅ SUCCESS: Search client created\\033[0m\")\n",
    "    print(f\"   • Server URL: {search_client._base_url}\")\n",
    "except AttributeError:\n",
    "    print(\"\\033[92m✅ SUCCESS: Search client created\\033[0m\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a collection\n",
    "The collection is the place where our searchable content will be stored. The following code will user the Pharia Data SDK to create a new collection with the specified details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pharia_data_sdk.connectors.document_index.document_index import CollectionPath\n",
    "from time import sleep\n",
    "\n",
    "collection_path = CollectionPath(namespace=NAMESPACE, collection=COLLECTION)\n",
    "\n",
    "try:\n",
    "    search_client.delete_collection(collection_path)\n",
    "    \n",
    "    # Wait in loop for at max 10 seconds for the collection to be deleted. Poll every 0.5 seconds.\n",
    "    max_wait_time = 10.0\n",
    "    poll_interval = 0.5\n",
    "    elapsed_time = 0.0\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"\\033[93m⚙️  PROCESSING: Waiting for collection deletion...\\033[0m\")\n",
    "    \n",
    "    while elapsed_time < max_wait_time:\n",
    "        try:\n",
    "            # Try to get the collection - if it exists, continue waiting\n",
    "            search_client.documents(collection_path)\n",
    "            sleep(poll_interval)\n",
    "            elapsed_time += poll_interval\n",
    "        except Exception:\n",
    "            # Collection is deleted when we can't access it anymore\n",
    "            print(\"\\033[92m✅ SUCCESS: Collection successfully deleted\\033[0m\")\n",
    "            break\n",
    "    else:\n",
    "        # Timeout reached\n",
    "        print(f\"\\033[93m⚙️  PROCESSING: Timeout after {max_wait_time}s, proceeding anyway\\033[0m\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\033[94mℹ️  INFO: Collection deletion: {e}\\033[0m\")\n",
    "    pass\n",
    "\n",
    "search_client.create_collection(collection_path)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"\\033[92m✅ SUCCESS: Collection created/retrieved\\033[0m\")\n",
    "print(f\"   • Collection: {COLLECTION}\")\n",
    "print(f\"   • Namespace: {NAMESPACE}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a semantic index configuration\n",
    "\n",
    "Now, let's create an index and assign it to this collection. You can do this before or after populating the collection with documents; PhariaSearch automatically updates semantic indexes in the background.\n",
    "In this tutorial we use `pharia-1-embedding-256-control` embedding model. If this model is not available in the environment, please, replace the name with the one that is available (e.g. `luminous-base`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pharia_data_sdk.connectors.document_index.document_index import (\n",
    "    IndexConfiguration,\n",
    "    IndexPath,\n",
    "    SemanticEmbed,\n",
    ")\n",
    "\n",
    "index_path = IndexPath(namespace=NAMESPACE, index=INDEX)\n",
    "\n",
    "# customise the parameters of the index here\n",
    "index_configuration = IndexConfiguration(\n",
    "    chunk_size=64,\n",
    "    chunk_overlap=0,\n",
    "    embedding=SemanticEmbed(model_name=EMBEDDING_MODEL_NAME, representation=\"asymmetric\"),\n",
    ")\n",
    "\n",
    "search_client.create_index(index_path, index_configuration)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"\\033[92m✅ SUCCESS: Semantic index created\\033[0m\")\n",
    "print(f\"   • Index: {INDEX}\")\n",
    "print(f\"   • Model: {EMBEDDING_MODEL_NAME}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# assign the index to the collection\n",
    "search_client.assign_index_to_collection(collection_path, INDEX)\n",
    "\n",
    "print(\"\\033[92m✅ SUCCESS: Index assigned to collection\\033[0m\")\n",
    "print(f\"   • Collection: {COLLECTION}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Upload some text to the collection\n",
    "\n",
    "Now that we have our collection set up, we need to populate it with content that can be searched. Let's create three text objects that will serve as our sample knowledge base. These documents will demonstrate how PhariaSearch can identify relevant information across different types of content.\n",
    "\n",
    "We'll add biographical information about notable figures to showcase the semantic search capabilities. The document content is defined in `sample_documents.py` file, outside of the notebook and imported below.\n",
    "\n",
    "We'll upload each document to our collection. The SDK will automatically handle the document storage and prepare them for indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pharia_data_sdk.connectors.document_index.document_index import (\n",
    "    DocumentContents,\n",
    "    DocumentPath,\n",
    ")\n",
    "\n",
    "from sample_documents import documents\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"\\033[93m⚙️  PROCESSING: Uploading documents to collection\\033[0m\")\n",
    "print(\"\\n\")\n",
    "\n",
    "for doc in documents:\n",
    "    document_path = DocumentPath(\n",
    "        collection_path=collection_path, document_name=doc[\"name\"]\n",
    "    )\n",
    "    search_client.add_document(\n",
    "        document_path, contents=DocumentContents.from_text(doc[\"content\"])\n",
    "    )\n",
    "    print(f\"   ✓ Document '{doc['name']}' uploaded\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"\\033[92m✅ SUCCESS: All {len(documents)} documents uploaded\\033[0m\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### Verify Documents in Collection\n",
    "\n",
    "  Let's verify that our documents have been successfully uploaded to the collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_utils import print_documents_with_time\n",
    "\n",
    "docs = search_client.documents(collection_path)\n",
    "print_documents_with_time(docs, COLLECTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the text is indexed, we can also have a look at its chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pharia_data_sdk.connectors import ResourceNotFound\n",
    "from local_utils import print_document_chunks\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"\\033[94mℹ️  INFO: Checking document chunks (if indexed)\\033[0m\")\n",
    "print(\"\\n\")\n",
    "\n",
    "try:\n",
    "    chunks = search_client.chunks(\n",
    "        DocumentPath(collection_path=collection_path, document_name=documents[0][\"name\"]),\n",
    "        index_name=INDEX,\n",
    "    )\n",
    "    print(f\"\\033[92m✅ SUCCESS: Found {len(chunks)} chunks for document '{documents[0]['name']}'\\033[0m\")\n",
    "    print_document_chunks(chunks, documents[0][\"name\"])\n",
    "except ResourceNotFound:\n",
    "    print(\"\\033[93m⚙️  PROCESSING: Document still embedding, try again in a couple of minutes\\033[0m\")\n",
    "    pass  # This is expected if the document is still embedding.\n",
    "    \n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Setup search indexes\n",
    "\n",
    "### 5.1. Perform semantic search\n",
    "\n",
    "Now that we have uploaded our text, we can search through it using the semantic similarities between a given query and each chunk.\n",
    "\n",
    "To do so, let's use the `DocumentIndexRetriever`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pharia_data_sdk.connectors.retrievers import DocumentIndexRetriever\n",
    "from local_utils import print_search_results\n",
    "\n",
    "document_index_retriever = DocumentIndexRetriever(\n",
    "    document_index=search_client,\n",
    "    index_name=INDEX,\n",
    "    namespace=NAMESPACE,\n",
    "    collection=COLLECTION,\n",
    "    k=12,\n",
    "    threshold=0.55\n",
    ")\n",
    "\n",
    "query = \"The influence of Robert Moses\"\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"\\033[94mℹ️  INFO: Testing semantic search with query '{query}'\\033[0m\")\n",
    "print(\"\\033[93m⚙️  PROCESSING: Search in progress, please wait...\\033[0m\")\n",
    "print(\"\\n\")\n",
    "\n",
    "results = document_index_retriever.get_relevant_documents_with_scores(query=query)\n",
    "\n",
    "print_search_results(results, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Hybrid Search\n",
    "\n",
    "PhariaSearch supports hybrid search, which combines results of semantic search and keyword search.\n",
    "In order to use hybrid search, we need to create a hybrid index and assign it to the collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_path = IndexPath(namespace=NAMESPACE, index=HYBRID_INDEX)\n",
    "\n",
    "# customise the parameters of the index here\n",
    "index_configuration = IndexConfiguration(\n",
    "    chunk_size=64,\n",
    "    chunk_overlap=0,\n",
    "    hybrid_index=\"bm25\",\n",
    "    embedding=SemanticEmbed(model_name=EMBEDDING_MODEL_NAME, representation=\"asymmetric\"),\n",
    ")\n",
    "\n",
    "# create the namespace-wide index resource\n",
    "index = search_client.create_index(index_path, index_configuration)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"\\033[92m✅ SUCCESS: Hybrid index created\\033[0m\")\n",
    "print(f\"   • Index: {HYBRID_INDEX}\")\n",
    "print(f\"   • Type: BM25 + Semantic\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# assign the index to the collection\n",
    "search_client.assign_index_to_collection(collection_path, HYBRID_INDEX)\n",
    "\n",
    "print(\"\\033[92m✅ SUCCESS: Hybrid index assigned to collection\\033[0m\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_client.list_indexes(NAMESPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now search on the hybrid index, we will not only get chunks with a semantic similarity but also chunks that match the keywords in the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_utils import print_search_results\n",
    "\n",
    "document_index_retriever = DocumentIndexRetriever(\n",
    "    document_index=search_client,\n",
    "    index_name=HYBRID_INDEX,\n",
    "    namespace=NAMESPACE,\n",
    "    collection=COLLECTION,\n",
    "    k=5,\n",
    "    threshold=0.5,\n",
    ")\n",
    "\n",
    "query = \"25 April\"\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"\\033[94mℹ️  INFO: Testing hybrid search with query '{query}'\\033[0m\")\n",
    "print(\"\\033[93m⚙️  PROCESSING: Search in progress, please wait...\\033[0m\")\n",
    "print(\"\\n\")\n",
    "\n",
    "results = document_index_retriever.get_relevant_documents_with_scores(query=query)\n",
    "\n",
    "print_search_results(results, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3  Search with Metadata filtering\n",
    "\n",
    "PhariaSearch also supports filter-indexes, which gives us the ability to provide specific filters in case we want to filter our search based on each document's metadata.\n",
    "\n",
    "To do so, let's first upload another version of our documents but this time with some metadata e.g. the \"title\" field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\")\n",
    "print(\"\\033[93m⚙️  PROCESSING: Re-uploading documents with metadata\\033[0m\")\n",
    "print(\"\\n\")\n",
    "\n",
    "for doc in documents:\n",
    "    document_path = DocumentPath(\n",
    "        collection_path=collection_path, document_name=doc[\"name\"]\n",
    "    )\n",
    "    search_client.add_document(\n",
    "        document_path,\n",
    "        contents=DocumentContents(\n",
    "            contents=[doc[\"content\"]], metadata={\"title\": doc[\"name\"]}\n",
    "        ),\n",
    "    )\n",
    "    print(f\"   ✓ Document '{doc['name']}' uploaded with metadata\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"\\033[92m✅ SUCCESS: All {len(documents)} documents uploaded with metadata\\033[0m\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation of the Filter Index\n",
    "To be able to use metadata filtering, we need to first check the following:\n",
    "1. Check if we already have a search index assigned. If not, we need to assign one because filter-indexes can be defined at the namespace level but can only be assigned to already existing search indexes \n",
    "2. Define a new filter-index configuration for our specific collection metadata.\n",
    "3. Assign the filter-index that we created to a search index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "# list all the assigned search indexes for our collection\n",
    "search_client.list_assigned_index_names(collection_path=collection_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "# define a new filter-index\n",
    "search_client.create_filter_index_in_namespace(\n",
    "    namespace=collection_path.namespace,\n",
    "    filter_index_name=FILTER_INDEX,  # this is how our filter-index is identified in our namespace\n",
    "    field_name=\"title\",  # this is the name of the field to which we want to apply our filter\n",
    "    field_type=\"string\",  # type of the field we want to apply our filter to. Must be one of \"string\", \"integer\", \"float\", \"boolean\" or \"datetime\"\n",
    ")\n",
    "\n",
    "# let's check if our index is present now\n",
    "filter_present = FILTER_INDEX in search_client.list_filter_indexes_in_namespace(\n",
    "    namespace=collection_path.namespace\n",
    ")\n",
    "\n",
    "print(\"\\n\")\n",
    "if filter_present:\n",
    "    print(f\"\\033[92m✅ SUCCESS: Filter index created\\033[0m\")\n",
    "    print(f\"   • Filter: {FILTER_INDEX}\")\n",
    "    print(f\"   • Field: title (string)\")\n",
    "else:\n",
    "    print(f\"\\033[91m⚠️  ERROR: Filter index creation failed\\033[0m\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "# assign our new filter-index to our collection\n",
    "search_client.assign_filter_index_to_search_index(\n",
    "    collection_path=collection_path,\n",
    "    index_name=INDEX,  # we assign it to intelligence-layer-sdk-demo-index\n",
    "    filter_index_name=FILTER_INDEX,\n",
    ")\n",
    "\n",
    "# check if our filter-index is assigned to our collection\n",
    "assigned_filters = search_client.list_assigned_filter_index_names(\n",
    "    collection_path=collection_path, index_name=INDEX\n",
    ")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"\\033[92m✅ SUCCESS: Filter index assigned to collection\\033[0m\")\n",
    "print(f\"   • Assigned filters: {assigned_filters}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now, as we have the filter-index enabled, we need to initialize a new `DocumentIndexRetriever` with the search index for which we added the filter-index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_index_retriever = DocumentIndexRetriever(\n",
    "    document_index=search_client,\n",
    "    index_name=INDEX,\n",
    "    namespace=NAMESPACE,\n",
    "    collection=COLLECTION,\n",
    "    k=5,\n",
    ")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"\\033[94mℹ️  INFO: Document retriever configured for filtered search\\033[0m\")\n",
    "print(f\"   • Index: {INDEX}\")\n",
    "print(f\"   • Collection: {COLLECTION}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining and Using a Filter\n",
    "Before we perform the filtered search we have to define a filter.\n",
    "Filters are composed of the following elements:\n",
    "- `filter_type` which can be one of \"with\", \"without\" or \"with_one_of\"\n",
    "- `filter_fields`, which defines the actual filtering criteria over a certain value for our chosen field\n",
    "\n",
    "If we want a filter that accepts only documents with the value of the \"title\" field equal to the \"name\" field of `document_1`, we define the filter as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pharia_data_sdk.connectors import FilterField, FilterOps, Filters\n",
    "\n",
    "filters = Filters(\n",
    "    filter_type=\"with\",  # we want to only return documents matching our filter\n",
    "    fields=[\n",
    "        FilterField(\n",
    "            field_name=\"title\",  # this is the key we used in our metadata dict\n",
    "            field_value=documents[0][\"name\"],  # this is what we used as a value in the metadata dict\n",
    "            criteria=FilterOps.EQUAL_TO,  # we want to match exactly\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"\\033[94mℹ️  INFO: Filter created for documents with title='{documents[0]['name']}'\\033[0m\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use the filters with our query to restrict the search to documents with the title \"robert_moses\"\n",
    "from local_utils import print_search_results\n",
    "\n",
    "query = \"Robert Moses\"\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"\\033[94mℹ️  INFO: Testing filtered search with query '{query}'\\033[0m\")\n",
    "print(f\"   • Filter: Only documents with title='{documents[0]['name']}'\")\n",
    "print(\"\\033[93m⚙️  PROCESSING: Search in progress, please wait...\\033[0m\")\n",
    "print(\"\\n\")\n",
    "\n",
    "results = document_index_retriever.get_relevant_documents_with_scores(\n",
    "    query=query, filters=[filters]\n",
    ")\n",
    "\n",
    "print_search_results(results, f\"{query} (filtered)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We only get document with the \"robert_moses\" as the exact title in the metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exclusion of Documents\n",
    "We can also modify our filter such that we only get documents that do *not* match the specified filter fields. This is as simply as replacing the \"with\" filter_type with a \"without\": "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's now try to exclude the document with the title \"robert_moses\"\n",
    "filters_without = Filters(\n",
    "    filter_type=\"without\",  # we change this to \"without\" to exclude the document\n",
    "    fields=[\n",
    "        FilterField(\n",
    "            field_name=\"title\",  # this is the key we used in our metadata dict\n",
    "            field_value=documents[0][\"name\"],  # this is what we used as a value in the metadata dict\n",
    "            criteria=FilterOps.EQUAL_TO,  # we want to match exactly\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"\\033[94mℹ️  INFO: Filter created to EXCLUDE documents with title='{documents[0]['name']}'\\033[0m\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use the filters with our query to exclude the document with the title \"robert_moses\"\n",
    "from local_utils import print_search_results\n",
    "\n",
    "query = \"Robert Moses\"\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"\\033[94mℹ️  INFO: Testing exclusion filter with query '{query}'\\033[0m\")\n",
    "print(f\"   • Filter: Exclude documents with title='{documents[0]['name']}'\")\n",
    "print(\"\\033[93m⚙️  PROCESSING: Search in progress, please wait...\\033[0m\")\n",
    "print(\"\\n\")\n",
    "\n",
    "results = document_index_retriever.get_relevant_documents_with_scores(\n",
    "    query=query, filters=[filters_without]\n",
    ")\n",
    "\n",
    "print_search_results(results, f\"{query} (excluding filtered)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, how we only get results where the `document_name` is not \"robert_moses\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusions\n",
    "\n",
    "We have now a collection with some documents uploaded and with semantic indexing, hybrid indexing, and the possibility to use metadata for filtering results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
